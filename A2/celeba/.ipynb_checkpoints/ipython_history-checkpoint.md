 1/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, beta
import pandas as pd

from sklearn.preprocessing import StandardScaler, Imputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.datasets import load_boston

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
 1/2: print(boston['DESCR'])
 1/3:
boston = load_boston()
X = boston['data']
y = boston['target']
 2/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, beta
import pandas as pd

from sklearn.preprocessing import StandardScaler, Imputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.datasets import load_boston

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
 2/2:
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, beta
import pandas as pd

from sklearn.preprocessing import StandardScaler, Imputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.datasets import load_boston

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
 2/3:
n_s, p_s = [20, 20, 40], [0.5, 0.7, 0.5]

fig, ax = plt.subplots()
for counter in range(3):
    ax.scatter(range(n_s[counter]), 
               binom.pmf(range(n_s[counter]), n_s[counter], p_s[counter]),
               label=r'$n=%d, p=%.1f$' % (n_s[counter], p_s[counter]))
ax.legend(loc='best')

plt.show()
 2/4:
a_s, b_s = [0.5, 1, 2, 5], [0.5, 5, 2, 1]
x_range = np.linspace(0.01, 1, 100)

fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for counter, ax in enumerate(axes.flat):
    ax.plot(x_range, beta.pdf(x_range, a_s[counter], b_s[counter]))
    ax.set_title(r'$\alpha=%.1f, \beta=%.1f$' % (a_s[counter], b_s[counter]))
    
plt.show()
 2/5:
mu_s, sigma_s = [0, 1, 0], [1, 1, 2]

fig, axes = plt.subplots(1, 3, sharex=True, sharey=True)
for counter in range(3):
    axes[counter].hist(np.random.normal(loc=mu_s[counter], scale=sigma_s[counter], size=10**5), 
                     bins=100)
    axes[counter].set_title(r'$\mu=%d, \sigma=%d$' % (mu_s[counter], sigma_s[counter]))
    
plt.show()
 2/6:
n_samples = [10**2, 10**3, 10**5]

figs,axes = plt.subplots(1, 3, sharex=True)
for counter in range(3):
    axes[counter].hist(np.random.randn(n_samples[counter]), bins=100)
    axes[counter].set_title('%d data points' % n_samples[counter])
    
plt.show()
 2/7:
boston = load_boston()
X = boston['data']
y = boston['target']
 2/8:
boston = load_boston()
X = boston['data']
y = boston['target']
 2/9: print(boston['DESCR'])
2/10:
np.random.seed(42)

X_new = X.copy()
mask = np.random.randint(0, 2, size=X.shape).astype(np.bool)
X_new[mask] = np.nan
2/11:
frequency_count = pd.crosstab(index=X[:, 8], columns="count")
frequency_count
2/12: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
2/13: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
2/14: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
2/15:
a_s, b_s = [0.5, 1, 2, 5], [0.5, 5, 2, 1]
x_range = np.linspace(0.01, 1, 100)

fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for counter, ax in enumerate(axes.flat):
    ax.plot(x_range, beta.pdf(x_range, a_s[counter], b_s[counter]))
    ax.set_title(r'$\alpha=%.1f, \beta=%.1f$' % (a_s[counter], b_s[counter]))
    
plt.show()
 3/1:
#PLEASE DO NOT CHANGE THIS CELL
#We will create some one dimensional data with a bit of noise
num_points = 50
X = np.linspace(0,100,num_points).reshape(num_points,1)
y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)

#Plot the data, we want to divine a best fit
plt.plot(X, y, 'b.')
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.title('Training Data')
plt.show()
 3/2:
#PLEASE DO NOT CHANGE THIS CELL

#Standard python libraries for data and visualisation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#SciKit Learn a python ML Library
#Import models
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

#Import error metric
from sklearn.metrics import mean_squared_error

#Import a dataset
from sklearn.datasets import load_boston

#Import data munging tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures

#Display charts in the notebook
%matplotlib inline
 3/3:
#PLEASE DO NOT CHANGE THIS CELL
#We will create some one dimensional data with a bit of noise
num_points = 50
X = np.linspace(0,100,num_points).reshape(num_points,1)
y = (4 + 3 * X) + 25*np.random.randn(num_points, 1)

#Plot the data, we want to divine a best fit
plt.plot(X, y, 'b.')
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.title('Training Data')
plt.show()
 3/4: help(np.reshape)
 3/5: help(np.random.randn)
 3/6:
def simple_linear_regression(X, y):
    #[your code here]
    pass
 6/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
 8/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
10/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/6:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/7:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/8:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
14/9:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
16/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/6:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/7:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/8:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
"""
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/9:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_t.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
20/10:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
20/11:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
df_train.describe()
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
20/12:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
df_train.describe()
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
20/13:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
df_train.describe()
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
20/14: df_train.describe()
20/15: X_train.describe()
20/16: y_train.describe()
20/17: print(y_train)
20/18:
#print(y_train)
#y_train.describe()
20/19:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['LIMIT_BAL'], kde = True, bins = 200, colour = "blue")
plt.show()
21/1:
plt.figure(figsize = (14,6))
plt.title('Amount of credit limit - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(data_df['LIMIT_BAL'],kde=True,bins=200, color="blue")
plt.show()
20/20:
print(X_train['X1'].value_counts().nlargest(5))
print('\nNANs found:', sum(X_train['X1'] == 0))
20/21:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['LIMIT_BAL'], kde = True, bins = 200, colour = "blue")
plt.show()
20/22:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins = 200, colour = "blue")
plt.show()
20/23:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins = 200, color = "blue")
plt.show()
20/24:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True)
plt.show()
20/25:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True bins = 10000, color = "blue")
plt.show()
20/26:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins = 10000, color = "blue")
plt.show()
20/27:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins = 10000, color = "blue")
plt.show()
21/2: data_df = pd.read_csv(r"C:\/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default\UCI_Credit_Card.csv")
21/3:
import pandas as pd 
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline 

import gc
from datetime import datetime 
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

pd.set_option('display.max_columns', 100)
20/28:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True)
plt.show()
20/29:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True)
X_train.describe()
plt.show()
20/30:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
#sns.distplot(X_train['X1'], kde = True)
X_train.describe()
plt.show()
20/31:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
#sns.distplot(X_train['X1'], kde = True)
X_train.describe()
#plt.show()
20/32:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
#sns.distplot(X_train['X1'], kde = True)
X_train['X1'].describe()
#plt.show()
21/4:
import pandas as pd 
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline 

import gc
from datetime import datetime 
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from catboost import CatBoostClassifier
import lightgbm as lgb
import xgboost as xgb

pd.set_option('display.max_columns', 100)
21/5:
RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier
NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier
NO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier

#VALIDATION
VALID_SIZE = 0.20 # simple validation using train_test_split

#CROSS-VALIDATION
NUMBER_KFOLDS = 5 #number of KFolds for cross-validation

RANDOM_STATE = 2018

MAX_ROUNDS = 1000 #lgb iterations
EARLY_STOP = 50 #lgb early stop 
OPT_ROUNDS = 1000  #To be adjusted based on best validation rounds
VERBOSE_EVAL = 50 #Print out metric result

IS_LOCAL = False

import os

#if(IS_LOCAL):
#    PATH="../input/default-of-credit-card-clients-dataset"
#else:
#    PATH="../input"
#print(os.listdir(PATH))
21/6: data_df = pd.read_csv(r"C:\/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default\UCI_Credit_Card.csv")
21/7: data_df = pd.read_csv(r"C:/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default\UCI_Credit_Card.csv")
21/8: data_df = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default\UCI_Credit_Card.csv")
21/9: data_df = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default\UCI_Credit_Card.csv")
21/10: data_df = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/UCI_Credit_Card.csv")
21/11: data_df = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/UCI_Credit_Card.csv")
21/12: print("Default Credit Card Clients data -  rows:",data_df.shape[0]," columns:", data_df.shape[1])
21/13: data_df.head()
21/14: data_df.describe()
21/15:
total = data_df.isnull().sum().sort_values(ascending = False)
percent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)
pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()
21/16:
temp = data_df["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()
21/17:
temp = data_df["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()
21/18:
plt.figure(figsize = (14,6))
plt.title('Amount of credit limit - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(data_df['LIMIT_BAL'],kde=True,bins=200, color="blue")
plt.show()
20/33:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins=200, color="blue")
X_train['X1'].describe()
#plt.show()
20/34:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
#plt.show()
20/35:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
plt.show()
20/36:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
#sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
plt.show()
21/19:
plt.figure(figsize = (14,6))
plt.title('Amount of credit limit - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(data_df['LIMIT_BAL'],kde=True,bins=200, color="blue")
data_df['LIMIT_BAL'].describe()
#plt.show()
20/37:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
plt.show()
20/38:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
#sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
plt.show()
20/39:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
#sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
#plt.show()
20/40:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
20/41:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
20/42:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
20/43:
#print(y_train)
#y_train.describe()
20/44: X_train.head()
20/45: X_train.describe(include='all')
20/46:
"""temp = dataset["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()"""
20/47:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
20/48:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_test:
    if data_test[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
20/49: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
20/50: X_train.head()
20/51: y_train.head()
20/52:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")
20/53:
print(X_train['X1'].value_counts().nlargest(5))
print('\nNANs found:', sum(X_train['X1'] == 0))
20/54:
#print(y_train)
X_train.describe()
20/55:
#print(y_train)
X_train['X1'].describe()
20/56:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1'].astype(str).astype(int)
#sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
#plt.show()
20/57:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
#sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
#plt.show()
20/58:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
sns.distplot(X_train['X1'], kde = True, bins=24000, color="blue")
X_train['X1'].describe()
#plt.show()
22/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
22/2:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
22/3:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
22/4:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
sns.distplot(X_train['X1'], kde = True, bins=1000, color="blue")
plt.show()
22/5:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
sns.distplot(X_train['X1'], kde = True, bins=100, color="blue")
plt.show()
22/6:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
sns.distplot(X_train['X1'], kde = True, bins=1000, color="blue")
plt.show()
22/7:
plt.figure(figsize = (14,6))
plt.title('Amount of Credit Card Limits - Density Plot')
sns.set_color_codes("pastel")
X_train['X1']=X_train['X1'].astype(str).astype(int)
sns.distplot(X_train['X1'], kde = True, bins=100, color="blue")
plt.show()
22/8: X_train['X1'].value_counts().shape
22/9: X_train['X1'].value_counts().head(5)
22/10:
class_0 = X_train.loc[X_train['Default.Payment.Next.Month']==0]["X1"]
class_1 = X_train.loc[X_train['Default.Payment.Next.Month']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0. kde = True, bins = 100. color = "green")
plt.show()
22/11:
class_0 = X_train.loc[X_train['Default.Payment.Next.Month']==0]["X1"]
class_1 = X_train.loc[X_train['Default.Payment.Next.Month']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100. color = "green")
plt.show()
22/12:
class_0 = X_train.loc[X_train['Default.Payment.Next.Month']==0]["X1"]
class_1 = X_train.loc[X_train['Default.Payment.Next.Month']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/13:
class_0 = X_train.loc[X_train['default.payment.next.month']==0]["X1"]
class_1 = X_train.loc[X_train['default.payment.next.month']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/14:
class_0 = X_train.loc[X_train['default.payment.next.month']==0]["X1"]
class_1 = X_train.loc[X_train['default.payment.next.month']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/15:
class_0 = X_train.loc[X_train['y']==0]["X1"]
class_1 = X_train.loc[X_train['y']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/16:
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
plt.fugure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/17:
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/18:
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/19:
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
21/20:
class_0 = data_df.loc[data_df['default.payment.next.month'] == 0]["LIMIT_BAL"]
class_1 = data_df.loc[data_df['default.payment.next.month'] == 1]["LIMIT_BAL"]
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit limit  - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1,kde=True,bins=200, color="red")
sns.distplot(class_0,kde=True,bins=200, color="green")
plt.show()
22/20:
#print(y_train)
X_train['X1'].str()
22/21:
#print(y_train)
X_train['X1'].describe()
22/22:
#print(y_train)
X_train.describe()
22/23:
X_train['X1']=X_train['X1'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]

plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/24:
df_train['X1']=df_train['X1'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]

plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/25:
df_train['X1']=df_train['X1'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/26:
df_train['X1']=df_train['X1'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y']==0]["X1"]
class_1 = df_train.loc[df_train['Y']==1]["X1"]
print(class_1)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/27:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]

class_0['X1']=class_0['X1'].astype(str).astype(int)
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/28:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
class_0['X1']=class_0['X1'].astype(str).astype(int)
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/29:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
22/30:
#print(y_train)
df_train.describe()
22/31:
#print(y_train)
df_train.describe()
22/32:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
class_0['X1']=class_0['X1'].astype(str).astype(int)
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
21/21:
class_0 = data_df.loc[data_df['default.payment.next.month'] == 0]["LIMIT_BAL"]
class_1 = data_df.loc[data_df['default.payment.next.month'] == 1]["LIMIT_BAL"]
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit limit  - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1,kde=True,bins=200, color="red")
sns.distplot(class_0,kde=True,bins=200, color="green")
plt.show()
22/33:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
class_0['X1']=class_0['X1'].astype(str).astype(int)
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/34:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
class_0['X1']=class_0['X1'].astype(str).astype(int)
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/35:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]

plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/36:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
class_0.describe()
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/37:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/38:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
print(class_1)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/39:
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
print(class_1)
class_0['X1']=class_0['X1'].astype(str).astype(int)
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/40:




df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
print(class_0)
print(class_1)

plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/41: class_0['X1'].value_counts().head(5)
22/42: class_0.value_counts().head(5)
22/43:
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/44:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/45:
df_train['X2']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/46:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/47:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, y="X2", x="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, y="X2", x="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/48:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
21/22:
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="SEX", y="LIMIT_BAL", hue="SEX",data=data_df, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="SEX", y="LIMIT_BAL", hue="SEX",data=data_df, palette="PRGn",showfliers=False)
plt.show();
22/49:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, y="X2", x="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, y="X2", x="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/50:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2",data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/51:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/52:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2",, y="Y",hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y",hue="X2", data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/53:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2", data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/54: df_train['X2'].describe()
22/55: df_train['X1'].describe()
22/56: df_train['X3'].describe()
22/57: print(df_train['X2'])
22/58: print(df_train['X1'])
22/59: print(df_train['X3'])
22/60: print(df_train['X2'])
22/61: print(df_train['X4'])
22/62: print(df_train['X5'])
22/63: print(df_train)
22/64:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
22/65: ## Creating our X and Y range of values from the training and testing datasets
22/66:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
22/67:
#print(y_train)
df_train.describe()
22/68: X_train.head()
22/69:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="Y", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="Y", hue="X2", data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/70: print(df_train)
22/71: print(df_train["Y"])
22/72:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
plt.show();
22/73: ax1.median()
22/74: ax1.median()
22/75:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
plt.show();
df_train.describe()
22/76:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]

plt.show();
df_train.describe()
22/77:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s.get_xticklabels()):
    s.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='w', weight='semibold')
plt.show();
df_train.describe()
22/78:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s.get_xticklabels()):
    s.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
plt.show();
df_train.describe()
22/79:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s1 = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s2 = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s1.get_xticklabels()):
    s1.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
for tick, label in zip(pos, s2.get_xticklabels()):
    s2.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
plt.show();
df_train.describe()
22/80:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s1 = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s2 = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s1.get_xticklabels()):
    s1.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
for tick, label in zip(pos, s2.get_xticklabels()):
    s2.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
plt.show();
22/81:
var = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels = corr.colums, yticklabels = corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()
22/82: print(X_train['BILL_AMT1'])
22/83:
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels = corr.colums, yticklabels = corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()
22/84:
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()
22/85:
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()
22/86: print(X_train['X12'])
22/87:
# things to consider are that the 0s here are for the atual amount or representing NaN

print(X_train['X6'].value_counts().nlargest(5))
fig, ax = plt.subplots(2, 3, sharex = 'col', sharey = 'row')

sns.distplot(X_train['X6'], ax = ax[0,0])
sns.distplot(X_train['X7'], ax = ax[0,1])
sns.distplot(X_train['X8'], ax = ax[0,2])
sns.distplot(X_train['X9'], ax = ax[1,0])
sns.distplot(X_train['X10'], ax = ax[1,1])
sns.distplot(X_train['X11'], ax = ax[1,2])
plt.title("Title")
22/88:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()
22/89:
cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption("Hover to magify")\
    .set_precision(2)\
    .set_table_styles(magnify())
22/90:
cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
22/91:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
22/92: #Correlation between Amount of Previous Payment
22/93: #Correlation between Amount of Previous Payment
22/94: #Amount of bill statement in April - September 2005
22/95: # Amount of bill statement in April - September 2005
22/96: ## Amount of bill statement in April - September 2005
22/97: ##Correlation between Amount of Previous Payment
22/98: ## Correlation between Amount of Previous Payment
22/99: ## Correlation between Amount of Previous Payment
22/100:
X_train['X18']=X_train['X18'].astype(str).astype(int)
X_train['X19']=X_train['X19'].astype(str).astype(int)
X_train['X20']=X_train['X20'].astype(str).astype(int)
X_train['X21']=X_train['X21'].astype(str).astype(int)
X_train['X22']=X_train['X22'].astype(str).astype(int)
X_train['X23']=X_train['X23'].astype(str).astype(int)
var = ['X18','X19','X20','X21','X22','X23']
plt.figure(figsize=(8,8))
plt.title('Amount of Previous Payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
22/101:
X_train['X18']=X_train['X18'].astype(str).astype(int)
X_train['X19']=X_train['X19'].astype(str).astype(int)
X_train['X20']=X_train['X20'].astype(str).astype(int)
X_train['X21']=X_train['X21'].astype(str).astype(int)
X_train['X22']=X_train['X22'].astype(str).astype(int)
X_train['X23']=X_train['X23'].astype(str).astype(int)
var = ['X18','X19','X20','X21','X22','X23']
plt.figure(figsize=(8,8))
plt.title('Amount of Previous Payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
22/102: ## History of past payment
22/103:
X_train['X6']=X_train['X6'].astype(str).astype(int)
X_train['X7']=X_train['X7'].astype(str).astype(int)
X_train['X8']=X_train['X8'].astype(str).astype(int)
X_train['X9']=X_train['X9'].astype(str).astype(int)
X_train['X10']=X_train['X10'].astype(str).astype(int)
X_train['X11']=X_train['X11'].astype(str).astype(int)

var = ['X6','X7','X8','X9','X10','X11']
plt.figure(figsize=(8,8))
plt.title('History of past payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
22/104: ## Credit card limit vs sex
22/105: ## 2.1.3 Education
22/106:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
22/107:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
22/108:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
boxplot_variation('X4','X5', 'X2',8)
22/109:
X_train['X4']=X_train['X4'].astype(str).astype(int)
X_train['X5']=X_train['X5'].astype(str).astype(int)
X_train['X6']=X_train['X6'].astype(str).astype(int)

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
boxplot_variation('X4','X5', 'X2',8)
22/110:
X_train['X4']=X_train['X4'].astype(str).astype(int)
X_train['X5']=X_train['X5'].astype(str).astype(int)
X_train['X6']=X_train['X6'].astype(str).astype(int)

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
22/111:
X_train['X4']=X_train['X4'].astype(str).astype(int)
X_train['X5']=X_train['X5'].astype(str).astype(int)
X_train['X6']=X_train['X6'].astype(str).astype(int)

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X6',8)
22/112: print(X_train['X4'])
22/113:


def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=df_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X3','X4', 'X5',8)
22/114:


def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X3','X4', 'X5',8)
22/115:


def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
22/116: ##Education, Age, Marriage
22/117:
boxplot_variation('X3','X5', 'X4',12)

""""
X3 = Education
X5 = Age
X4 = Marriage
"""
22/118: boxplot_variation('X3','X5', 'X4',12)
22/119: ## Age, Sex and Credit Card Limit
22/120:
boxplot_variation('X5','X1', 'X2',16)

""""

X5 = Age
X1 = Limit Balance
X2 = Sex
""""
22/121:
boxplot_variation('X5','X1', 'X2',16)

""""
X5 = Age
X1 = Limit Balance
X2 = Sex
""""
22/122: boxplot_variation('X5','X1', 'X2',16)
22/123: print(X_train['X1'])
22/124:
print(X_train['X5'].value_counts().nlargest(10))
print('\nNANs found:', sum(X_train['X5'] == 0))
"""plt.boxplot(dataset['X5'])
plt.title("Age Distribution")"""
22/125: print(df_train)
22/126:
data = pd.read_csv(r"C:\Users\Ken Yew\Notebooks\Making Predictions with Data and Python\Samples\UCI_Credit_Card.csv")
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )
print(cols)
f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/127:
output = 'Y'
cols = [f for f in X_train.columns]

f = pd.melt( df_train, id_vars=output, value_vars=cols)
print(f)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/128:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X5','X1', 'X2',16)

""""
X5 = Age
X1 = Limit Balance
X2 = Sex
""""
22/129:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X5','X1', 'X2',16)
22/130:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X4','X1', 'X3',12)
22/131:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")
22/132:
print(X_train['X1'].value_counts().nlargest(5))
print('\n Number of Missing Values: ', sum(X_train['X1'] == 0))
22/133:
print(X_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
22/134:
print("X1 (Limit Balance)")
print(X_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
22/135:
print("X1 (Limit Balance)")
print(\nX_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
22/136:
print("X1 (Limit Balance)\n")
print(X_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
22/137:
print("X2 (Sex)\n")
print(X_train['X2'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X2'] == 0))
22/138:
print("X3 (Education)\n")
print(X_train['X3'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X3'] == 0))
22/139:
print("X2 (Sex)\n")
print(X_train['X2'].value_counts().nlargest(5))
print("1: Male\n2: Female")
print('\nNumber of Missing Values: ', sum(X_train['X2'] == 0))
22/140:
print("X2 (Sex)\n")
print("1: Male\n2: Female\n")
print(X_train['X2'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X2'] == 0))
22/141:
print("X3 (Education)\n")
print("1: Graduate School\n2: University\n3: High School\n4:Others\n")
print(X_train['X3'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X3'] == 0))
22/142:
print("X4 (Marital Status)\n")
print("1: Married\n2: Single\n3: Others\n")
print(X_train['X3'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X3'] == 0))
22/143:
print("X4 (Marital Status)\n")
print("1: Married\n2: Single\n3: Others\n")
print(X_train['X4'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X4'] == 0))
22/144:
print("X4 (Marital Status)\n")
print("1: Married\n2: Single\n3: Others\n")
print(X_train['X4'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X4'] == 0))
22/145: print(X_train['X4'])
22/146:
print("X5 (Age)\n")
print(X_train['X5'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X5'] == 0))
22/147:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
22/148:
print(X_train['X1'].value_counts().nlargest(5))
print('\nNANs found:', sum(X_train['X1'] == 0))
22/149:
print("X1 (Limit Balance)\n")
print(X_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
22/150:
print("X2 (Sex)\n")
print("1: Male\n2: Female\n")
print(X_train['X2'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X2'] == 0))
22/151:
print("X3 (Education)\n")
print("1: Graduate School\n2: University\n3: High School\n4:Others\n")
print(X_train['X3'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X3'] == 0))
22/152:
print("X4 (Marital Status)\n")
print("1: Married\n2: Single\n3: Others\n")
print(X_train['X4'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X4'] == 0))
22/153:
print("X5 (Age)\n")
print(X_train['X5'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X5'] == 0))
"""plt.boxplot(dataset['X5'])
plt.title("Age Distribution")"""
22/154: plt.boxplot(X_train['X5'])
22/155:
plt.boxplot(X_train['X5'])
plt.title("Boxplot of Age")
22/156: 2.1.6 X6 - X11: History of Past Payment
22/157:
-1 = pay duly; 1 =

print("X6 - X11 (History of Past Payment)\n")
print("\n-1: payment delay for one month\n2: payment delay for two months\n.\n.\n.\n8: payment delay for eight months\n9: payment delay for nine months and above")
22/158:
-1 = pay duly; 1 =

print("X6 - X11 (History of Past Payment)\n")
print("\n-1: payment delay for one month\n")
22/159:
-1 = pay duly; 1 =

print("X6 - X11 (History of Past Payment)\n")
print("\n-1: \n")
22/160:
-1 = pay duly; 1 =

print("X6 - X11 (History of Past Payment)\n")
22/161:
print("X6 - X11 (History of Past Payment)\n")
print("\n-1: payment delay for one month\n2: payment delay for two months\n.\n.\n.\n8: payment delay for eight months\n9: payment delay for nine months and above")
22/162:
print("X6 - X11 (History of Past Payment)\n")
print("\n-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above")
22/163:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above")
22/164:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above")

print(X_trin[X6].value_counts().nlargest(5))
22/165:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above")

print(X_train[X6].value_counts().nlargest(5))
22/166:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above")

print(X_train['X6'].value_counts().nlargest(5))
22/167:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")

print(X_train['X6'].value_counts().nlargest(5))
22/168:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")
print("X6: (PAY_0)\n")
print(X_train['X6'].value_counts().nlargest(5))
22/169:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")
print("X6: (PAY_0)\n")
print(X_train['X6'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X6'] == 0))
22/170:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
22/171: ![title]("Desktop/11.png")
22/172: ![title]("img/11.png")
22/173: ![title]("img/11.png")
22/174: ![title](img/11.png)
22/175: ![title](Desktop/11.png)
22/176:
PATH = "/Users/Desktop"
Image(filename = PATH + "11.png", width=100, height=100)
22/177:
from IPython.display import Image
Image(filename='11.png')
22/178:
from IPython.Desktop import Image
Image(filename='11.png')
22/179:
from Desktop import Image
Image(filename='11.png')
22/180: 2) Ramón Díaz-Uriarte and Sara Alvarez de Andrés. “Gene selection and classification of microarray data using random forest” 2006
22/181:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")

print("X6: (PAY_0)\n")
print(X_train['X6'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X6'] == 0))

print("X7: (PAY_2)\n")
print(X_train['X7'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X7'] == 0))

print("X8: (PAY_3)\n")
print(X_train['X8'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X8'] == 0))

print("X9: (PAY_4)\n")
print(X_train['X9'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X9'] == 0))

print("X10: (PAY_5)\n")
print(X_train['X10'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X10'] == 0))

print("X11: (PAY_6)\n")
print(X_train['X11'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X11'] == 0))
22/182:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")

print("X6: (PAY_0)\n")
print(X_train['X6'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X6'] == 0))

print("\nX7: (PAY_2)\n")
print(X_train['X7'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X7'] == 0))

print("\nX8: (PAY_3)\n")
print(X_train['X8'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X8'] == 0))

print("\nX9: (PAY_4)\n")
print(X_train['X9'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X9'] == 0))

print("\nX10: (PAY_5)\n")
print(X_train['X10'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X10'] == 0))

print("\nX11: (PAY_6)\n")
print(X_train['X11'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X11'] == 0))
22/183:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("ADD TITLE")
22/184:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("")
22/185:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distribution Plot of Past Payments")
22/186:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])

plt.title("Distribution Plot of Past Payments")
22/187:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/188:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
plt.title("xx")
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/189:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("xx")
22/190:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("")
22/191:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=1.08)
22/192:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=100)
22/193:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=10)
22/194:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=3)
22/195:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2, {'center'})
22/196:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2)
22/197:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2.8)
22/198:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2.6)
22/199:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2.4)
22/200:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distri",y=2.4, loc = 'left')
22/201:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
plt.title("Distri")
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/202:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
plt.title("Distri")
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/203:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distribution Plot of Past Payments", y = 2.04, loc = 'left')
22/204:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distribution Plot of Past Payments", y = 2.06, loc = 'left')
22/205:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
22/206:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/207:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
22/208:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
sns.distplot(X_train['X6'], ax=ax[0,0])

sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/209:
plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
sns.distplot(X_train['X6'], ax=ax[0,0])

sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/210:

fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
sns.distplot(X_train['X6'], ax=ax[0,0])

sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
22/211:

fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

plt.title("Distribution Plot of Past Payments", y = 2.4, loc = 'left')
sns.distplot(X_train['X6'], ax=ax[0,0])

sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
fig.suptitle("Distribution Plot of Past Payments", fontsize=16)
22/212:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
fig.suptitle("Distribution Plot of Past Payments", fontsize=16)
22/213:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
22/214:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("sfsdfsfds")
22/215:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/216:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
ax.title.set_text('First Plot')
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
ax.title.set_text('First Plot')
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/217:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
plt.gca().set_title('title')
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
plt.gca().set_title('title')
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/218:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
plt.gca().set_title('first')
sns.countplot(x="X7", data=X_train, ax=ax[0,1])

sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/219:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
ax.set_title("Title for first plot")
sns.countplot(x="X7", data=X_train, ax=ax[0,1])

sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/220:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as pltpyp
"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
22/221:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
ax.set_title("Title for first plot")
sns.countplot(x="X7", data=X_train, ax=ax[0,1])

sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/222:
fig, ax = pltpyp.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
ax.set_title("Title for first plot")
sns.countplot(x="X7", data=X_train, ax=ax[0,1])

sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/223:
fig = plt.figure()
ax1 = fig.add_subplot(x="X6", data=X_train, ax=ax[0,0])
ax2 = fig.add_subplot(x="X7", data=X_train, ax=ax[0,1])
ax3 = fig.add_subplot(223)
ax4 = fig.add_subplot(224)
ax1.title.set_text('First Plot')
ax2.title.set_text('Second Plot')
ax3.title.set_text('Third Plot')
ax4.title.set_text('Fourth Plot')
plt.show()
22/224:
fig = pltpyp.figure()
ax1 = fig.add_subplot(x="X6", data=X_train, ax=ax[0,0])
ax2 = fig.add_subplot(x="X7", data=X_train, ax=ax[0,1])
ax3 = fig.add_subplot(223)
ax4 = fig.add_subplot(224)
ax1.title.set_text('First Plot')
ax2.title.set_text('Second Plot')
ax3.title.set_text('Third Plot')
ax4.title.set_text('Fourth Plot')
plt.show()
22/225:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Pasy Payments")
22/226:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Past Payments")
22/227:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X6'], ax=ax[0,0])
sns.distplot(X_train['X7'], ax=ax[0,1])
sns.distplot(X_train['X8'], ax=ax[0,2])
sns.distplot(X_train['X9'], ax=ax[1,0])
sns.distplot(X_train['X10'], ax=ax[1,1])
sns.distplot(X_train['X11'], ax=ax[1,2])
fig.suptitle("Distribution Plot of Past Payments", fontsize=16)
22/228:
print("X12 - X17: Amount of Bill Statement\n")

print("X12: (BILL_AMT1)\n")
print(X_train['X12'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X12'] == 0))

print("\nX13: (BILL_AMT2)\n")
print(X_train['X13'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X13'] == 0))

print("\nX14: (BILL_AMT3)\n")
print(X_train['X14'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X14'] == 0))

print("\nX15: (BILL_AMT4)\n")
print(X_train['X15'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X15'] == 0))

print("\nX16: (BILL_AMT5)\n")
print(X_train['X16'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X16'] == 0))

print("\nX17: (BILL_AMT6)\n")
print(X_train['X17'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X17'] == 0))
22/229:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(dat['X12'], ax=ax[0,0])
sns.distplot(dat['X13'], ax=ax[0,1])
sns.distplot(dat['X14'], ax=ax[0,2])
sns.distplot(dat['X15'], ax=ax[1,0])
sns.distplot(dat['X16'], ax=ax[1,1])
sns.distplot(dat['X17'], ax=ax[1,2])
22/230:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X12'], ax=ax[0,0])
sns.distplot(X_train['X13'], ax=ax[0,1])
sns.distplot(X_train['X14'], ax=ax[0,2])
sns.distplot(X_train['X15'], ax=ax[1,0])
sns.distplot(X_train['X16'], ax=ax[1,1])
sns.distplot(X_train['X17'], ax=ax[1,2])
22/231:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X12'], ax=ax[0,0])
sns.distplot(X_train['X13'], ax=ax[0,1])
sns.distplot(X_train['X14'], ax=ax[0,2])
sns.distplot(X_train['X15'], ax=ax[1,0])
sns.distplot(X_train['X16'], ax=ax[1,1])
sns.distplot(X_train['X17'], ax=ax[1,2])
fig.suptitle("Amount of Bill Statement")
22/232:
print("X18-X23: Amount of Previous Payment\n")

print("X18: (PAY_AMT1)\n")
print(X_train['X18'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X18'] == 0))

print("\nX19: (PAY_AMT2)\n")
print(X_train['X19'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X19'] == 0))

print("\nX20: (PAY_AMT3)\n")
print(X_train['X20'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X20'] == 0))

print("\nX21: (PAY_AMT4)\n")
print(X_train['X21'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X21'] == 0))

print("\nX22: (PAY_AMT5)\n")
print(X_train['X22'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X22'] == 0))

print("\nX23: (PAY_AMT5)\n")
print(X_train['X23'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X23'] == 0))
22/233:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X18'], ax=ax[0,0])
sns.distplot(X_train['X19'], ax=ax[0,1])
sns.distplot(X_train['X20'], ax=ax[0,2])
sns.distplot(X_train['X21'], ax=ax[1,0])
sns.distplot(X_train['X22'], ax=ax[1,1])
sns.distplot(X_train['X23'], ax=ax[1,2])
fig.suptitle("Amount of Previous Payment")
22/234:
data = pd.read_csv(r"C:\Users\Ken Yew\Notebooks\Making Predictions with Data and Python\Samples\UCI_Credit_Card.csv")
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )
print(cols)
f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/235:
output = 'Y'
cols = [f for f in X_train.columns]

f = pd.melt( df_train, id_vars=output, value_vars=cols)
print(f)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/236: ## Defalut Payment Next Month, Education, Sex
22/237:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default by Education and Sex")
22/238:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education and categorised by Sex")
22/239:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education (categorised by Sex)")
22/240:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education (categorised by Sex)")
22/241:
sns.boxplot(x="Y", y="X5", data=dat)
plt.title("Distribution of Default by Age")
22/242:
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Distribution of Default by Age")
22/243: print(X_train("X5"))
22/244: print(df_train("X5"))
22/245: print(df_train('X5'))
22/246: print(df_train("X1"))
22/247: print(df_train["X5"])
22/248:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Distribution of Default by Age")
22/249:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Boxplot of Default Payment Next Month against Age")
22/250: ## Default Payment Next Month, Sex, Age
22/251:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.violinplot(x="Y", y="X5", hue="X2", data=df_train, split=True)
plt.title("Distribution of Default by Sex and Age")
22/252:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Boxplot of Age against Default Payment Next Month")
22/253:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Education agianst Default Payment Next Month (categorised by Sex)")
22/254:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education (categorised by Sex)")
22/255:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.violinplot(x="Y", y="X5", hue="X2", data=df_train, split=True)
plt.title("Violinplot of Age against Default Payment Next Month (categorised by Sex)")
22/256:
dataset['Y'].value_counts()
sns.barplot (x = "X3", y = "Y", hue = "X2", data = dataset)
title("Default by Education and Sex")
22/257:

corr = df_train.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
corr_mask = np.zeros_like(corr, dtype=np.bool)
corr_mask[np.triu_indices_from(corr_mask)] = True
corr_f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/258: print(df_train)
22/259:

corr = X_train.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
corr_mask = np.zeros_like(corr, dtype=np.bool)
corr_mask[np.triu_indices_from(corr_mask)] = True
corr_f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/260:

corr = df_train.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
corr_mask = np.zeros_like(corr, dtype=np.bool)
corr_mask[np.triu_indices_from(corr_mask)] = True
corr_f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/261:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
corr = df_train.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
corr_mask = np.zeros_like(corr, dtype=np.bool)
corr_mask[np.triu_indices_from(corr_mask)] = True
corr_f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/262:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)




corr = df_train.corr()
cmap = sns.diverging_palette(220, 10, as_cmap=True)
corr_mask = np.zeros_like(corr, dtype=np.bool)
corr_mask[np.triu_indices_from(corr_mask)] = True
corr_f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/263:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

# Compute the correlation matrix
corr = df_train.corr()
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correlation ratio
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
22/264:
def plot_data_distribution(subsets, labels=None):
fig, axes= plt.subplots(nrows=5, ncols=5, figsize = (15,15))
for index, (ax, name) in enumerate(zip(axes.ravel(), subsets[0].columns
[1:])):
attribute_values = []
for data in subsets:
attribute_values.append(data[name].values)
ax.hist(attribute_values, label=labels)
ax.set_title(r'%s' % column_mapping[name])
fig.tight_layout()
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='center right')
plt.show()
22/265:
def plot_data_distribution(subsets, labels=None):
    fig, axes= plt.subplots(nrows=5, ncols=5, figsize = (15,15))
for index, (ax, name) in enumerate(zip(axes.ravel(), subsets[0].columns
[1:])):
attribute_values = []
for data in subsets:
attribute_values.append(data[name].values)
ax.hist(attribute_values, label=labels)
ax.set_title(r'%s' % column_mapping[name])
fig.tight_layout()
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='center right')
plt.show()
22/266:
def plot_data_distribution(subsets, labels=None):
    fig, axes= plt.subplots(nrows=5, ncols=5, figsize = (15,15))
for index, (ax, name) in enumerate(zip(axes.ravel(), subsets[0].columns
[1:])):
    attribute_values = []
for data in subsets:
    attribute_values.append(data[name].values)
    ax.hist(attribute_values, label=labels)
    ax.set_title(r'%s' % column_mapping[name])
fig.tight_layout()
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='center right')
plt.show()
22/267:
def label_bar(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01

    for bar in bars:
        text = text_format.format(bar.get_height())
        text_x = bar.get_x() + bar.get_width() / 2
        if is_inside:
            text_y = bar.get_height() - distance
        else:
            text_y = bar.get_height() + distance

        ax.text(text_x, text_y, text, ha='center', va='bottom', **kwargs)


def label_barh(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each horizontal bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01


    for bar in bars:
        text = text_format.format(bar.get_width())
        if is_inside:
            text_x = bar.get_width() - distance
        else:
            text_x = bar.get_width() + distance
        text_y = bar.get_y() + bar.get_height() / 2

        ax.text(text_x, text_y, text, va='center', **kwargs)
        
        
bars = ax.bar(y_train['Y'], width=0.5, align="center")
value_format = "{:.1%}"  # displaying values as percentage with one fractional digit
label_bar(ax, bars, value_format, is_inside=True, color="white")
22/268:
def label_bar(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01

    for bar in bars:
        text = text_format.format(bar.get_height())
        text_x = bar.get_x() + bar.get_width() / 2
        if is_inside:
            text_y = bar.get_height() - distance
        else:
            text_y = bar.get_height() + distance

        ax.text(text_x, text_y, text, ha='center', va='bottom', **kwargs)


def label_barh(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each horizontal bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01


    for bar in bars:
        text = text_format.format(bar.get_width())
        if is_inside:
            text_x = bar.get_width() - distance
        else:
            text_x = bar.get_width() + distance
        text_y = bar.get_y() + bar.get_height() / 2

        ax.text(text_x, text_y, text, va='center', **kwargs)
        
        
bars = ax.bar(y_train,Y, width=0.5, align="center")
value_format = "{:.1%}"  # displaying values as percentage with one fractional digit
label_bar(ax, bars, value_format, is_inside=True, color="white")
22/269:
def label_bar(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01

    for bar in bars:
        text = text_format.format(bar.get_height())
        text_x = bar.get_x() + bar.get_width() / 2
        if is_inside:
            text_y = bar.get_height() - distance
        else:
            text_y = bar.get_height() + distance

        ax.text(text_x, text_y, text, ha='center', va='bottom', **kwargs)


def label_barh(ax, bars, text_format, is_inside=True, **kwargs):
    """
    Attach a text label to each horizontal bar displaying its y value
    """
    max_y_value = max(bar.get_height() for bar in bars)
    if is_inside:
        distance = max_y_value * 0.05
    else:
        distance = max_y_value * 0.01


    for bar in bars:
        text = text_format.format(bar.get_width())
        if is_inside:
            text_x = bar.get_width() - distance
        else:
            text_x = bar.get_width() + distance
        text_y = bar.get_y() + bar.get_height() / 2

        ax.text(text_x, text_y, text, va='center', **kwargs)
        
        
bars = ax.bar(y_train,'Y', width=0.5, align="center")
value_format = "{:.1%}"  # displaying values as percentage with one fractional digit
label_bar(ax, bars, value_format, is_inside=True, color="white")
22/270:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")
for i in ax.patches:
    # get_x pulls left or right; get_height pushes up or down
    ax.text(i.get_x()+.04, i.get_height()+12000, \
            str(round((i.get_height()), 2)), fontsize=11, color='dimgrey',
                rotation=45)
22/271:
output = df_train'Y'

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/272:
output = df_train['Y']

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/273:
output = df_train['Y']

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/274:
output = df_train['Y']

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/275:
output = df_train['Y']

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]
cols.remove( output )

f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/276:


# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]


f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
22/277:


# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]


f = pd.melt( df_train, id_vars=df_train['Y'], value_vars=cols)
g = sns.FacetGrid( f, hue=df_train['Y'], col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
24/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
24/2:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
24/3: ## Creating our X and Y range of values from the training and testing datasets
24/4:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
24/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
24/6:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
24/7: ## Creating our X and Y range of values from the training and testing datasets
24/8:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
24/9:
#print(y_train)
df_train.describe()
24/10:

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]

f = pd.melt( df_train, id_vars=df_train['Y'], value_vars=cols)
g = sns.FacetGrid( f, hue=df_train['Y'], col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
24/11:

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]
cols.remove("Y")
f = pd.melt( df_train, id_vars=df_train['Y'], value_vars=cols)
g = sns.FacetGrid( f, hue=df_train['Y'], col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
24/12:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
""""
X4 = Marriage
X5 = Age
X2 = Sex
""""
24/13:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
24/14:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
24/15:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
24/16: ## Creating our X and Y range of values from the training and testing datasets
24/17:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
24/18:
#print(y_train)
df_train.describe()
24/19: X_train.head()
24/20: X_train.describe(include='all')
24/21:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
24/22: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
24/23: X_train.head()
24/24: y_train.head()
24/25:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s1 = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s2 = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s1.get_xticklabels()):
    s1.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
for tick, label in zip(pos, s2.get_xticklabels()):
    s2.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
plt.show();
24/26:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
24/27:
boxplot_variation('X3','X5', 'X4',12)

""""
X3 = Education
X5 = Age
X4 = Marriage
"""
24/28: boxplot_variation('X3','X5', 'X4',12)
25/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
25/2:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
25/3: ## Creating our X and Y range of values from the training and testing datasets
25/4:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
25/5:
#print(y_train)
df_train.describe()
25/6: X_train.head()
25/7: X_train.describe(include='all')
25/8:
"""temp = dataset["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()"""
25/9:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
25/10:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_test:
    if data_test[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
25/11: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
25/12: X_train.head()
25/13: y_train.head()
25/14:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
# y = mlab.normpdf( bins, mu, sigma)
l = plt.plot(bins, y, 'r--', linewidth=1)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/15:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
# y = mlab.normpdf( bins, mu, sigma)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/16:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
# y = mlab.normpdf( bins, mu, sigma)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.grid(True)

plt.show()
25/17:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='navy', alpha=0.75)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.grid(True)

plt.show()
25/18:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='navy', alpha=0.75)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/19:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/20:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], df_train['Y'], normed=1, facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/21:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], df_train['Y'], facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/22:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


x = 1 + 5*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 10, facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/23:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


x = 1 + 5*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 1, facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/24:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


x = 1 + 5*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 15, facecolor='navy')

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'hahaha')
plt.grid(True)

plt.show()
25/25:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

figs, axs = plt.subplot(1,1,figsize(12,10))
axs = axs.ravel()

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
y = mlab.normpdf( bins, mu, sigma)
l = plt.plot(bins, y, 'r--', linewidth=1)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/26:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
y = mlab.normpdf( bins, mu, sigma)
l = plt.plot(bins, y, 'r--', linewidth=1)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/27:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
l = plt.plot(bins, y, 'r--', linewidth=1)

plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/28:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/29:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/30:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], 50, normed=1, facecolor='green', alpha=0.75)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/31:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], 1, facecolor='green', alpha=0.75)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/32:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'])


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')
plt.axis([40, 160, 0, 0.03])
plt.grid(True)

plt.show()
25/33:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


mu, sigma = 100, 15
x = mu + sigma*np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'])


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')

plt.grid(True)

plt.show()
25/34:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


print(df_train['X1'])
# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'])


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')

plt.grid(True)

plt.show()
25/35:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


print(df_train['X1'])
# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], normed=True, bins=30)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')

plt.grid(True)

plt.show()
25/36:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


print(df_train['X1'])
# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], normed=True, bins=60)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')

plt.grid(True)

plt.show()
25/37:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
n, bins, patches = plt.hist(df_train['X1'], normed=True, bins=60)


plt.xlabel('Smarts')
plt.ylabel('Probability')
plt.title(r'???')

plt.grid(True)

plt.show()
25/38:
import seaborn as sns

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in data.columns if data.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/39:
import seaborn as sns

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]
cols.remove( "ID")
cols.remove( output )

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/40:
import seaborn as sns

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]
cols.remove( output )

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/41:
import seaborn as sns

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]

f = pd.melt( data, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/42:
import seaborn as sns

df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)


# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]

f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/43:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

# the histogram of the data
output = 'default.payment.next.month'

# Let's do a little EDA
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object"]

f = pd.melt( df_train, id_vars=output, value_vars=cols)
g = sns.FacetGrid( f, hue=output, col="variable", col_wrap=5, sharex=False, sharey=False )
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/44:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

x = np.random.normal(size = 1000)
plt.hist(df_train['X1'], normed=True, bins=30)
25/45:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], normed=True, bins=300)
25/46:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], density=True, bins=300)
25/47:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], density=True, bins=300)
25/48:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], density=True, bins=300)
plot.show()
25/49:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], density=True, bins=300)
plt.show()
25/50:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.hist(df_train['X1'], density=True, bins=250)
plt.show()
25/51:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

for i in range(1,24):
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
    plt.show()
25/52:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr = plt.subplots(2, 2)
for i in range(1,24):
    cname = 'X' + str(i)
    axarr.hist(df_train[cname], density=True, bins=250)
    axarr.show()
25/53:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr = plt.subplots(5, 5)
for i in range(1,24):
    cname = 'X' + str(i)
    axarr.hist(df_train[cname], density=True, bins=250)
    axarr.show()
25/54:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.subplots(5, 5)
for i in range(1,24):
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
    plt.show()
25/55:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr =plt.subplots(5, 5)
for i in range(1,24):
    cname = 'X' + str(i)
    f.hist(df_train[cname], density=True, bins=250)
    f.show()
25/56:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr =plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    if j == 5:
        i+= 1
        j = 0
        
    cname = 'X' + str(i)
    axarr[i, j].hist(df_train[cname], density=True, bins=250)
    axarr.show()
    j += 1
25/57:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr =plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    if j == 5:
        i+= 1
        j = 0
        
    cname = 'X' + str(i)
    axarr[i, j].hist(df_train[cname], density=True, bins=250)
    axarr[i,j].show()
    j += 1
25/58:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

f, axarr =plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    if j == 5:
        i+= 1
        j = 0
        
    cname = 'X' + str(i)
    axarr[i, j].hist(df_train[cname], density=True, bins=250)
    j += 1
25/59:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
25/60:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
25/61:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

plt.subplots(5, 5)
i,j=0,0
for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
25/62:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
25/63:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname], density=True, bins=250)
25/64:
import seaborn as sns
'''
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
'''

for i in range(1,24):
    
    cname = 'X' + str(i)
    plt.hist(df_train[cname] , bins=250)
    plt.show()
25/65:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

# Compute the correlation matrix
corr = df_train.corr()
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correlation ratio
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
25/66:
dataset['X3'].loc[dataset['X3'] == 6] = 5
dataset['Y'].value_counts()
25/67:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
25/68:
print("X6 - X11 (History of Past Payment)\n")
print("-1: payment delay for one month\n 2: payment delay for two months\n.\n.\n.\n 8: payment delay for eight months\n 9: payment delay for nine months and above\n")

print("X6: (PAY_0)\n")
print(X_train['X6'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X6'] == 0))

print("\nX7: (PAY_2)\n")
print(X_train['X7'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X7'] == 0))

print("\nX8: (PAY_3)\n")
print(X_train['X8'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X8'] == 0))

print("\nX9: (PAY_4)\n")
print(X_train['X9'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X9'] == 0))

print("\nX10: (PAY_5)\n")
print(X_train['X10'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X10'] == 0))

print("\nX11: (PAY_6)\n")
print(X_train['X11'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X11'] == 0))
25/69:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.countplot(x="X6", data=X_train, ax=ax[0,0])
sns.countplot(x="X7", data=X_train, ax=ax[0,1])
sns.countplot(x="X8", data=X_train, ax=ax[0,2])
sns.countplot(x="X9", data=X_train, ax=ax[1,0])
sns.countplot(x="X10", data=X_train, ax=ax[1,1])
sns.countplot(x="X11", data=X_train, ax=ax[1,2])
fig.suptitle("History of Past Payments")
25/70:
def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
25/71:
X_train['X4']=X_train['X4'].astype(str).astype(int)
X_train['X5']=X_train['X5'].astype(str).astype(int)
X_train['X2']=X_train['X2'].astype(str).astype(int)

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
25/72: boxplot_variation('X3','X5', 'X4',12)
25/73:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X5','X1', 'X2',16)
25/74:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X4','X1', 'X3',12)
25/75:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education (categorised by Sex)")
25/76:
# Let's do a little data visualisation
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(data[cols])
25/77:
# Let's do a little data visualisation
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
25/78:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

#loggedData = data.drop(columns = cols[:11])
#print(list(loggedData))

# We plot some distibution plot
f = pd.melt(data, id_vars='default payment next month', value_vars= cols)
g = sns.FacetGrid( f, hue='default payment next month', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/79:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

#loggedData = data.drop(columns = cols[:11])
#print(list(loggedData))

# We plot some distibution plot
f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/80:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

# Compute the correlation matrix
corr = df_train.corr()
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correlation ratio
sns.heatmap(corr, mask=corr_mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
25/81:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

# Compute the correlation matrix
corr = df_train.corr()
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correlation ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
25/82:
tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})
tmp = tmp.sort_values(by='Feature importance',ascending=False)
plt.figure(figsize = (7,4))
plt.title('Features importance',fontsize=14)
s = sns.barplot(x='Feature',y='Feature importance',data=tmp)
s.set_xticklabels(s.get_xticklabels(),rotation=90)
plt.show()
25/83:
X_train['X6']=X_train['X6'].astype(str).astype(int)
X_train['X7']=X_train['X7'].astype(str).astype(int)
X_train['X8']=X_train['X8'].astype(str).astype(int)
X_train['X9']=X_train['X9'].astype(str).astype(int)
X_train['X10']=X_train['X10'].astype(str).astype(int)
X_train['X11']=X_train['X11'].astype(str).astype(int)

var = ['X6','X7','X8','X9','X10','X11']
plt.figure(figsize=(8,8))
plt.title('History of past payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
25/84:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
25/85:
X_train['X18']=X_train['X18'].astype(str).astype(int)
X_train['X19']=X_train['X19'].astype(str).astype(int)
X_train['X20']=X_train['X20'].astype(str).astype(int)
X_train['X21']=X_train['X21'].astype(str).astype(int)
X_train['X22']=X_train['X22'].astype(str).astype(int)
X_train['X23']=X_train['X23'].astype(str).astype(int)
var = ['X18','X19','X20','X21','X22','X23']
plt.figure(figsize=(8,8))
plt.title('Amount of Previous Payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
25/86:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X3']=df_train['X3'].astype(str).astype(int)
df_train['X4']=df_train['X4'].astype(str).astype(int)
df_train['X5']=df_train['X5'].astype(str).astype(int)
df_train['X6']=df_train['X6'].astype(str).astype(int)
df_train['X7']=df_train['X7'].astype(str).astype(int)
df_train['X8']=df_train['X8'].astype(str).astype(int)
df_train['X9']=df_train['X9'].astype(str).astype(int)
df_train['X10']=df_train['X10'].astype(str).astype(int)
df_train['X11']=df_train['X11'].astype(str).astype(int)
df_train['X12']=df_train['X12'].astype(str).astype(int)
df_train['X13']=df_train['X13'].astype(str).astype(int)
df_train['X14']=df_train['X14'].astype(str).astype(int)
df_train['X15']=df_train['X15'].astype(str).astype(int)
df_train['X16']=df_train['X16'].astype(str).astype(int)
df_train['X17']=df_train['X17'].astype(str).astype(int)
df_train['X18']=df_train['X18'].astype(str).astype(int)
df_train['X19']=df_train['X19'].astype(str).astype(int)
df_train['X20']=df_train['X20'].astype(str).astype(int)
df_train['X21']=df_train['X21'].astype(str).astype(int)
df_train['X22']=df_train['X22'].astype(str).astype(int)
df_train['X23']=df_train['X23'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)

# Compute the correlation matrix
corr = df_train.corr()
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correlation ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.3, center=0, annot=True,
fmt=".1f",
square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()
25/87:

from ipywidgets import *
import pandas as pd
df = pd.DataFrame({"x":[1,2,3], "y":[6,4,3], "z":["testing","pretty","tables"], "f":[0.023432, 0.234321,0.5555]})
pt = PrettyTable(df)
pt
25/88:

from ipywidgets import *
import pandas as pd
df = pd.DataFrame({"x":[1,2,3], "y":[6,4,3], "z":["testing","pretty","tables"], "f":[0.023432, 0.234321,0.5555]})
pt = PrettyTable(df)
pt
25/89:

from ipywidgets import *
import pandas as pd
df = pd.DataFrame({"x":[1,2,3], "y":[6,4,3], "z":["testing","pretty","tables"], "f":[0.023432, 0.234321,0.5555]})
pt = PrettyTable(df)
pt
25/90: ## 3.1 Distribution Plot of All Variables in Dataset
25/91:
def plot_data_distribution(subsets, labels=None):
    fig, axes= plt.subplots(nrows=5, ncols=5, figsize = (15,15))
for index, (ax, name) in enumerate(zip(axes.ravel(), subsets[0].columns
[1:])):
    attribute_values = []
for data in subsets:
    attribute_values.append(data[name].values)
    ax.hist(attribute_values, label=labels)
    ax.set_title(r'%s' % column_mapping[name])
fig.tight_layout()
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='center right')
plt.show()
29/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import tensorflow as tf
"""
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec
"""


import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
import tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec
"""


import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/4:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
29/5:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
29/6: X_train.describe(include='all')
29/7:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
29/8:
f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
29/9:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
25/92:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
25/93:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
25/94: ## Creating our X and Y range of values from the training and testing datasets
25/95:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
25/96:
#print(y_train)
df_train.describe()
25/97: X_train.head()
25/98: X_train.describe(include='all')
25/99:
"""temp = dataset["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()"""
25/100:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
25/101:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_test:
    if data_test[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
25/102: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
25/103: X_train.head()
25/104: y_train.head()
25/105:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
25/106:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
29/10:
#cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
#del cols[-1:]
cols =['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23']
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
29/11:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
29/12:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
29/13:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
29/14:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
import tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec
"""


import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/15:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
29/16:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
29/17: X_train.describe(include='all')
29/18:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
29/19:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
30/1:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
30/2:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
30/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
30/4:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
30/5: ## Creating our X and Y range of values from the training and testing datasets
30/6:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
30/7:
#print(y_train)
df_train.describe()
30/8:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
30/9:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
30/10:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
30/11:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
30/12: ## Creating our X and Y range of values from the training and testing datasets
30/13:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
30/14:
#print(y_train)
df_train.describe()
30/15: X_train.head()
30/16: X_train.describe(include='all')
30/17:
"""temp = dataset["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()"""
30/18:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
30/19:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_test:
    if data_test[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
30/20: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
30/21: X_train.head()
30/22: y_train.head()
30/23:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
30/24:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
30/25:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")


ax = losses[['attacker_size', 'defender_size']].plot(kind='bar',
              figsize=(15,7), color=['dodgerblue', 'slategray'], fontsize=13);
ax.set_alpha(0.8)
for i in ax.patches:
    # get_x pulls left or right; get_height pushes up or down
    ax.text(i.get_x()+.04, i.get_height()+12000, \
            str(round((i.get_height()), 2)), fontsize=11, color='black',
                rotation=45)
30/26:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns
"""
from tensorflow as tf
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, Imputer, LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
"""

import matplotlib.gridspec as gridspec

import warnings
warnings.filterwarnings("ignore")
30/27:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
30/28: ## Creating our X and Y range of values from the training and testing datasets
30/29:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])

X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
30/30:
#print(y_train)
df_train.describe()
30/31: X_train.head()
30/32: X_train.describe(include='all')
30/33:
"""temp = dataset["default.payment.next.month"].value_counts()
df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})
plt.figure(figsize = (6,6))
plt.title('Default Credit Card Clients - target value - data unbalance\n (Default = 0, Not Default = 1)')
sns.set_color_codes("pastel")
sns.barplot(x = 'default.payment.next.month', y="values", data=df)
locs, labels = plt.xticks()
plt.show()"""
30/34:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_train:
    if data_train[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
30/35:
# Checking if there are any NaN values, if there are , the data will be sanitised.
for column in data_test:
    if data_test[column].isnull().values.any():
        print ("NaN value(s) detected in " + column)
    else: 
        print ("{} doesn't have any null values".format(column))
30/36: #X_train.query('(X3 == 0) | (X3 > 4) | (X4 == 0) | ' + ' | '.join('(X%d == -2)' % i for i in range(6,12))).describe()
30/37: X_train.head()
30/38: y_train.head()
30/39:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
print(df_train[cols])
30/40:
print('Columms:', cols)
print('Basic',basicData)
print('Money',moneyData)

f = pd.melt(df_train, id_vars='Y', value_vars= cols)
g = sns.FacetGrid( f, hue='Y', col="variable", col_wrap=5, sharex=False, sharey=False)
g = g.map( sns.distplot, "value", kde=True).add_legend()
29/20:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

"""

import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/21:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
29/22:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
29/23: X_train.describe(include='all')
29/24:
cols = [ f for f in df_train.columns if df_train.dtypes[ f ] != "object" and f!= 'ID']
del cols[-1:]
basicData = cols[1:11]
moneyData = [x for x in cols if x not in basicData]
29/25:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")


ax = losses[['attacker_size', 'defender_size']].plot(kind='bar',
              figsize=(15,7), color=['dodgerblue', 'slategray'], fontsize=13);
ax.set_alpha(0.8)
for i in ax.patches:
    # get_x pulls left or right; get_height pushes up or down
    ax.text(i.get_x()+.04, i.get_height()+12000, \
            str(round((i.get_height()), 2)), fontsize=11, color='black',
                rotation=45)
29/26:
pd.value_counts(y_train['Y']).plot.bar()
plt.title("Credit Card Default Counts")
29/27:
print("X1 (Limit Balance)\n")
print(X_train['X1'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X1'] == 0))
29/28: X_train['X1'].value_counts().shape
29/29:
df_train['X1']=df_train['X1'].astype(str).astype(int)
df_train['Y']=df_train['Y'].astype(str).astype(int)
class_0 = df_train.loc[df_train['Y'] == 0]["X1"]
class_1 = df_train.loc[df_train['Y'] == 1]["X1"]
plt.figure(figsize = (14,6))
plt.title('Default amount of credit card limit - grouped by Payment Next Month (Density Plot)')
sns.set_color_codes("pastel")
sns.distplot(class_1, kde = True, bins = 100, color = "red")
sns.distplot(class_0, kde = True, bins = 100, color = "green")
plt.show()
29/30: class_0.value_counts().head(5)
29/31:
print("X2 (Sex)\n")
print("1: Male\n2: Female\n")
print(X_train['X2'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X2'] == 0))
29/32:
print("X3 (Education)\n")
print("1: Graduate School\n2: University\n3: High School\n4:Others\n")
print(X_train['X3'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X3'] == 0))
29/33:
print("X4 (Marital Status)\n")
print("1: Married\n2: Single\n3: Others\n")
print(X_train['X4'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X4'] == 0))
29/34:
print("X5 (Age)\n")
print(X_train['X5'].value_counts().nlargest(5))
print('\nNumber of Missing Values: ', sum(X_train['X5'] == 0))
29/35:
plt.boxplot(X_train['X5'])
plt.title("Boxplot of Age")
29/36:
df_train['X5']=df_train['X5'].astype(str).astype(int)
plt.boxplot(X_train['X5'])
plt.title("Boxplot of Age")
29/37:
X_train['X5']=X_train['X5'].astype(str).astype(int)
plt.boxplot(X_train['X5'])
plt.title("Boxplot of Age")
29/38:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X12'], ax=ax[0,0])
sns.distplot(X_train['X13'], ax=ax[0,1])
sns.distplot(X_train['X14'], ax=ax[0,2])
sns.distplot(X_train['X15'], ax=ax[1,0])
sns.distplot(X_train['X16'], ax=ax[1,1])
sns.distplot(X_train['X17'], ax=ax[1,2])
fig.suptitle("Amount of Bill Statement")
29/39:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X12'].astype(str).astype(int)
X_train['X14']=X_train['X12'].astype(str).astype(int)
X_train['X15']=X_train['X12'].astype(str).astype(int)
X_train['X16']=X_train['X12'].astype(str).astype(int)
X_train['X17']=X_train['X12'].astype(str).astype(int)


fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X12'], ax=ax[0,0])
sns.distplot(X_train['X13'], ax=ax[0,1])
sns.distplot(X_train['X14'], ax=ax[0,2])
sns.distplot(X_train['X15'], ax=ax[1,0])
sns.distplot(X_train['X16'], ax=ax[1,1])
sns.distplot(X_train['X17'], ax=ax[1,2])
fig.suptitle("Amount of Bill Statement")
29/40:
fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X18'], ax=ax[0,0])
sns.distplot(X_train['X19'], ax=ax[0,1])
sns.distplot(X_train['X20'], ax=ax[0,2])
sns.distplot(X_train['X21'], ax=ax[1,0])
sns.distplot(X_train['X22'], ax=ax[1,1])
sns.distplot(X_train['X23'], ax=ax[1,2])
fig.suptitle("Amount of Previous Payment")
29/41:
X_train['X18']=X_train['X18'].astype(str).astype(int)
X_train['X19']=X_train['X19'].astype(str).astype(int)
X_train['X20']=X_train['X20'].astype(str).astype(int)
X_train['X21']=X_train['X21'].astype(str).astype(int)
X_train['X22']=X_train['X22'].astype(str).astype(int)
X_train['X23']=X_train['X23'].astype(str).astype(int)




fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')
sns.distplot(X_train['X18'], ax=ax[0,0])
sns.distplot(X_train['X19'], ax=ax[0,1])
sns.distplot(X_train['X20'], ax=ax[0,2])
sns.distplot(X_train['X21'], ax=ax[1,0])
sns.distplot(X_train['X22'], ax=ax[1,1])
sns.distplot(X_train['X23'], ax=ax[1,2])
fig.suptitle("Amount of Previous Payment")
29/42:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s1 = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s2 = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s1.get_xticklabels()):
    s1.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
for tick, label in zip(pos, s2.get_xticklabels()):
    s2.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='b', weight='semibold')
plt.show();
29/43:
df_train['X2']=df_train['X2'].astype(str).astype(int)
df_train['X1']=df_train['X1'].astype(str).astype(int)
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s1 = sns.boxplot(ax = ax1, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=True)
s2 = sns.boxplot(ax = ax2, x="X2", y="X1", hue="X2", data=df_train, palette="PRGn",showfliers=False)
medians = df_train.groupby(['X2'])['X1'].median().values
median_labels = [str(np.round(s, 2)) for s in medians]
pos = range(len(medians))
for tick, label in zip(pos, s1.get_xticklabels()):
    s1.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='red', weight='semibold')
for tick, label in zip(pos, s2.get_xticklabels()):
    s2.text(pos[tick], medians[tick] + 0.5, median_labels[tick], horizontalalignment = 'center', size = 'x-small', color='red', weight='semibold')
plt.show();
29/44:
X_train['X4']=X_train['X4'].astype(str).astype(int)
X_train['X5']=X_train['X5'].astype(str).astype(int)
X_train['X2']=X_train['X2'].astype(str).astype(int)

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=X_train, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();
    
boxplot_variation('X4','X5', 'X2',8)
29/45: boxplot_variation('X3','X5', 'X4',12)
29/46:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X5','X1', 'X2',16)
29/47:
X_train['X1']=X_train['X1'].astype(str).astype(int)
boxplot_variation('X4','X1', 'X3',12)
29/48:
df_train['Y'].value_counts()
sns.barplot(x="X3", y="Y", hue="X2", data=df_train)
plt.title("Default Payment Next Month against Education (categorised by Sex)")
29/49:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Boxplot of Age against Default Payment Next Month")
29/50:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.boxplot(x="Y", y="X5", data=df_train)
plt.title("Boxplot of Age against Default Payment Next Month")
29/51:
df_train['X5']=df_train['X5'].astype(str).astype(int)
sns.violinplot(x="Y", y="X5", hue="X2", data=df_train, split=True)
plt.title("Violinplot of Age against Default Payment Next Month (categorised by Sex)")
29/52:
X_train['X6']=X_train['X6'].astype(str).astype(int)
X_train['X7']=X_train['X7'].astype(str).astype(int)
X_train['X8']=X_train['X8'].astype(str).astype(int)
X_train['X9']=X_train['X9'].astype(str).astype(int)
X_train['X10']=X_train['X10'].astype(str).astype(int)
X_train['X11']=X_train['X11'].astype(str).astype(int)

var = ['X6','X7','X8','X9','X10','X11']
plt.figure(figsize=(8,8))
plt.title('History of past payment (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
29/53:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
29/54:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)

var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
29/55:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)

var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
29/56:
pca = PCA(n_components=2)
X_scaled = StandardScaler().fit_transform(X_train.values)
projected = pca.fit_transform(X_scaled)
print(projected[:, 0])
print("Training data shape :", X_train.shape)
print("Projected data shape :", projected.shape)
print("Explained variance :", np.sum(pca.explained_variance_ratio_))
29/57:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
"""
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

"""

import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/58:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
"""
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

"""

import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/59:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

"""
import tensorflow as tf

from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,StandardScaler, RobustScaler, Imputer, LabelEncoder, PolynomialFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate
from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding, TSNE
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import \
    VotingClassifier, BaggingClassifier, RandomForestClassifier, \
    AdaBoostClassifier, GradientBoostingClassifier, GradientBoostingRegressor

from sklearn.cross_validation import train_test_split
from sklearn.utils import shuffle
import matplotlib.gridspec as gridspec

"""

import warnings
warnings.filterwarnings("ignore")

MODEL_NAME = []
ACCURACY = []    
AUROC_SCORE = []
PRECISION = []
AVG_PRECISION = []
F1_SCORE = []
RECALL = []

MODEL_NAME_TEST = []
ACCURACY_TEST = []
29/60:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
29/61:
data_train = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_train.csv")
data_test = pd.read_csv(r"/Users/Hyunjee/Desktop/Assignment 2 - Predicting Credit Card Default/CreditCard_test.csv")
print("Default Credit Card Clients data -  rows:",data_train.shape[0]," columns:", data_train.shape[1])
print("Default Credit Card Clients data -  rows:",data_test.shape[0]," columns:", data_test.shape[1])
29/62:
df_train, df_test = data_train.drop(data_train.index[0]), data_test.drop(data_test.index[0])
X_train, y_train = df_train.drop(columns = [df_train.columns[0], 'Y'], axis = 1), df_train.drop(df_train.columns[0:24], axis = 1)
X_test, y_test = df_test.drop(columns = [df_test.columns[0], 'Y'], axis = 1), df_test.drop(df_test.columns[0:24], axis = 1)
29/63: X_train.describe(include='all')
29/64:
X_train['X12']=X_train['X12'].astype(str).astype(int)
X_train['X13']=X_train['X13'].astype(str).astype(int)
X_train['X14']=X_train['X14'].astype(str).astype(int)
X_train['X15']=X_train['X15'].astype(str).astype(int)
X_train['X16']=X_train['X16'].astype(str).astype(int)
X_train['X17']=X_train['X17'].astype(str).astype(int)
var = ['X12','X13','X14','X15','X16','X17']
plt.figure(figsize=(8,8))
plt.title('Amount of Bill Statement (April 2005 - September 2005) \n Pearson Correlation Plot ')
corr = X_train[var].corr()
sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns, linewidths = .1, vmin = -1, vmax = 1)
plt.show()

cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "12pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_precision(2)\
    .set_table_styles(magnify())
31/1: Install OpenCV using pip install opencv-python
31/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
31/3: import cv2
32/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
32/2: import cv2
31/4:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
35/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
35/2: import cv2
35/3:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
35/4:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow{[
    [colour_1, colour_2],
    [colour_3, colour_4],
]}
35/5:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow{[
    [colour_1, colour_2],
    [colour_3, colour_4],
]})
35/6:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow( np.array {[
    [colour_1, colour_2],
    [colour_3, colour_4],
]})
35/7:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array{
    [colour_1, colour_2],
    [colour_3, colour_4],
})
35/8:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array[{
    [colour_1, colour_2],
    [colour_3, colour_4],
}])
35/9:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array{[
    [colour_1, colour_2],
    [colour_3, colour_4],
]})
35/10:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array{
    [colour_1, colour_2],
    [colour_3, colour_4],
})
35/11:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array
    [colour_1, colour_2],
    [colour_3, colour_4],
)
35/12:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array{[
    [colour_1, colour_2],
    [colour_3, colour_4],
]})
35/13:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array([
    [colour_1, colour_2],
    [colour_3, colour_4],
)})
35/14:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array(
    [colour_1, colour_2],
    [colour_3, colour_4],
))
35/15:
colour_1 = [255, 0, 0]
colour_2 = [0, 255, 0]
colour_3 = [0, 0, 255]
colour_4 = [30, 54, 0]
plt.imshow(np.array([[colour_1, colour_2],
                     [colour_3, colour_4]]))
35/16:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
35/17: type(image)
35/18: image.shape
35/19: plt.imhow(image)
35/20: plt.imshow(image)
35/21:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
35/22:
# parse image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'gray')
35/23:
WIDTH = 300
HEIGHT = 300
35/24:
# resize, ignoring aspect ratio
resized = cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(image)
35/25:
# resize, ignoring aspect ratio
resized = cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(resized)
35/26:
WIDTH = 100
HEIGHT = 100
35/27:
# resize, ignoring aspect ratio
resized = cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(resized)
35/28:
WIDTH = 300
HEIGHT = 300
35/29:
# resize, ignoring aspect ratio
resized = cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(resized)
35/30:
aspect = image.shape[1]/float(image.shape[0])
print(aspect)
35/31:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    scaled = cv2.resize(image, (resized, HEIGHT))
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)
35/32:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)
35/33:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print image.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)
35/34:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print img.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)
35/35:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    scaled.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)
35/36:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    scaled.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
#plt.imshow(scaled)
35/37:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print scaled.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
#plt.imshow(scaled)
35/38:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print (scaled.shape)
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
#plt.imshow(scaled)
35/39:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print scaled.shape
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
#plt.imshow(scaled)
35/40:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))
    print (scaled.shape)
if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
#plt.imshow(scaled)
35/41:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled.shape)
35/42:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))
    
# show image
plt.imshow(scaled)

print(scaled.shape)
35/43:
# image.shape[1] =  height
# image.shape[0] = width

aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
35/44:
def crop_center(img, cropx, cropy):
    y,x,c = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)
    return img(starty:starty+cropy, startx:startx+cropx)
# functin above should match resize and take a tuple

# Scaled image
cropped = crop_center(scaled, WIDTH, WIDTH)

# show image
plt.imshow(cropped, cmap = 'gray')
35/45:
def crop_center(img, cropx, cropy):
    y,x,c = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)
    return img[starty:starty+cropy, startx:startx+cropx]
# functin above should match resize and take a tuple

# Scaled image
cropped = crop_center(scaled, WIDTH, WIDTH)

# show image
plt.imshow(cropped, cmap = 'gray')
35/46:
def crop_center(img, cropx, cropy):
    y,x,c = img.shape
    startx = x//2-(cropx//2)
    starty = y//2-(cropy//2)
    return img[starty:starty+cropy, startx:startx+cropx]
# functin above should match resize and take a tuple

# Scaled image
cropped = crop_center(scaled, WIDTH, WIDTH)

# show image
print(cropped.shape)
plt.imshow(cropped, cmap = 'gray')
35/47:
# parse image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'grey')
35/48:
# parse image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'gray')
35/49:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'gray')
35/50:
hist,bins = np.histogram(img.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(img.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
35/51:
hist,bins = np.histogram(img.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
35/52:
hist,bins = np.histogram(image.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
35/53:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
35/54:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.png',res)
35/55:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
print (res.image)
35/56:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
plt.imshow(res)
35/57:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
 cv2.imshow('res.png',res)
35/58:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res.png',res)
36/1:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res.png',res)
36/2:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res.png',res)
36/3:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res',res)
cv2.waitKey(0)
36/4:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
39/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
39/2: import cv2
39/3:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res',res)
cv2.waitKey(0)
40/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
40/2: import cv2
40/3:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(image)
res = np.hstack((image,equ)) #stacking images side-by-side
plt.imshow(res)
40/4:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(image)
res = np.hstack((image,equ)) #stacking images side-by-side
plt.imshow(res)
40/5:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
equ = cv2.equalizeHist(image)
res = np.hstack((image,equ)) #stacking images side-by-side
plt.imshow(res)
40/6:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
equ = cv2.equalizeHist(image)
res = np.hstack((image,equ)) #stacking images side-by-side
res_1 = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)
plt.imshow(res_1)
40/7:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
40/8:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'gray')
40/9:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
40/10:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
40/11: type(image)
40/12: image.shape
40/13: plt.imshow(image)
40/14:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
40/15:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
plt.imshow(img)
40/16:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
plt.imshow(img)
40/17:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
plt.imshow(img)
40/18:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
plt.imshow(img)
40/19:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
cv2.imshow('img',img)
41/1:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.png',res)
41/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
41/3: import cv2
41/4:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.png',res)
41/5:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imshow('res.png',res)
42/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
42/2: import cv2
42/3: #CLAHE (Contrast Limited Adaptive Histogram Equalization)
42/4:
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)

cv2.imwrite('clahe_2.jpg',cl1)
42/5:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.png',res)
42/6:
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)

cv2.imwrite('clahe_2.jpg',cl1)
42/7:
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/8: plt.imshow(res)
42/9:
plt.imshow(res)

res_1 = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)

plt.imshow(res_1)
42/10:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
plt.imshow(img)
42/11:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
plt.imshow(img)
42/12:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)
42/13:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)

img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img_1)
42/14:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)

#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#plt.imshow(img_1)
42/15:
## cv2.imread 
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)
plt.imshow(img)

#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#plt.imshow(img_1)
42/16:
## cv2.imread 
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)

#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#plt.imshow(img_1)
42/17:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)

#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
#plt.imshow(img_1)
42/18:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
plt.imshow(img)
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img_1)
42/19:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img_1)
plt.imshow(equ)
42/20:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img_1)
plt.imshow(equ)
42/21:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img_1)
plt.imshow(equ)
42/22:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/23:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
#img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
equ = cv2.equalizeHist(img_1)
#plt.imshow(equ)
42/24:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/25:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/26:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
cl1 = clahe.apply(img_output )
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/27:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
cl1 = clahe.apply(img_output)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/28:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)

clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img_output)
cv2.imwrite('clahe_2.jpg',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
42/29:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
cv2.imshow('Color input image', img)
cv2.imshow('Histogram equalized', img_output)

cv2.waitKey(0)
#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
43/2: import cv2
43/3:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
43/4:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(image_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = clip_limit, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/5:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = clip_limit, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/6:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/7:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/8:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
43/9:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwshow('clahe_2',cl1)
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
43/10:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwshow('clahe_2',cl1)
cv2.waitKey(0)
cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
cv2.waitKey(0)
cv2.destroyAllWindows()
43/11:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
43/12: import cv2
43/13:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
43/14: type(image)
43/15:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imwshow('clahe_2',cl1)
cv2.waitKey(0)
cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
cv2.waitKey(0)
cv2.destroyAllWindows()
43/16:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
44/1:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
44/2:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
44/3:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
44/4: import cv2
44/5:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
45/1:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
45/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
45/3: import cv2
45/4:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imshow('clahe',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
46/1:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
46/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
46/3: import cv2
46/4:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
#cv2.waitKey(0)
#cv2.destroyAllWindows()
46/5:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
return cv2_to_pil(final_image)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/6:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)




#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/7:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

equ = cv2.equalizeHist(final_img)
res = np.hstack((img,equ,cl1))
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/8:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

equ = cv2.equalizeHist(img)
res = np.hstack((img,equ,cl1))
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/9:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

#equ = cv2.equalizeHist(img)
res = np.hstack((img,final_img))
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/10:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

#equ = cv2.equalizeHist(img)
res = np.concatenate((img,final_img), axis =1)
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/11:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

#equ = cv2.equalizeHist(img)
res = np.hstack((img,final_img), axis =1)
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/12:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

#equ = cv2.equalizeHist(img)
res = np.hstack((img,final_img))
cv2.imwrite('comparison.jpg',res)



#img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
#img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
#img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)

#clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
#cl1 = clahe.apply(img_output)
#cv2.imwrite('clahe_2.jpg',cl1)
#res = np.hstack((img,equ,cl1))
#cv2.imwrite('clahe.jpg',res)
46/13:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

img.shape
46/14:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)

img.shape
final_img.shape
46/15:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",-1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

img.shape
final_img.shape
46/16:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

img.shape
final_img.shape
46/17:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

img.shape
46/18:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)
img.shape
46/19:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
#cv2.imwshow('res',res)
plt.imshow(res)
46/20:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)
img.shape

plt.imshow(combined)
46/21:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2RGB)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)
img.shape


plt.imshow(combined)
46/22:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)
img.shape


plt.imshow(combined)
46/23:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)
46/24:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('combined.jpg',combined)

plt.imshow(final_img)
46/25:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img,final_img))
plt.imshow(final_img)
46/26:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img,final_img))
plt.imshow(combined)
46/27:
# Change defalut to BGR to RGB -> plt.imshow
final_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img,final_img))
plt.imshow(combined)
46/28:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)
46/29:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img,final_img))
plt.imshow(combined)
46/30:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)
46/31:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

plt.imshow(img)
46/32:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img,final_img))
plt.imshow(combined)
46/33:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,final_img_rgb))
plt.imshow(combined)
46/34:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)

plt.imshow(final_img)
46/35:
# Change defalut to BGR to RGB -> plt.imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,final_img_rgb))
plt.imshow(combined)
46/36:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB(Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)
cv2.imwrite('final.jpg',final_img)
cv2.imwrite('img.jpg',img)
46/37:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
cl1 = clahe.apply(img)
#cv2.imshow('clahe_2',cl1)
res = np.hstack((img,equ,cl1))
cv2.imwrite('clahe.jpg',res)
46/38:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res',res)
46/39:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.jpg',res)
46/40: plt.imshow(img)
46/41:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
equ = cv2.equalizeHist(img)
res = np.hstack((img,equ)) #stacking images side-by-side
cv2.imwrite('res.jpg',res)
46/42: plt.imshow(img)
46/43:
img_equal = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img)
46/44:
img_equal = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img_equal)
46/45:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

img_y_cr_cb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)
y, cr, cb = cv2.split(img_y_cr_cb)

# Applying equalize Hist operation on Y channel.
y_eq = cv2.equalizeHist(y)

img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))
img_rgb_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('eq',img_rgb_eq)
46/46:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

img_y_cr_cb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)
y, cr, cb = cv2.split(img_y_cr_cb)

# Applying equalize Hist operation on Y channel.
y_eq = cv2.equalizeHist(y)

img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))
img_rgb_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('eq.jpg',img_rgb_eq)
46/47:
# Load the lmage
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert the image from BGR to YCrCb color space
img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)

# Split the image in YCrCb color splace
y, cr, cb = cv2.split(img_ycrcb)

# Apply Histogram Equalisation on Y channel.
y_eq = cv2.equalizeHist(y)

# Merge enhanced Y channel with the other two channels
img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))

# Conver the color space back to orgianl color space
img_rgb_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('Histogram Equalisation.jpg',img_rgb_eq)
46/48:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_rgb_eq,final_img_rgb))
plt.imshow(combined)
46/49:
# Load the lmage
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert the image from BGR to YCrCb color space
img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)

# Split the image in YCrCb color splace
y, cr, cb = cv2.split(img_ycrcb)

# Apply Histogram Equalisation on Y channel.
y_eq = cv2.equalizeHist(y)

# Merge enhanced Y channel with the other two channels
img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))

# Conver the color space back to orgianl color space
img_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('Histogram Equalisation.jpg',img_eq)
46/50:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)
46/51:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)
46/52:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)

cv2.imwrite("Image Enhancement",combined)
46/53:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)

cv2.imwrite("Image Enhancement.jpg",combined)
46/54:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)

# Save the image
combined_save = cv2.cvtColor(combined_save, cv2.COLOR_RGB2BGR)
cv2.imwrite("Image Enhancement.jpg",combined_save)
46/55:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)

# Save the image
combined_save = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)
cv2.imwrite("Image Enhancement.jpg",combined_save)
46/56:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
46/57:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
46/58:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
48/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
48/2:
img = cv2.imread('/Users/Hyunjee/Desktop/gradient.png',0)
ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC)
ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO)
ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV)

titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]

for i in xrange(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()
48/3:
img = cv2.imread('/Users/Hyunjee/Desktop/gradient.png',0)
ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)
ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)
ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC)
ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO)
ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV)

titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']
images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]
48/4:
for i in xrange(6):
    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()
46/59: plt.imshow(image)
46/60:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
46/61: type(image)
46/62: image.shape
46/63: plt.imshow(image)
46/64:
plt.imshow(image)
cv2.imwrite('BGR.jpg',image)
46/65:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
46/66: type(image)
46/67: image.shape
46/68:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
46/69:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
46/70:
WIDTH = 300
HEIGHT = 300
46/71:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
46/72:
# Load the lmage
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert the image from BGR to YCrCb color space
img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)

# Split the image in YCrCb color splace
y, cr, cb = cv2.split(img_ycrcb)

# Apply Histogram Equalisation on Y channel.
y_eq = cv2.equalizeHist(y)

# Merge enhanced Y channel with the other two channels
img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))

# Conver the color space back to orgianl color space
img_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('Histogram Equalisation.jpg',img_eq)
46/73:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(img.flatten(),256,[0,256], color = 'r')
46/74:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(img.flatten(),256,[0,256], color = 'r')
plt.show()
46/75:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
46/76:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
46/77:
WIDTH = 300
HEIGHT = 300
46/78:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
46/79:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
46/80:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(img.flatten(),256,[0,256], color = 'r')
plt.show()
46/81:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

# Load the image
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model as CLAHE does not work on RGB color space
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB (Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)

# Save the image
cv2.imwrite('CLAHE.jpg',final_img)
cv2.imwrite('Original Image.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('Original CLAHE ',combined)
46/82:
## cv2.imread( , 1): Load Color imgae, any transpareny of image will be neglected (Default)
## cv2.imread( , 0): Load image in Grayscale 
## cv2.imread( , -1): Load image as such including alpha channel (unchanged)

# Load the image
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert image to LAB color model as CLAHE does not work on RGB color space
img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

# Split the image into L, A and b channels
l_channel, a_channel, b_channel = cv2.split(img_lab)

# Apply CLAHE to lightness channel

clache = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))
cl = clahe.apply(l_channel)

# Merge the CLAHE enhanced L channel with the original A and B channel
merged_channels = cv2.merge((cl, a_channel, b_channel))

# Convert image from LAB to RGB (Image defalut is BRR so change it to BGR)
final_img = cv2.cvtColor(merged_channels, cv2.COLOR_LAB2BGR)

# Save the image
cv2.imwrite('CLAHE.jpg',final_img)
cv2.imwrite('Original Image.jpg',img)

combined = np.hstack ((img,final_img))
cv2.imwrite('Original CLAHE.jpg',combined)
46/83:
# To show original image with clahe image using matplot, change color mode from BGR to RGB .imshow
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
final_img_rgb = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)
img_eq_rgb = cv2.cvtColor(img_eq, cv2.COLOR_BGR2RGB)
combined = np.hstack ((img_rgb,img_eq_rgb,final_img_rgb))
plt.imshow(combined)

# Save the image
combined_save = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)
cv2.imwrite("Image Enhancement.jpg",combined_save)
46/84:
hist,bins = np.histogram(final_img_rgb.flatten(),256,[0,256])
plt.hist(final_img_rgb.flatten(),256,[0,256], color = 'r')
plt.show()
46/85:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.show()
46/86:
# Load the lmage
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert the image from BGR to YCrCb color space
img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)

# Split the image in YCrCb color splace
y, cr, cb = cv2.split(img_ycrcb)

# Apply Histogram Equalisation on Y channel.
y_eq = cv2.equalizeHist(y)

# Merge enhanced Y channel with the other two channels
img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))

# Conver the color space back to orgianl color space
img_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('Histogram Equalisation.jpg',img_eq)
46/87:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.show()
46/88:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
46/89:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
46/90:
WIDTH = 300
HEIGHT = 300
46/91:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
46/92:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
46/93:
hist,bins = np.histogram(image.flatten(),256,[0,256])
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.show()
46/94:
# Load the lmage
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

# Convert the image from BGR to YCrCb color space
img_ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)

# Split the image in YCrCb color splace
y, cr, cb = cv2.split(img_ycrcb)

# Apply Histogram Equalisation on Y channel.
y_eq = cv2.equalizeHist(y)

# Merge enhanced Y channel with the other two channels
img_y_cr_cb_eq = cv2.merge((y_eq, cr, cb))

# Conver the color space back to orgianl color space
img_eq = cv2.cvtColor(img_y_cr_cb_eq, cv2.COLOR_YCR_CB2BGR)

cv2.imwrite('Histogram Equalisation.jpg',img_eq)
46/95:
hist,bins = np.histogram(img_eq_rgb.flatten(),256,[0,256])
plt.hist(img_eq_rgb.flatten(),256,[0,256], color = 'r')
plt.show()
46/96:
hist,bins = np.histogram(img_eq.flatten(),256,[0,256])
plt.hist(img_eq.flatten(),256,[0,256], color = 'r')
plt.show()
46/97:
hist,bins = np.histogram(final_img_rgb.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(final_img_rgb.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
46/98:
hist,bins = np.histogram(img_eq.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(img_eq.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
46/99:
hist,bins = np.histogram(img_eq_rgb.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(img_eq_rgb.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
46/100:
hist,bins = np.histogram(image.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
46/101:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
49/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
49/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
49/3:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

#from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                # denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/1:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

#from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                # denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/2:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/3:
import matplotlib.pyplot as plt
import numpy as np
import skimage
%matplotlib inline


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/4:
import matplotlib.pyplot as plt
import numpy as np
import networkx
import scipy
import PyWavelets
import imageio
import pillow

import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/5:
import matplotlib.pyplot as plt
import numpy as np
import networkx
import scipy
#import PyWavelets
import imageio
import pillow

import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/6:
import matplotlib.pyplot as plt
import numpy as np
import networkx
import scipy
#import PyWavelets
#import imageio
import pillow

import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/7:
import matplotlib.pyplot as plt
import numpy as np
import networkx
import scipy
#import PyWavelets
#import imageio
#import pillow

import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/8:
import matplotlib.pyplot as plt
import numpy as np


%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/9: import cv2
50/10:
import matplotlib.pyplot as plt
import numpy as np


%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/11: import cv2
50/12:
import matplotlib.pyplot as plt
import numpy as np


%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/13:
import matplotlib.pyplot as plt
import numpy as np
import scikit-image as skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/14:
import matplotlib.pyplot as plt
import numpy as np


%matplotlib inline
from skimage import data, img_as_float


import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/15:
import matplotlib.pyplot as plt
import numpy as np


%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/16:
import matplotlib.pyplot as plt
import numpy as np
import io
import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/17:
import matplotlib.pyplot as plt
import numpy as np
import scipy

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/18:
import matplotlib.pyplot as plt
import numpy as np
import scipy

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/19:
import matplotlib.pyplot as plt
import numpy as np
import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/20:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/21:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/22:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
50/23: import cv2
50/24:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
50/25: type(image)
50/26: image.shape
50/27:
plt.imshow(image)
cv2.imwrite('BGR.jpg',image)
50/28:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
50/29:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
plt.imshow(gray, cmap = 'gray')
50/30:
WIDTH = 300
HEIGHT = 300
50/31:
# resize, ignoring aspect ratio
res= cv2.resize(image, (WIDTH, HEIGHT))

# show image
plt.imshow(res)
50/32:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
50/33:
hist,bins = np.histogram(image.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
50/34:
original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/35:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
50/36:
filename = os.path.join(skimage.data_dir, 'tomato.png')

original = io.imread(filename)
50/37:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
50/38: import cv2
50/39:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
image = img_as_float(tomato)
50/40:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
original = img_as_float(tomato)
50/41:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/42:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)
original = img_as_float(tomato)
50/43:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/44:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)
original = img_as_float(tomato[100:250, 50:300])
50/45:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/46:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)
original = img_as_float(tomato)
50/47:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

#fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)

fig, ax = plt.subplots(nrows=2, ncols=4,
                       sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/48:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

#fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
 #                      sharex=True, sharey=True)

fig, ax = plt.subplots(nrows=2, ncols=4,sharex=True, sharey=True)

plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/49:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=50,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/50:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=5,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/51:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=1,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/52:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=15,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/53:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/54:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.2, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.3, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/55:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.2, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.5, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/56:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)
original = img_as_float(tomato)
50/57:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)



plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/58:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
#noisy = random_noise(original, var=sigma**2)

#astro = img_as_float(data.astronaut())
#astro = astro[220:300, 220:320]

noisy = original + 0.6 * original.std() * np.random.random(original.shape)
noisy = np.clip(noisy, 0, 1)


fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)





plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/59:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
#noisy = random_noise(original, var=sigma**2)

#astro = img_as_float(data.astronaut())
#astro = astro[220:300, 220:320]

noisy = original + 0.6 * original.std() * np.random.random(original.shape)
noisy = np.clip(noisy, 0, 1)


fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)





plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/60:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
#noisy = random_noise(original, var=sigma**2)

#astro = img_as_float(data.astronaut())
#astro = astro[220:300, 220:320]

noisy = original + 0.6 * original.std() * np.random.random(original.shape)
noisy = np.clip(noisy, 0, 1)


fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)





plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/61:


#original = img_as_float(data.chelsea()[100:250, 50:300])

sigma = 0.155
noisy = random_noise(original, var=sigma**2)



#noisy = original + 0.6 * original.std() * np.random.random(original.shape)
#noisy = np.clip(noisy, 0, 1)


fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(8, 5),
                       sharex=True, sharey=True)





plt.gray()

# Estimate the average noise standard deviation across color channels.
sigma_est = estimate_sigma(noisy, multichannel=True, average_sigmas=True)
# Due to clipping in random_noise, the estimate will be a bit smaller than the
# specified sigma.
print("Estimated Gaussian noise standard deviation = {}".format(sigma_est))

ax[0, 0].imshow(noisy)
ax[0, 0].axis('off')
ax[0, 0].set_title('Noisy')
ax[0, 1].imshow(denoise_tv_chambolle(noisy, weight=0.1, multichannel=True))
ax[0, 1].axis('off')
ax[0, 1].set_title('TV')
ax[0, 2].imshow(denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=2,
                multichannel=True))
ax[0, 2].axis('off')
ax[0, 2].set_title('Bilateral')
ax[0, 3].imshow(denoise_wavelet(noisy, multichannel=True))
ax[0, 3].axis('off')
ax[0, 3].set_title('Wavelet denoising')

ax[1, 1].imshow(denoise_tv_chambolle(noisy, weight=0.2, multichannel=True))
ax[1, 1].axis('off')
ax[1, 1].set_title('(more) TV')
ax[1, 2].imshow(denoise_bilateral(noisy, sigma_color=0.1, sigma_spatial=2,
                multichannel=True))
ax[1, 2].axis('off')
ax[1, 2].set_title('(more) Bilateral')
ax[1, 3].imshow(denoise_wavelet(noisy, multichannel=True, convert2ycbcr=True))
ax[1, 3].axis('off')
ax[1, 3].set_title('Wavelet denoising\nin YCbCr colorspace')
ax[1, 0].imshow(original)
ax[1, 0].axis('off')
ax[1, 0].set_title('Original')

fig.tight_layout()

plt.show()
50/62:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)

th = 127
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC)

output = [img, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i])
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/63:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)

th = 127
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i])
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/64:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)

th = 127
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i])
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/65:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)
th = 127
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i])
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/66:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \ 
                           cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \ 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/67:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,\
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \ 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/68:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/69:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(3):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/70:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(4):
    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/71:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in xrange(4):
    plt.subplot(2,2,i+1)
    plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/72:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.medianBlur(tomato, 5)

ret, th1 = cv2.threshold(tomato, 127, 255, cv2.THRESH_BINARY)
th2 = cv2.adaptiveThreshold(tomato,255,cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,11,2)
th3 = cv2.adaptiveThreshold(tomato, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                           cv2.THRESH_BINARY,11,2)

titles = ['Original Image', 'Global Thresholding (v = 127)',
            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']

images = [tomato, th1, th2, th3]

for i in range(4):
    plt.subplot(2,2,i+1)
    plt.imshow(images[i],'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/73:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)

# global thresholding
ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)

# Otsu's thresholding
ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)

# Otsu's thresholding after Gaussian filtering
blur = cv2.GaussianBlur(img,(5,5),0)
ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)

# plot all the images and their histograms
images = [img, 0, th1,
          img, 0, th2,
          blur, 0, th3]
titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',
          'Original Noisy Image','Histogram',"Otsu's Thresholding",
          'Gaussian filtered Image','Histogram',"Otsu's Thresholding"]

for i in xrange(3):
    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')
    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)
    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')
    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])
plt.show()
50/74:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)

# global thresholding
ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)

# Otsu's thresholding
ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)

# Otsu's thresholding after Gaussian filtering
blur = cv2.GaussianBlur(img,(5,5),0)
ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)

# plot all the images and their histograms
images = [img, 0, th1,
          img, 0, th2,
          blur, 0, th3]
titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',
          'Original Noisy Image','Histogram',"Otsu's Thresholding",
          'Gaussian filtered Image','Histogram',"Otsu's Thresholding"]

for i in range(3):
    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')
    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)
    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])
    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')
    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])
plt.show()
50/75:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",1)
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)

th = 0
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i])
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/76:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)

th = 0
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i], cmap = "gray")
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/77:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)
tomato = cv2.cvtColor(tomato, cv2.COLOR_BGR2RGB)

th = 0
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/78:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)


th = 0
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/79:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)

    contour_info = []
    _, contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/80:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)

    contour_info = []
    _, contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/81:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/82: plt.show(masked)
50/83:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/84: plt.show(masked)
50/85: masked
50/86:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/87: masked
50/88: plt.show(masked)
50/89: plt.show(img)
50/90: plt.imshow(img)
50/91: plt.imshow(masked)
50/92:
    BLUR = 21
    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/93: plt.imshow(masked)
50/94:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/95: plt.imshow(masked)
50/96:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 255
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/97: plt.imshow(masked)
50/98:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/99: plt.imshow(masked)
50/100:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    high_thresh, thresh_im = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    lowThresh = 0.5*high_thresh
    CANNY_THRESH_1 = high_thresh
    CANNY_THRESH_2 = lowThresh
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/101:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    high_thresh, thresh_im = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    lowThresh = 0.5*high_thresh
    CANNY_THRESH_1 = high_thresh
    CANNY_THRESH_2 = lowThresh
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/102: plt.imshow(masked)
50/103:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
   v = np.median(gray)

    #---- apply automatic Canny edge detection using the computed median----
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    CANNY_THRESH_1 = upper
    CANNY_THRESH_2 = lower
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/104:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    v = np.median(gray)

    #---- apply automatic Canny edge detection using the computed median----
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    CANNY_THRESH_1 = upper
    CANNY_THRESH_2 = lower
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/105: plt.imshow(masked)
50/106:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)

    #---- apply automatic Canny edge detection using the computed median----
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    CANNY_THRESH_1 = upper
    CANNY_THRESH_2 = lower
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/107: plt.imshow(masked)
50/108:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)

    #---- apply automatic Canny edge detection using the computed median----
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/109: plt.imshow(masked)
50/110:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    #v = np.median(gray)
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))
    high_thresh, thresh_im = cv2.threshold(im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    lowThresh = 0.5*high_thresh
    
    
    CANNY_THRESH_1 = lowThresh
    CANNY_THRESH_2 = high_thresh
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/111:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    #v = np.median(gray)
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))
    high_thresh, thresh_im = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    lowThresh = 0.5*high_thresh
    
    
    CANNY_THRESH_1 = lowThresh
    CANNY_THRESH_2 = high_thresh
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/112: plt.imshow(masked)
50/113:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    #v = np.median(gray)
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    double otsu_thresh_val = cv::threshold(
    orig_img, _img, 0, 255, CV_THRESH_BINARY | CV_THRESH_OTSU
    );
    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/114:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    #v = np.median(gray)
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    double otsu_thresh = cv::threshold(
    orig_img, _img, 0, 255, CV_THRESH_BINARY | CV_THRESH_OTSU
    );
    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/115:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/116: plt.imshow(masked)
50/117: plt.imshow(mask)
50/118: plt.imshow(masked)
50/119: plt.imshow(edges)
50/120: plt.imshow(masked)
50/121: plt.imshow(contour)
50/122: plt.imshow(contours)
50/123:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    #mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/124: plt.imshow(masked)
50/125:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    #mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/126: plt.imshow(masked)
50/127:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    #mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/128: plt.imshow(masked)
50/129:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    
    
    
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    
    
    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
    
    
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    
    
    
    
    # Empty list
    contour_info = []
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/130: plt.imshow(masked)
50/131:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/132: plt.imshow(masked)
50/133:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/134: plt.imshow(masked)
50/135:
BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/136:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/137: plt.imshow(edges)
50/138:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
50/139: plt.imshow(edges)
50/140:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
50/141: plt.imshow(edges)
50/142:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/143: plt.imshow(edges)
50/144:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/145: plt.imshow(masked)
50/146:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/147:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/148:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE,offset = new cv.Point(0, 0))

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/149:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/150: plt.imshow(masked)
50/151:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/152:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/153: plt.imshow(masked)
50/154:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    #mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/155: plt.imshow(masked)
50/156:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    #mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/157: plt.imshow(masked)
50/158:
    BLUR = 23
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/159: plt.imshow(masked)
50/160:
    BLUR = 24
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/161:
    BLUR = 23
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/162: plt.imshow(masked)
50/163:
    BLUR = 20
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/164: plt.imshow(masked)
50/165:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
    
    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/166: plt.imshow(masked)
50/167:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[1], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/168: plt.imshow(masked)
50/169:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/170: plt.imshow(masked)
50/171:
contour_info
plt.imshow(masked)
50/172:
print(contour_info)
plt.imshow(masked)
50/173:
print(contour_info[2])
plt.imshow(masked)
50/174:
print(contour_info.shape)
plt.imshow(masked)
50/175:
len(contour_info)
plt.imshow(masked)
50/176:
len(contour_info)
#plt.imshow(masked)
50/177:
contour_info[3]
#plt.imshow(masked)
50/178:
contour_info[4]
#plt.imshow(masked)
50/179:
contour_info[5]
#plt.imshow(masked)
50/180:
contour_info
#plt.imshow(masked)
50/181:
c
#plt.imshow(masked)
50/182:
c[1]
#plt.imshow(masked)
50/183:
c[2]
#plt.imshow(masked)
50/184:
c[0]
#plt.imshow(masked)
50/185:
c[1]
#plt.imshow(masked)
50/186:
contour_info

#plt.imshow(masked)
50/187:
c

#plt.imshow(masked)
50/188:
c[2]

#plt.imshow(masked)
50/189:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[1], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/190:
c[2]

#plt.imshow(masked)
50/191:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/192: plt.imshow(edges)
50/193:
#c[2]

plt.imshow(masked)
50/194:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[0], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/195:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/196:
#c[2]

plt.imshow(masked)
50/197:
len(c)

plt.imshow(masked)
50/198:
len(c)

#plt.imshow(masked)
50/199:
c

#plt.imshow(masked)
50/200:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
50/201:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
print(x)
50/202:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
x[0]
50/203:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
print(x[0])
50/204:
mylist = [1, 2, 3]
x = len(mylist)
print(x[0])
50/205:
mylist = [1, 2, 3]
x = len(mylist)
x[0]
50/206:
mylist = [1, 2, 3]
x = len(mylist)
50/207: x[1]
50/208:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
mylist
50/209:
mylist = ["apple", "banana", "cherry"]
x = len(mylist)
50/210:
mylist = ["apple", "banana", "cherry"]
len(mylist)
50/211:
len(c)

#plt.imshow(masked)
50/212:
mylist = ["apple", "banana", "cherry"]
len(mylist)
mylist[2]
50/213:
mylist = ["apple", "banana", "cherry"]
len(mylist)
mylist[2]
50/214:
contour_info

#plt.imshow(masked)
50/215:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/216:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        print(contour_info)
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/217:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info according to contour point, c, 2nd element in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/218:
m = [[3,2], [2,4], [3,5]]
x = sorted(sorted, key=lambda c: c[2], reverse=True)
50/219:
m = [[3,2], [2,4], [3,5]]
x = sorted(m, key = lambda x: x[2], reverse=True)
50/220:
m = [[3,2,3], [2,4], [3,5]]
x = sorted(m, key = lambda x: x[2], reverse=True)
50/221:
m = [[3,2,3], [2,4,4], [3,5,2]]
x = sorted(m, key = lambda x: x[2], reverse=True)
50/222:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
        
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/223: max_contour
50/224: contour_info(2)
50/225: len(contour_infro)
50/226: len(contour_info)
50/227:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6]]

mylistSort = sorted(mylist, key = lambda x: x[1])
50/228:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylistSort
50/229:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylistSort
len(mylist)
50/230:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6], [3,3,4]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylistSort
len(mylist)
50/231:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6], [3,3,4]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylistSort
50/232: contours
50/233: contour_info
50/234: contour_info[0]
50/235: len(contour_info[0])
50/236: edges.shape
50/237:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6], [3,3,4]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylistSort.shape
50/238:
mylist = [[7, 8], [1, 2, 3], [2, 5, 6], [3,3,4]]

mylistSort = sorted(mylist, key = lambda x: x[1])

mylist.shape
50/239:
array.mylist = ([[7, 8,8], 
                 [1, 2, 3], 
                 [2, 5, 6], 
                 [3,3,4]])

mylistSort = sorted(mylist, key = lambda x: x[1])

mylist.shape
50/240:
mylist = ([[7, 8, 8], 
          [1, 2, 3], 
                 [2, 5, 6], 
                 [3, 3, 4]])

mylistSort = sorted(mylist, key = lambda x: x[1])

mylist.shape
50/241:
mylist = np.array([[7, 8, 8], 
                  [1, 2, 3], 
                 [2, 5, 6], 
                 [3, 3, 4]])

mylistSort = sorted(mylist, key = lambda x: x[1])
50/242:
mylist = np.array([[7, 8, 8], 
                  [1, 2, 3], 
                 [2, 5, 6], 
                 [3, 3, 4]])

mylistSort = sorted(mylist, key = lambda x: x[1])

mylist.shape
50/243: contour.shape
50/244: contour_info.shape
50/245: max_contour[0]
50/246: max_contour[1]
50/247: plt.imshow(masked)
50/248:
    BLUR = 22
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/249: plt.imshow(masked)
50/250:
    BLUR = 20
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/251:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/252: plt.imshow(masked)
50/253:
    BLUR = 22
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/254:
    BLUR = 5
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/255: plt.imshow(masked)
50/256:
    BLUR = 10
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/257:
    BLUR = 25
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/258: plt.imshow(masked)
50/259:
    BLUR = 40
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/260: plt.imshow(masked)
50/261:
    BLUR = 30
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/262:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/263: plt.imshow(masked)
50/264:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
    
    
    
    c_red, c_green, c_blue = cv2.split(img)
    img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))

    cv2.imwrite(args.file_out, img_a*255
50/265:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
    
    
    
    c_red, c_green, c_blue = cv2.split(img)
    img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))
    cv2.imwrite(args.file_out, img_a*255)
50/266:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
    
    
    
    c_red, c_green, c_blue = cv2.split(img)
    img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))
50/267: plt.imshow(img_a)
50/268:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    plt.imshow(mask)
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/269:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    plt.imshow(mask)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/270:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
   
    mask.shape


    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/271:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
   
    print(mask.shape)


    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/272:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
   
    print(mask.shape)


    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    img = img.astype('float32') / 255.0     # Easy Blending
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
50/273:
a = [1,3]
b = [2,5]
c= np.dstack([a]*3)
50/274:
a = [1,3]
b = [2,5]
c= np.dstack([a]*3)
c
50/275:
a = [1,3]
b = [2,5]
c= np.dstack([a]*3)
c.shape
50/276:
a = [1,3]
b = [2,5]
c= np.dstack([a]*3)
c
50/277:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    gray.shape
50/278:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
gray.shape
50/279:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
img.shape
50/280:

img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

v = hsv[:,:,2]
th, threshed = cv2.threshold(v, 100, 255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)
threshed[-1] = 255

cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]

mask = np.zeros_like(threshed)
cv2.drawContours(mask, cnts, -1, (255, 0, 0), -1, cv2.LINE_AA)
mask = cv2.erode(mask, np.ones((3,3), np.int32), iterations=1)

png = np.dstack((img, mask))

plt.imshow(png)
50/281:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0     
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/282: plt.imshow(masked)
50/283:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 200
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0     
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/284: plt.imshow(masked)
50/285:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    plt.imshow(img)
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/286:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    plt.imshow(img)
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/287: plt.imshow(masked)
50/288:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    plt.imshow(img)
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/289:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/290: plt.imshow(masked)
50/291:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/292: plt.imshow(masked)
50/293:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/294: plt.imshow(masked)
50/295:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/296: plt.imshow(masked)
50/297:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = 10
    CANNY_THRESH_2 = 100
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/298: plt.imshow(masked)
50/299:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/300: plt.imshow(masked)
50/301:
tomato = cv2.imread("/Users/Hyunjee/Desktop/tomato.png",0)


th = 0
max_val = 255

ret, o1 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
ret, o2 = cv2.threshold(tomato, th, max_val, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
ret, o3 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO + cv2.THRESH_OTSU)
ret, o4 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)
ret, o5 = cv2.threshold(tomato, th, max_val, cv2.THRESH_TRUNC + cv2.THRESH_OTSU)

output = [tomato, o1, o2, o3, o4, o5]

titles = ['Original', 'Binary', 'Binary Inv', 'Zero', 'Zero Inv', 'Trunc']

for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/302:
    BLUR = 5
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/303: plt.imshow(masked)
50/304:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/305: plt.imshow(masked)
50/306:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/307: plt.imshow(masked)
50/308: plt.imshow(edges)
50/309: plt.imshow(masked)
50/310: plt.imshow(img_a)
50/311:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
    
    c_red, c_green, c_blue = cv2.split(img)
    img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))
50/312: plt.imshow(img_a)
50/313:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/314: plt.imshow(img_a)
50/315:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/316: plt.imshow(img_a)
50/317:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/318: plt.imshow(img_a)
50/319:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/320: plt.imshow(masked)
50/321:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/322: plt.imshow(masked)
50/323: plt.imshow(edges)
50/324:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/325: plt.imshow(edges)
50/326:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/327: plt.imshow(edges)
50/328: plt.imshow(mask)
50/329:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/330: plt.imshow(mask)
50/331: plt.imshow(masked)
50/332:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_CCOMP  , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/333: plt.imshow(masked)
50/334:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_TREE  , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/335: plt.imshow(masked)
50/336:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.FLOODFILL , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/337: plt.imshow(masked)
50/338:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_FLOODFILL , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/339:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/340: plt.imshow(masked)
50/341:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_TC89_L1 )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/342: plt.imshow(masked)
50/343:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_TC89_KCOS  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/344: plt.imshow(masked)
50/345:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/346: plt.imshow(masked)
50/347:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/348: plt.imshow(masked)
50/349: plt.imshow(mask)
50/350:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,L2gradient=True)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/351: plt.imshow(maskED)
50/352: plt.imshow(masked)
50/353:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,L2gradient=True)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/354: plt.imshow(masked)
50/355:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,L2gradient=True)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/356: plt.imshow(masked)
50/357:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/358: plt.imshow(masked)
50/359:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/360: plt.imshow(masked)
50/361:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    img = cv2.GaussianBlur(img, (BLUR, BLUR), 0)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/362:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    #img = cv2.GaussianBlur(img, (BLUR, BLUR), 0)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/363: plt.imshow(masked)
50/364:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    img = cv2.GaussianBlur(img, (BLUR, BLUR), 0)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/365:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    img = cv2.GaussianBlur(img, (BLUR, BLUR), 0)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/366: plt.imshow(edges)
50/367:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    #img = cv2.GaussianBlur(img, (BLUR, BLUR), 0)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
50/368: plt.imshow(edges)
50/369:
    BLUR = 50
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/370:
    BLUR = 40
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/371:
    BLUR = 28
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/372:
    BLUR = 2
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/373:
    BLUR = 3
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/374: plt.imshow(masked)
50/375:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/376: plt.imshow(masked)
50/377:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
b,g,r = cv2.split(img)
rgb_img = cv2.merge([r,g,b])
blur = cv2.GaussianBlur(rgb_img,(5,5),0)
    gray = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
50/378:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/379:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
b,g,r = cv2.split(img)
rgb_img = cv2.merge([r,g,b])
blur = cv2.GaussianBlur(rgb_img,(5,5),0)
gray = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
50/380: plt.show(gray)
50/381: plt.imshow(gray)
50/382: plt.imshow(img)
50/383: plt.imshow(blur)
50/384:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    #img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/385: plt.imshow(masked)
50/386:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/387: plt.imshow(masked)
50/388:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
b,g,r = cv2.split(img)
rgb_img = cv2.merge([r,g,b])
blur = cv2.bilateralFilter(rgb_img,9,75,75
gray = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
50/389:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
b,g,r = cv2.split(img)
rgb_img = cv2.merge([r,g,b])
blur = cv2.bilateralFilter(rgb_img,9,75,75)
gray = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
50/390: plt.imshow(blur)
50/391:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/392: plt.imshow(masked)
50/393:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    #img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/394: plt.imshow(masked)
50/395:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
b,g,r = cv2.split(img)
rgb_img = cv2.merge([r,g,b])
blur = cv2.bilateralFilter(rgb_img,9,75,75)
gray = cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
50/396: plt.imshow(blur)
50/397:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    #img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/398: plt.imshow(masked)
50/399:
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")

  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
  morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

  _, cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  cnt = sorted(cnts, key=cv2.contourArea)[-1]
  x,y,w,h = cv2.boundingRect(cnt)
  new_img = img[y:y+h, x:x+w]

     

 
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
  morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

  _, roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

  mask = np.zeros(img.shape, img.dtype)

  cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

  masked_image = cv2.bitwise_and(img, mask)
50/400:
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    _, cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = sorted(cnts, key=cv2.contourArea)[-1]
    x,y,w,h = cv2.boundingRect(cnt)
    new_img = img[y:y+h, x:x+w]

     

 
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    _, roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    mask = np.zeros(img.shape, img.dtype)

    cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

    masked_image = cv2.bitwise_and(img, mask)
50/401:
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = sorted(cnts, key=cv2.contourArea)[-1]
    x,y,w,h = cv2.boundingRect(cnt)
    new_img = img[y:y+h, x:x+w]

     

 
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    mask = np.zeros(img.shape, img.dtype)

    cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

    masked_image = cv2.bitwise_and(img, mask)
50/402: plt.show(masked_image)
50/403: plt.imshow(masked_image)
50/404: plt.imshow(new_img)
50/405:
def cut(img):
  # crop image
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = sorted(cnts, key=cv2.contourArea)[-1]
    x,y,w,h = cv2.boundingRect(cnt)
    new_img = img[y:y+h, x:x+w]
return new_img        

def transBg(img):   
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    mask = np.zeros(img.shape, img.dtype)

    cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

    masked_image = cv2.bitwise_and(img, mask)

return masked_image

def fourChannels(img):
    height, width, channels = img.shape
    if channels < 4:
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
    return new_img

return img

s_img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",-1)

# set to 4 channels
s_img = fourChannels(s_img)

# remove white background
s_img = cut(s_img)

# set background transparent
s_img = transBg(s_img)
50/406: plt.imshow(s_img)
50/407: plt.imshow(s_img)
50/408:
def cut(img):
  # crop image
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = sorted(cnts, key=cv2.contourArea)[-1]
    x,y,w,h = cv2.boundingRect(cnt)
    new_img = img[y:y+h, x:x+w]
return new_img        

def transBg(img):   
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    mask = np.zeros(img.shape, img.dtype)

    cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

    masked_image = cv2.bitwise_and(img, mask)

return masked_image

def fourChannels(img):
    height, width, channels = img.shape
    if channels < 4:
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
    return new_img

return img

s_img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",-1)

# set to 4 channels
s_img = fourChannels(s_img)

# remove white background
s_img = cut(s_img)

# set background transparent
s_img = transBg(s_img)

s_img
50/409: s_img
50/410:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    ret, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)
    edges = thresh
    
    
    
    
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/411:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    
    edges = thresh
    
    
    
    
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/412:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    
    
    
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/413: plt.imshow(masked)
50/414:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    
    
    
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/415: plt.imshow(masked)
50/416:
#plt.imshow(masked)
lower
50/417:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+50
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/418:
#plt.imshow(masked)
lower
50/419: plt.imshow(masked)
50/420:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+150
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/421: plt.imshow(masked)
50/422:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+20
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/423: plt.imshow(masked)
51/1:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)

parser = argparse.ArgumentParser(description='Code for Image Segmentation with Distance Transform and Watershed Algorithm.\
    Sample code showing how to segment overlapping objects using Laplacian filtering, \
    in addition to Watershed and Distance Transformation')
parser.add_argument('--input', help='Path to input image.', default='cards.png')
args = parser.parse_args()
src = cv.imread(cv.samples.findFile(args.input))
if src is None:
    print('Could not open or find the image:', args.input)
    exit(0)
# Show source image
cv.imshow('Source Image', src)
src[np.all(src == 255, axis=2)] = 0
# Show output image
cv.imshow('Black Background Image', src)
kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)
# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated
imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian
# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
51/2:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
51/3:
src = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv2.imshow('Black Background Image', src)
51/4:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)
52/1:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian

# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
cv.waitKey()
52/2:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
52/3:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
52/4:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
52/5:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
52/6:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian

# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
cv.waitKey()
54/1:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
import matplotlib.pyplot as plt
54/2:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian

# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
plt.imshow('Final Result', dst)
56/1:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian
56/2:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
import matplotlib.pyplot as plt
56/3:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian
56/4:
# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
56/5:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
plt.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
plt.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
56/6:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
56/7:
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
56/8:
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
56/9:
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
56/10:
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
56/11:
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
plt.imshow('Final Result', dst)
56/12:
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
56/13:
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
#cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
56/14:
from __future__ import print_function
import cv2 as cv
import numpy as np
import argparse
import random as rng
rng.seed(12345)
import matplotlib.pyplot as plt
56/15:
src = cv.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
src[np.all(src == 255, axis=2)] = 0
cv.imshow('Black Background Image', src)

kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)


# do the laplacian filtering as it is
# well, we need to convert everything in something more deeper then CV_8U
# because the kernel has some negative values,
# and we can expect in general to have a Laplacian image with negative values
# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255
# so the possible negative number will be truncated

imgLaplacian = cv.filter2D(src, cv.CV_32F, kernel)
sharp = np.float32(src)
imgResult = sharp - imgLaplacian
56/16:
# convert back to 8bits gray scale
imgResult = np.clip(imgResult, 0, 255)
imgResult = imgResult.astype('uint8')
imgLaplacian = np.clip(imgLaplacian, 0, 255)
imgLaplacian = np.uint8(imgLaplacian)
56/17:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
56/18:
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
56/19:
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
56/20:
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
#cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
56/21:
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
56/22: plt.imshow(dst)
56/23: plt.imshow(src)
56/24: plt.imshow(dst)
56/25: plt.imshow(mark)
56/26: plt.imshow(dst)
56/27: plt.imshow(dist)
56/28: plt.imshow(src)
56/29: plt.imshow(imgLaplacian)
56/30: plt.imshow(imgResult)
56/31: plt.imshow(bw)
56/32:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_RGB2GRAY)
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
56/33: plt.imshow(bw)
56/34:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
56/35: plt.imshow(bw)
56/36: plt.imshow(imgResult)
56/37: plt.imshow(bw)
50/424:
imgResult = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")

a = cv2.cvtColor(imgResult, cv2.COLOR_BGR2GRAY)
50/425: plt.imshow(a)
50/426: plt.imshow(imgResult)
56/38:
#cv.imshow('Laplace Filtered Image', imgLaplacian)
cv.imshow('New Sharped Image', imgResult)
bw = cv.cvtColor(imgResult, cv.COLOR_BGR2GRAY)
56/39: plt.imshow(bw)
56/40:
_, bw = cv.threshold(bw, 40, 255, cv.THRESH_BINARY | cv.THRESH_OTSU)
cv.imshow('Binary Image', bw)
dist = cv.distanceTransform(bw, cv.DIST_L2, 3)
56/41:
# Normalize the distance image for range = {0.0, 1.0}
# so we can visualize and threshold it
cv.normalize(dist, dist, 0, 1.0, cv.NORM_MINMAX)
cv.imshow('Distance Transform Image', dist)
_, dist = cv.threshold(dist, 0.4, 1.0, cv.THRESH_BINARY)
56/42: plt.imshow(dist)
56/43:
# Dilate a bit the dist image
kernel1 = np.ones((3,3), dtype=np.uint8)
dist = cv.dilate(dist, kernel1)
cv.imshow('Peaks', dist)
dist_8u = dist.astype('uint8')
56/44:
# Find total markers
contours, _ = cv.findContours(dist_8u, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
# Create the marker image for the watershed algorithm
markers = np.zeros(dist.shape, dtype=np.int32)
# Draw the foreground markers
for i in range(len(contours)):
    cv.drawContours(markers, contours, i, (i+1), -1)
# Draw the background marker
cv.circle(markers, (5,5), 3, (255,255,255), -1)
#cv.imshow('Markers', markers*10000)
cv.watershed(imgResult, markers)
#mark = np.zeros(markers.shape, dtype=np.uint8)
mark = markers.astype('uint8')
mark = cv.bitwise_not(mark)
56/45:
# uncomment this if you want to see how the mark
# image looks like at that point
#cv.imshow('Markers_v2', mark)
# Generate random colors
colors = []
for contour in contours:
    colors.append((rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)))
# Create the result image
dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)
# Fill labeled objects with random colors
for i in range(markers.shape[0]):
    for j in range(markers.shape[1]):
        index = markers[i,j]
        if index > 0 and index <= len(contours):
            dst[i,j,:] = colors[index-1]
# Visualize the final image
cv.imshow('Final Result', dst)
56/46: plt.imshow(dst)
56/47:
import cv2 

import matplotlib.pyplot as plt
56/48: image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/49:
# blur and grayscale before thresholding
blur = cv2.cvtColor(src = image, code = cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/50:
# blur and grayscale before thresholding
src = image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/51:
# blur and grayscale before thresholding
src = image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/52:
# blur and grayscale before thresholding
src = image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/53:
import cv2 

import matplotlib.pyplot as plt
56/54: image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/55:
# blur and grayscale before thresholding
src = image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
50/427:
def cut(img):
  # crop image
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    cnts, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnt = sorted(cnts, key=cv2.contourArea)[-1]
    x,y,w,h = cv2.boundingRect(cnt)
    new_img = img[y:y+h, x:x+w]
return new_img        

def transBg(img):   
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

    roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    mask = np.zeros(img.shape, img.dtype)

    cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

    masked_image = cv2.bitwise_and(img, mask)

return masked_image

def fourChannels(img):
    height, width, channels = img.shape
    if channels < 4:
        new_img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
    return new_img

return img

s_img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",-1)

# set to 4 channels
s_img = fourChannels(s_img)

# remove white background
s_img = cut(s_img)

# set background transparent
s_img = transBg(s_img)

s_img
56/56:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_floatimport cv2 

import matplotlib.pyplot as plt
56/57:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
56/58: import cv2
56/59: image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/60:
# blur and grayscale before thresholding
src = image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/61: src= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/62:
# blur and grayscale before thresholding

blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/63: src= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/64:
# blur and grayscale before thresholding

blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/65:
# blur and grayscale before thresholding

blur = cv2.cvtColor(src=img, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/66:
# blur and grayscale before thresholding

blur = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/67: img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
56/68:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
src = img
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/69:
# blur and grayscale before thresholding
src= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")

blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/70:
# blur and grayscale before thresholding
src= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")

blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/71:
# blur and grayscale before thresholding
src= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
plt.imshow(src)
56/72:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.png")
plt.imshow(img)
56/73:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
56/74: import cv2
56/75:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
plt.imshow(img)
56/76:
blur = cv2.cvtColor(src=img, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/77:
src=img
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (k, k), 
    sigmaX = 0)
56/78: import cv2, sys
56/79:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
56/80:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
k = int(sys.argv[2])
56/81:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
56/82:
src=img
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (5, 5), 
    sigmaX = 0)
56/83:
# perform adaptive thresholding 
(t, maskLayer) = cv2.threshold(src = blur, 
    thresh = 0, 
    maxval = 255, 
    type = cv2.THRESH_BINARY + cv2.THRESH_OTSU)
56/84:
# make a mask suitable for color images
mask = cv2.merge(mv = [maskLayer, maskLayer, maskLayer])

cv2.namedWindow(winname = "mask", flags = cv2.WINDOW_NORMAL)
cv2.imshow(winname = "mask", mat = mask)
cv2.waitKey(delay = 0)

# use the mask to select the "interesting" part of the image
sel = cv2.bitwise_and(src1 = image, src2 = mask)

plt.imshow(sel)
57/1:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
57/2: import cv2
57/3:
# blur and grayscale before thresholding
img= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
57/4:
src=img
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (5, 5), 
    sigmaX = 0)
57/5:
# perform adaptive thresholding 
(t, maskLayer) = cv2.threshold(src = blur, 
    thresh = 0, 
    maxval = 255, 
    type = cv2.THRESH_BINARY + cv2.THRESH_OTSU)
57/6:
# make a mask suitable for color images
mask = cv2.merge(mv = [maskLayer, maskLayer, maskLayer])

#cv2.namedWindow(winname = "mask", flags = cv2.WINDOW_NORMAL)
#cv2.imshow(winname = "mask", mat = mask)
#cv2.waitKey(delay = 0)

# use the mask to select the "interesting" part of the image
sel = cv2.bitwise_and(src1 = image, src2 = mask)

plt.imshow(sel)
57/7:
# blur and grayscale before thresholding
image= cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
57/8:
src=image
blur = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
blur = cv2.GaussianBlur(
    src = blur, 
    ksize = (5, 5), 
    sigmaX = 0)
57/9:
# perform adaptive thresholding 
(t, maskLayer) = cv2.threshold(src = blur, 
    thresh = 0, 
    maxval = 255, 
    type = cv2.THRESH_BINARY + cv2.THRESH_OTSU)
57/10:
# make a mask suitable for color images
mask = cv2.merge(mv = [maskLayer, maskLayer, maskLayer])

#cv2.namedWindow(winname = "mask", flags = cv2.WINDOW_NORMAL)
#cv2.imshow(winname = "mask", mat = mask)
#cv2.waitKey(delay = 0)

# use the mask to select the "interesting" part of the image
sel = cv2.bitwise_and(src1 = image, src2 = mask)

plt.imshow(sel)
57/11: img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
57/12:
 gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
  morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

  _, roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

  mask = np.zeros(img.shape, img.dtype)

  cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

  masked_image = cv2.bitwise_and(img, mask)

  return masked_image
57/13:
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/14: plt.imshow(masked_image)
57/15: plt.imshow(img)
57/16: plt.imshow(gray)
57/17:
gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/18: plt.imshow(gray)
57/19: img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg",-1)
57/20:
gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/21: plt.imshow(gray)
57/22: plt.imshow(img)
57/23:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
57/24: plt.imshow(img)
57/25:
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/26: plt.imshow(img)
57/27: plt.imshow(gray)
57/28:
gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/29: plt.imshow(gray)
57/30:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
57/31: plt.imshow(gray)
57/32: plt.imshow(img)
57/33: plt.imshow(img1)
57/34:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img_1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
57/35: plt.imshow(img1)
57/36: plt.imshow(img_1)
57/37: plt.imshow(img)
57/38: plt.imshow(gray)
57/39:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img_1, cv2.COLOR_RGB2GRAY)
57/40: plt.imshow(gray)
57/41:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img_1, cv2.COLOR_RGB2GRAY)
57/42: plt.imshow(gray)
57/43: plt.imshow(img)
57/44: plt.imshow(img_1)
57/45:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)
57/46: plt.imshow(img_1)
57/47: plt.imshow(gray)
50/428:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
#plt.imshow(gray, cmap = 'gray')
plt.imshow(gray)
50/429:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
#plt.imshow(gray, cmap = 'gray')
plt.imshow(gray)
50/430:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
50/431:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
50/432: import cv2
50/433:
# read image
image = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
50/434: type(image)
50/435: image.shape
50/436:
plt.imshow(image)
cv2.imwrite('BGR.jpg',image)
50/437:
# parse BRG to RGB
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# show image
plt.imshow(image)
50/438:
# parse image to greyscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# show image
#plt.imshow(gray, cmap = 'gray')
plt.imshow(gray)
57/48:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
img_1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img_1, cv2.COLOR_BGR2GRAY)


image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
gray_1 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
57/49: plt.imshow(gray)
57/50: plt.imshow(gray_1)
57/51:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
57/52: plt.imshow(gray)
57/53:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
57/54: plt.imshow(gray)
57/55:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
57/56: plt.imshow(gray)
57/57: img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
57/58: img.shape
57/59:


gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
57/60: cv2.imshow(img)
57/61: plt.imshow(img)
57/62:


gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
57/63: plt.imshow(gray)
57/64:


gray=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
57/65: plt.imshow(gray)
57/66:


rgb=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)
57/67: plt.imshow(gray)
57/68:
plt.imshow(gray)
gray.shape
57/69:
plt.imshow(gray,cmap="gray",)
gray.shape
57/70: plt.imshow(img,cmap="gray",)
57/71: plt.imshow(img,cmap="gray")
57/72:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_bgr2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/73:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/74: plt.imshow(mask)
57/75:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/76: plt.imshow(mask)
57/77:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 70, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/78: plt.imshow(mask)
57/79:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/80: plt.imshow(mask)
57/81:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 125, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/82: plt.imshow(mask)
57/83:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 110, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/84: plt.imshow(mask)
57/85: plt.imshow(masked_image)
57/86:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 110, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/87: plt.imshow(masked_image)
57/88:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
th, threshed = cv2.threshold(gray, 110, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/89: plt.imshow(masked_image)
57/90:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 110, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/91: plt.imshow(masked_image)
57/92:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/93: plt.imshow(masked_image)
57/94:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 230, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/95: plt.imshow(masked_image)
57/96:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 110, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/97: plt.imshow(masked_image)
57/98:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 190, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/99: plt.imshow(masked_image)
57/100:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/101: plt.imshow(masked_image)
57/102:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 140, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/103: plt.imshow(masked_image)
57/104:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/105: plt.imshow(masked_image)
57/106:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20,20))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/107: plt.imshow(masked_image)
57/108:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (21, 21), 0)

th, threshed = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY_INV)

kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)

roi, _ = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

mask = np.zeros(img.shape, img.dtype)

cv2.fillPoly(mask, roi, (255,)*img.shape[2], )

masked_image = cv2.bitwise_and(img, mask)
57/109: plt.imshow(masked_image)
57/110:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,3)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/111: plt.imshow(edges)
57/112:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/113: plt.imshow(edges)
57/114:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    #edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/115: plt.imshow(edges)
57/116:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+30
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    #edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/117: plt.imshow(edges)
57/118:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    #edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/119: plt.imshow(edges)
57/120:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/121:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/122:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/123: plt.imshow(edges)
57/124:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,6)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/125: plt.imshow(edges)
57/126:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,3)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/127: plt.imshow(edges)
57/128:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,3)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/129: plt.imshow(edges)
57/130:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,3)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/131: plt.imshow(edges)
57/132:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower + 130
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2,3)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/133: plt.imshow(edges)
57/134:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/135: plt.imshow(edges)
57/136:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2, 3)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/137: plt.imshow(edges)
57/138:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2, 9)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/139: plt.imshow(edges)
57/140:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2, 11)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/141: plt.imshow(edges)
57/142:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower +150
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/143: plt.imshow(edges)
57/144:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower + 200
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/145: plt.imshow(edges)
57/146:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower + 250
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/147: plt.imshow(edges)
57/148:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/149: plt.imshow(masked)
50/439:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    
    
    
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/150:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/151: plt.imshow(edges)
57/152:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/153: plt.imshow(masked)
57/154:
    BLUR = 21
    CANNY_THRESH_1 = 100
    CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/155: plt.imshow(edges)
57/156:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)

    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    

    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
57/157: plt.imshow(masked)
57/158:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/159: plt.imshow(edges)
57/160:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
57/161: plt.imshow(masked)
50/440:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower+20
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/441: plt.imshow(masked)
50/442:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/443: plt.imshow(masked)
57/162:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
57/163: plt.imshow(masked)
57/164:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
57/165: plt.imshow(edges)
57/166:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
57/167: plt.imshow(masked)
59/1:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
59/2: import cv2
59/3:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
59/4: plt.imshow(edges)
59/5:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
59/6: plt.imshow(masked)
59/7:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
59/8: plt.imshow(edges)
59/9:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
59/10: plt.imshow(masked)
59/11:
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    mask = np.zeros(edges.shape)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
    
     
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    img = img.astype('float32') / 255.0
    
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    masked = (masked * 255).astype('uint8')
59/12: plt.imshow(masked)
59/13:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/14: plt.imshow(masked)
59/15:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/16: plt.imshow(masked)
59/17: plt.imshow(edges)
59/18: plt.imshow(masked)
59/19:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    edges = thresh
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/20: plt.imshow(edges)
59/21: plt.imshow(masked)
59/22:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower + 200
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/23: plt.imshow(edges)
59/24: plt.imshow(masked)
59/25:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower -100
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/26: plt.imshow(edges)
59/27: plt.imshow(masked)
59/28:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower + 50
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/29: plt.imshow(edges)
59/30:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.cv.CHAIN_APPROX_SIMPLE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/31:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_SIMPLE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/32: plt.imshow(edges)
59/33: plt.imshow(masked)
59/34:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_FLOODFILL  , cv2.CHAIN_APPROX_SIMPLE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/35:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE  )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/36: plt.imshow(edges)
59/37: plt.imshow(masked)
59/38:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_TC89_L1 )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/39: plt.imshow(edges)
59/40: plt.imshow(masked)
59/41:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_CCOMP , cv2.CHAIN_APPROX_TC89_L1 )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/42: plt.imshow(edges)
59/43: plt.imshow(masked)
59/44:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_CCOMP , cv2.CHAIN_APPROX_NONE )
    
    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    mask = np.zeros(edges.shape)
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/45: plt.imshow(edges)
59/46: plt.imshow(masked)
59/47:
BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    plt.imshow(edges)
59/48:

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    plt.imshow(edges)
59/49:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
59/50: import cv2
59/51:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower 
CANNY_THRESH_2 = upper
        
edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
plt.imshow(edges)
59/52:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    
# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower 
CANNY_THRESH_2 = upper
        
edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
plt.imshow(edges)
59/53: img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
59/54:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
59/55:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
59/56:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower 
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
59/57: plt.imshow(edges)
59/58:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
59/59:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
59/60:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

plt.imshow(edges)
59/61:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,1, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/62:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/63:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(1):
    plt.subplot(1,1, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/64:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(1):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/65:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/66:
 Empty list
    contour_info = []

    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   


    for c in contours:
        

        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
  
    mask = np.zeros(edges.shape)
   
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
  
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/67: plt.imshow(contour)
59/68:

    contour_info = []

    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   


    for c in contours:
        

        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
  
    mask = np.zeros(edges.shape)
   
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
  
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/69:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
59/70:

    contour_info = []

    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   


    for c in contours:
        

        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
  
    mask = np.zeros(edges.shape)
   
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
  
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/71:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()

 contour_info = []

    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   


    for c in contours:
        

        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
  
    mask = np.zeros(edges.shape)
   
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
  
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
59/72:


BLUR = 21
#CANNY_THRESH_1 = 100
#CANNY_THRESH_2 = 200
MASK_DILATE_ITER = 10
MASK_ERODE_ITER = 10
MASK_COLOR = (0.0,0.0,0.0) # In BGR format
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# To find the optimal threshold value
v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper    

edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)

output = [rgb, edges]

titles = ['Original', 'Cannt Edge Detection']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()

contour_info = []

contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   

for c in contours: 

    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
max_contour = contour_info[0]
  
mask = np.zeros(edges.shape)
   
    
#for c in contour_info:
 #   cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
  
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
mask_stack = np.dstack([mask]*3)
    #
mask_stack  = mask_stack.astype('float32') / 255.0

    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
59/73: plt.imshow(contour)
59/74: plt.imshow(contours)
59/75: plt.imshow(contour_info)
50/444:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/445: plt.imshow(masked)
50/446:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/447:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
60/2:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/3:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/4: plt.imshow(output)
60/5:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/6:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/7:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
60/8:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/9:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/10:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/11:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
60/12:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/13: im.size
60/14:
im1 = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im2 = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
im1.size
im2.size
60/15:
im1 = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im2 = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
im1.size
#im2.size
60/16:
im1 = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im2 = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
#im1.size
im2.size
60/17: im1.shape
60/18:

im2.shape
60/19:
#im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
60/20:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/21:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    #im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/22:
#im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
60/23:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    #im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/24:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/25:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (2,2))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/26:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 10
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/27:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 5
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/28:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/one.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 5
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/29:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/one.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/30: im.shape
60/31:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
60/32:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, -1, (255,0,0), thickness = 2)
60/33:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, -1, (255,0,0), 2)

#cv2.drawContours(mask, cnt, 0, (0, 255, 0), -1)
60/34:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, 0, (255,0,0), 2)

#cv2.drawContours(mask, cnt, 0, (0, 255, 0), -1)
60/35:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

#cv2.drawContours(image, contours, -1, (255,0,0), 2)

contours = np.array(contours).reshape((-1,1,2)).astype(np.int32)
cv2.drawContours(image, [contours], -1, 255, 2)
60/36: plt.imshow(contours)
60/37: plt.imshow(image)
60/38:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

cv2.drawContours(image, contours, -1, (255,0,0), 2)

#contours = np.array(contours).reshape((-1,1,2)).astype(np.int32)
#cv2.drawContours(image, [contours], -1, 255, 2)
60/39:
image = cv2.imread('image.jpg',cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, -1, (255,0,0), thickness = 2)
60/40:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, -1, (255,0,0), thickness = 2)
60/41: plt.imshow(contours)
60/42:
    BLUR = 21
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

        
    image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hsv = cv2.split(hsv)
    gray = hsv[0]

    gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

    
    
    #v = np.median(gray)
    #sigma = 0.33
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    #CANNY_THRESH_1 = lower
    #CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    ret,edges = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)
    
    contour_info = []
    #contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    contours, __  = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
     
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
  
    mask = np.zeros(edges.shape)

    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    

    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/43:
    BLUR = 21
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

        
    image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hsv = cv2.split(hsv)
    gray = hsv[0]

    gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

    
    
    #v = np.median(gray)
    #sigma = 0.33
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    #CANNY_THRESH_1 = lower
    #CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    ret,edges = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)
    
    contour_info = []
    #contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    contours  = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
     
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
  
    mask = np.zeros(edges.shape)

    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    

    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/44:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
thresh1 =  cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
cv2.imshow('stalk_removed.jpg',stalk)
60/45:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
thresh1 =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
plt.imshow(stalk)
60/46:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
thresh1 =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
plt.imshow(stalk)
60/47:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
plt.imshow(stalk)
60/48:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(final)
60/49: plt.imshow(im)
60/50:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(final)
60/51:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(stalk)
60/52:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,10)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(stalk)
60/53:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,1)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(stalk)
60/54:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_BGR2RGB)
plt.imshow(stalk)
60/55:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(stalk)
60/56:
im = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/57: plt.imshow(im)
60/58:
im = cv2.imread("/Users/Hyunjee/Desktop/one.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/59:
im = cv2.imread("/Users/Hyunjee/Desktop/one.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(stalk)
60/60:
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(stalk)
60/61:
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/62:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/63:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/64:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (1,1), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/65:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(final)
60/66:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(stalk)
60/67:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]
gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(dist_transform)
60/68:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(img, np.uint8)
green[imask] = img[imask]
plt.imshow(green)
60/69:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(img, np.uint8)
green[imask] = img[imask]
plt.imshow(green)
60/70:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(image, np.uint8)
green[imask] = img[imask]
plt.imshow(green)
60/71:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(image, np.uint8)
green[imask] = image[imask]
plt.imshow(green)
60/72:
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(image, np.uint8)
green[imask] = image[imask]
plt.imshow(green)
60/73:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

## mask of green (36,25,25) ~ (86, 255,255)
# mask = cv2.inRange(hsv, (36, 25, 25), (86, 255,255))
mask = cv2.inRange(hsv, (36, 25, 25), (70, 255,255))

## slice the green
imask = mask>0
green = np.zeros_like(image, np.uint8)
green[imask] = image[imask]
plt.imshow(green)
60/74:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)
blurred = cv2.GaussianBlur(img, (5, 5), 0) 

#def edgedetect (channel):
sobelX = cv2.Sobel(channel, cv2.CV_16S, 1, 0)
sobelY = cv2.Sobel(channel, cv2.CV_16S, 0, 1)
sobel = np.hypot(sobelX, sobelY)

sobel[sobel > 255] = 255;
    
edgeImg = np.max( np.array([ edgedetect(blurred[:,:, 0]), edgedetect(blurred[:,:, 1]), edgedetect(blurred[:,:, 2]) ]), axis=0 )
60/75:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg",cv2.IMREAD_UNCHANGED)
blurred = cv2.GaussianBlur(img, (5, 5), 0) 

def edgedetect (channel):
    sobelX = cv2.Sobel(channel, cv2.CV_16S, 1, 0)
    sobelY = cv2.Sobel(channel, cv2.CV_16S, 0, 1)
    sobel = np.hypot(sobelX, sobelY)

    sobel[sobel > 255] = 255;
    
edgeImg = np.max( np.array([ edgedetect(blurred[:,:, 0]), edgedetect(blurred[:,:, 1]), edgedetect(blurred[:,:, 2]) ]), axis=0 )
60/76:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/77:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/78:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((10, 10)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/79:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/80:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/81:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=50, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/82:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/83:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=10, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/84:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=10, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/85:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=10, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/86:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 4)

print(total_area)
plt.imshow(image)
60/87:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    
    
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 10)

print(total_area)
plt.imshow(image)
60/88:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
    
    
    
    # Compute Euclidean distance from every binary pixel
    # to the nearest zero pixel then find peaks
    distance_map = ndimage.distance_transform_edt(thresh)
    local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

    # Perform connected component analysis then apply Watershed
    markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
    labels = watershed(-distance_map, markers, mask=thresh)

    # Iterate through unique labels
    total_area = 0
    for label in np.unique(labels):
        if label == 0:
            continue
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/89: plt.imshow(masked)
60/90:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
    
    
    
    # Compute Euclidean distance from every binary pixel
    # to the nearest zero pixel then find peaks
    distance_map = ndimage.distance_transform_edt(thresh)
    local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

    # Perform connected component analysis then apply Watershed
    markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
    labels = watershed(-distance_map, markers, mask=thresh)

    # Iterate through unique labels
    total_area = 0
    for label in np.unique(labels):
        if label == 0:
            continue
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(gray.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/91: plt.imshow(masked)
60/92:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    #edges = thresh
    
    
    
    # Compute Euclidean distance from every binary pixel
    # to the nearest zero pixel then find peaks
    distance_map = ndimage.distance_transform_edt(thresh)
    local_max = peak_local_max(distance_map, indices=False, min_distance=10, labels=thresh)

    # Perform connected component analysis then apply Watershed
    markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
    labels = watershed(-distance_map, markers, mask=thresh)

    # Iterate through unique labels
    total_area = 0
    for label in np.unique(labels):
        if label == 0:
            continue
    
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))
   
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/93: plt.imshow(masked)
60/94:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=40, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    
    
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 10)

print(total_area)
plt.imshow(image)
60/95:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    
    
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 10)

print(total_area)
plt.imshow(image)
60/96:
import cv2
import numpy as np
from skimage.feature import peak_local_max
from skimage.morphology import watershed
from scipy import ndimage

# Load in image, convert to gray scale, and Otsu's threshold
image = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)


v = np.median(gray)
sigma = 0.33
lower = int(max(0, (1.0 - sigma) * v))
upper = int(min(255, (1.0 + sigma) * v))

CANNY_THRESH_1 = lower
CANNY_THRESH_2 = upper
    
thresh = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
#thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

# Compute Euclidean distance from every binary pixel
# to the nearest zero pixel then find peaks
distance_map = ndimage.distance_transform_edt(thresh)
local_max = peak_local_max(distance_map, indices=False, min_distance=20, labels=thresh)

# Perform connected component analysis then apply Watershed
markers = ndimage.label(local_max, structure=np.ones((3, 3)))[0]
labels = watershed(-distance_map, markers, mask=thresh)

# Iterate through unique labels
total_area = 0
for label in np.unique(labels):
    if label == 0:
        continue

    # Create a mask
    
    
    mask = np.zeros(gray.shape, dtype="uint8")
    mask[labels == label] = 255

    # Find contours and determine contour area
    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cnts = cnts[0] if len(cnts) == 2 else cnts[1]
    c = max(cnts, key=cv2.contourArea)
    area = cv2.contourArea(c)
    total_area += area
    cv2.drawContours(image, [c], -1, (36,255,12), 10)

print(total_area)
plt.imshow(image)
50/448:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format
    
    img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    # To find the optimal threshold value
    #v = np.median(gray)
    #sigma = 0.33
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    #CANNY_THRESH_1 = lower
    #CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #ret, thresh = cv2.threshold(gray, 0.255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    #ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    cv2.fillConvexPoly(mask, max_contour[0], (255))

    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/449: plt.imshow(masked)
50/450:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/451:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/452:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    #CANNY_THRESH_1 = lower
    #CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/453:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/454:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    thresh = cv2.adaptiveThreshold(gray , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/455:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/456:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/457:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/458:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/459:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/97:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/98:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/99:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/100:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
50/460:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    thresh = cv2.adaptiveThreshold(s, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/461:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/101:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
#cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/102:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/103:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/104:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
#cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/105:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
#cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
60/106:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
##(2) convert to hsv-space, then split the channels
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
h,s,v = cv2.split(hsv)

##(3) threshold the S channel using adaptive method(`THRESH_OTSU`) or fixed thresh
th, threshed = cv2.threshold(s, 50, 255, cv2.THRESH_BINARY_INV)

##(4) find all the external contours on the threshed S
cnts = cv2.findContours(threshed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
canvas  = img.copy()
#cv2.drawContours(canvas, cnts, -1, (0,255,0), 1)

## sort and choose the largest contour
cnts = sorted(cnts, key = cv2.contourArea)
cnt = cnts[-1]

## approx the contour, so the get the corner points
arclen = cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, 0.02* arclen, True)
cv2.drawContours(canvas, [cnt], -1, (255,0,0), 1, cv2.LINE_AA)
#cv2.drawContours(canvas, [approx], -1, (0, 0, 255), 1, cv2.LINE_AA)

## Ok, you can see the result as tag(6)
plt.imshow(canvas)
50/462:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(s, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/463:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/107:
import scipy.ndimage as ndimage

def brightness_distortion(I, mu, sigma):
    return np.sum(I*mu/sigma**2, axis=-1) / np.sum((mu/sigma)**2, axis=-1)


def chromacity_distortion(I, mu, sigma):
    alpha = brightness_distortion(I, mu, sigma)[...,None]
    return np.sqrt(np.sum(((I - alpha * mu)/sigma)**2, axis=-1))

def bwareafilt ( image ):
    image = image.astype(np.uint8)
    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)
    sizes = stats[:, -1]

    max_label = 1
    max_size = sizes[1]
    for i in range(2, nb_components):
        if sizes[i] > max_size:
            max_label = i
            max_size = sizes[i]

    img2 = np.zeros(output.shape)
    img2[output == max_label] = 255

    return img2

img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plot.subplot(2,2,i+1),
    if i == 0 :
        plot.imshow(images[i])
    else :
        plot.imshow(images[i], cmap='gray')
    plot.title(titles[i])
    plot.xticks([]),plot.yticks([])
plot.show()
60/108:
import scipy.ndimage as ndimage

def brightness_distortion(I, mu, sigma):
    return np.sum(I*mu/sigma**2, axis=-1) / np.sum((mu/sigma)**2, axis=-1)


def chromacity_distortion(I, mu, sigma):
    alpha = brightness_distortion(I, mu, sigma)[...,None]
    return np.sqrt(np.sum(((I - alpha * mu)/sigma)**2, axis=-1))

def bwareafilt ( image ):
    image = image.astype(np.uint8)
    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)
    sizes = stats[:, -1]

    max_label = 1
    max_size = sizes[1]
    for i in range(2, nb_components):
        if sizes[i] > max_size:
            max_label = i
            max_size = sizes[i]

    img2 = np.zeros(output.shape)
    img2[output == max_label] = 255

    return img2

img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']


for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/109:
import scipy.ndimage as ndimage

def brightness_distortion(I, mu, sigma):
    return np.sum(I*mu/sigma**2, axis=-1) / np.sum((mu/sigma)**2, axis=-1)


def chromacity_distortion(I, mu, sigma):
    alpha = brightness_distortion(I, mu, sigma)[...,None]
    return np.sqrt(np.sum(((I - alpha * mu)/sigma)**2, axis=-1))

def bwareafilt ( image ):
    image = image.astype(np.uint8)
    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)
    sizes = stats[:, -1]

    max_label = 1
    max_size = sizes[1]
    for i in range(2, nb_components):
        if sizes[i] > max_size:
            max_label = i
            max_size = sizes[i]

    img2 = np.zeros(output.shape)
    img2[output == max_label] = 255

    return img2

img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']


for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/110:
import scipy.ndimage as ndimage

def brightness_distortion(I, mu, sigma):
    return np.sum(I*mu/sigma**2, axis=-1) / np.sum((mu/sigma)**2, axis=-1)


def chromacity_distortion(I, mu, sigma):
    alpha = brightness_distortion(I, mu, sigma)[...,None]
    return np.sqrt(np.sum(((I - alpha * mu)/sigma)**2, axis=-1))

def bwareafilt ( image ):
    image = image.astype(np.uint8)
    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)
    sizes = stats[:, -1]

    max_label = 1
    max_size = sizes[1]
    for i in range(2, nb_components):
        if sizes[i] > max_size:
            max_label = i
            max_size = sizes[i]

    img2 = np.zeros(output.shape)
    img2[output == max_label] = 255

    return img2

img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']


for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/111:
import scipy.ndimage as ndimage

def brightness_distortion(I, mu, sigma):
    return np.sum(I*mu/sigma**2, axis=-1) / np.sum((mu/sigma)**2, axis=-1)


def chromacity_distortion(I, mu, sigma):
    alpha = brightness_distortion(I, mu, sigma)[...,None]
    return np.sqrt(np.sum(((I - alpha * mu)/sigma)**2, axis=-1))

def bwareafilt ( image ):
    image = image.astype(np.uint8)
    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)
    sizes = stats[:, -1]

    max_label = 1
    max_size = sizes[1]
    for i in range(2, nb_components):
        if sizes[i] > max_size:
            max_label = i
            max_size = sizes[i]

    img2 = np.zeros(output.shape)
    img2[output == max_label] = 255

    return img2

img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']


for i in range(5):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/112:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/113: plt.imshow(total)
60/114:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/115: plt.imshow(masked)
60/116:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/117: plt.imshow(total)
60/118:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/119: plt.imshow(masked)
60/120:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/121: plt.imshow(total)
60/122:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/123: plt.imshow(masked)
60/124:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/125: plt.imshow(total)
60/126:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/127: plt.imshow(masked)
60/128:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
plt.imshow(sat)
60/129:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
plt.imshow(sat)
60/130:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,3]
plt.imshow(sat)
60/131:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
plt.imshow(sat)
60/132: img.shape
60/133: img.shape[:,:,0]
60/134:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/135: plt.imshow(total)
60/136:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/137: plt.imshow(masked)
60/138:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
60/139:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/140:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 401, 10);

#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/141: plt.imshow(total)
60/142:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/143:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/144:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/145: plt.imshow(total)
60/146:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
   # mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/147:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/148:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 50, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 50, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 50, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 50, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/149:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/150: plt.imshow(total)
60/151:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/152:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/153:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/154: plt.imshow(total)
60/155:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/156:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/157:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/158:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_v = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/159:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/160: plt.imshow(total)
60/161:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/162:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/163:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/164: plt.imshow(total)
60/165:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/166:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/167:
images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/168:
images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/169:
images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/170:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
60/171:
images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/172:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/173:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/174:
images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/175:
images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/176:
images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plot.subplot(2,2,i+1),
    if i == 0 :
        plot.imshow(images[i])
    else :
        plot.imshow(images[i], cmap='gray')
    plot.title(titles[i])
    plot.xticks([]),plot.yticks([])
plot.show()
60/177:
images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
  plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plot.show()
60/178:
images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plot.show()
60/179:
images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plot.show()
60/180:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

#total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plot.show()
60/181:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

#total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plot.show()
60/182:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

#total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/183:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/184:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/185:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/186:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/187:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/188:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/189:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/190:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_S, cv2.bitwise_and(thresh2_V))

images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/191:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

images = [img, thresh_S, thresh_V, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'S + V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/192:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/193:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/194:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/195:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 90, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 90, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/196:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 20, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 20, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/197:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/198:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/199:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/200:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/201:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/202:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf1.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/203:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/204:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/205:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/206:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/207:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/208:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/209:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/210:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/211:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/212:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/213:
    edges = thresh_S

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/214:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/215:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/216:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/217:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/218:
    edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/219:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/220:
    edges = total1

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/221:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/222:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/223:
    edges = total1

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/224:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/225:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

#thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/226:
    edges = total1

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/227:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/228:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

#thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

#thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
#thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']

total = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))
total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

images = [img, thresh_S, thresh_V, total, total]
titles = ['Original Image', 'Mask S', 'Mask V', 'V + S','S+V']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/229:
    edges = total1

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
60/230:
output = [img, total, masked]
titles = ['Original Image', 'masked','final']
for i in range(3):
    plt.subplot(1,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/231:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

    contour_info = []
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

    for c in contours:
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
    max_contour = contour_info[0]

    mask = np.zeros(edges.shape)

    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
        
    BLUR = 21
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
    mask_stack = np.dstack([mask]*3)
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(5):
    plt.subplot(3,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/232:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(5):
    plt.subplot(3,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/233:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(3,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/234:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(1,6, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/235:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(6,1, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/236:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

#thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
#thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(1,6, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/237:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)

thresh_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]


thresh2_S = cv2.threshold(sat, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
thresh2_V = cv2.threshold(val, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(1,6, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/238:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)



thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(chrom255_S , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(chrom255_V , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);



#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(6):
    plt.subplot(1,6, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/239:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/240:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S))



edges = total1

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/241:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_V, cv2.bitwise_not(thresh2_S), cv2.bitwise_not(thresh2_H))



edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/242:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V), cv2.bitwise_not(thresh2_H))



edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','V+S','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/243:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V), cv2.bitwise_not(thresh2_H))



edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','plus','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/244:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_H, cv2.bitwise_not(thresh2_S), cv2.bitwise_not(thresh2_V))



edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','plus','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/245:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_H, cv2.bitwise_not(thresh2_S), cv2.bitwise_not(thresh2_V))



edges = total1

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','plus','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/246:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))



edges = total1

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V', 'S+V','plus','Final']
for i in range(7):
    plt.subplot(1,7, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/247:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))



edges = total1

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/248:
images = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/249:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total1

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    


                

images = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/250:
images = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/251:
edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/252:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))



                

images = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/253:
images = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/254:
edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/255:
edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/256:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))



                

images = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/257:
images = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/258:
edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/259:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/260:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/261:
images1 = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images1[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/262:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/263:

    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/264:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

#total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
#total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
#total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/265:
images1 = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images1[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/266:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/267:

    
images = [img, thresh_H, thresh_S, thresh_V, total, total1,masked ]
titles = ['original','S+V','plus','Final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/268:

    
images = [img,masked ]
titles = ['original','Final']
for i in range(2):
    plt.subplot(1,1, i+1)
    plt.imshow(images[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/269:

    
images2 = [img,masked ]
titles = ['original','Final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/270:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total3

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/271:
images1 = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images1[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/272:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/273:

    
images2 = [img,masked ]
titles = ['original','Final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/274:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/275:
images1 = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images1[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/276:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/277:

    
images2 = [img,masked ]
titles = ['original','Final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/278:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))
60/279:
images2 = [img,sav,sah,sov,soh,sxv,sxh]
titles = ['img','sav','sah','sov','soh','sxv','sxh']
for i in range(7):
    plt.subplot(3,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/280:
images2 = [img,savah,savoh,saxoh]
titles = ['img','savah','savoh','saxoh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/281:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))
60/282:
images2 = [img,savah,savoh,saxoh]
titles = ['img','savah','savoh','saxoh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/283:
images2 = [img,savah,savoh,saxoh]
titles = ['img','savah','savoh','savxh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/284:
images2 = [img,savah,savoh,savxh]
titles = ['img','savah','savoh','savxh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/285:
images2 = [img,sahav,sahov,sahxv]
titles = ['img','sahav','sahov','sahxv']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/286:
images2 = [img,sovah,sovoh,sovxh]
titles = ['img','sovah','sovoh','sovxh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/287:
images2 = [img,sohav,sohov,sohxv]
titles = ['img','sohav','sohov','sohxv']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/288:
images2 = [img,sxvah,sxvoh,sxvxh]
titles = ['img','sxvah','sxvoh','sxvxh']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/289:
images2 = [img,sxhav,sxhov,sxhxv]
titles = ['img','sxhav','sxhov','sxhxv']
for i in range(4):
    plt.subplot(2,4, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/290:
images2 = [img,sxhov,sxvoh,sohov,sovoh]
titles = ['img','sxhov','sxvoh','sohov','sovoh']
for i in range(5):
    plt.subplot(1,5, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/291:
edges = sohov

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
plt.imshow(masked)
60/292:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))
60/293:
images2 = [img,sxhov,sxvoh,sohov,sovoh]
titles = ['img','sxhov','sxvoh','sohov','sovoh']
for i in range(5):
    plt.subplot(1,5, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/294:
edges = sohov

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
plt.imshow(masked)
60/295:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/296:
images1 = [img, thresh_H, thresh_S, thresh_V]
titles = ['Original Image', 'Mask H','Mask S', 'Mask V']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images1[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/297:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/298:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(ssohov, sahav)
60/299:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
60/300: plt.imshow(sample)
60/301:
images2 = [img,sample]
titles = ['img','sample']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/302:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
60/303:
images2 = [img,sample]
titles = ['img','sample']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/304:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_or(sohov, sahav)
60/305:
images2 = [img,sample]
titles = ['img','sample']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/306:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_and(sohov, sahav)
60/307:
images2 = [img,sample]
titles = ['img','sample']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/308:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
60/309:
images2 = [img,sample]
titles = ['img','sample']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/310:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
60/311:
images2 = [img,sample, not1, not2]
titles = ['img','sample','not1','not2']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/312:
images2 = [img,sohov, not1, not2,sahav]
titles = ['img','sohov','not1','not2','sahav']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/313:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
sample1 = cv2.bitwise_or(sohov, not2)
60/314:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/315:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/316:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
sample1 = cv2.bitwise_xor(sohov, not2)
60/317:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/318:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
sample1 = cv2.bitwise_or(sohov, not2)
60/319:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/320:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
sample1 = cv2.bitwise_xor(sahav, not1)
60/321:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/322:
edges = sohov

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
plt.imshow(masked)
60/323:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/324:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/325:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/326:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 199, 5);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/327:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
60/328:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/329:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/330:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
img =cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/331:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
#BLUR = 21
#mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/332:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/333:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/334:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total2

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/335:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total3

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/336:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
sav = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))
sah = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))
sov = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_V))
soh = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))
sxv = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_V))
sxh = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))

savah = cv2.bitwise_and(sav, cv2.bitwise_not(thresh2_H))
savoh = cv2.bitwise_or(sav, cv2.bitwise_not(thresh2_H))
savxh = cv2.bitwise_xor(sav, cv2.bitwise_not(thresh2_H))

sahav = cv2.bitwise_and(sah, cv2.bitwise_not(thresh2_V))
sahov = cv2.bitwise_or(sah, cv2.bitwise_not(thresh2_V))
sahxv = cv2.bitwise_xor(sah, cv2.bitwise_not(thresh2_V))

sovah = cv2.bitwise_and(sov, cv2.bitwise_not(thresh2_H))
sovoh = cv2.bitwise_or(sov, cv2.bitwise_not(thresh2_H))
sovxh = cv2.bitwise_xor(sov, cv2.bitwise_not(thresh2_H))

sohav = cv2.bitwise_and(soh, cv2.bitwise_not(thresh2_V))
sohov = cv2.bitwise_or(soh, cv2.bitwise_not(thresh2_V))
sohxv = cv2.bitwise_xor(soh, cv2.bitwise_not(thresh2_V))

sxvah = cv2.bitwise_and(sxv, cv2.bitwise_not(thresh2_H))
sxvoh = cv2.bitwise_or(sxv, cv2.bitwise_not(thresh2_H))
sxvxh = cv2.bitwise_xor(sxv, cv2.bitwise_not(thresh2_H))

sxhav = cv2.bitwise_and(sxh, cv2.bitwise_not(thresh2_V))
sxhov = cv2.bitwise_or(sxh, cv2.bitwise_not(thresh2_V))
sxhxv = cv2.bitwise_xor(sxh, cv2.bitwise_not(thresh2_V))

sample = cv2.bitwise_xor(sohov, sahav)
not1 = cv2.bitwise_not(sohov)
not2 = cv2.bitwise_not(sahav)
sample1 = cv2.bitwise_xor(sahav, not1)
60/337:
images2 = [img,sohov, not1, not2,sahav, sample1]
titles = ['img','sohov','not1','not2','sahav','s']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/338:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
total = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

total1 = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total2 = cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))
total3 = cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H), cv2.bitwise_not(thresh2_V))

edges = total3

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')

plt.imshow(masked)
60/339:
images2 = [img,total, total1, total2, total3]
titles = ['Original Image', 'h+s','and', 'or','xor']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(images2[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/340:
import cv2 
import numpy as np 

img = cv2.iimread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1] 
sat = cv2.medianBlur(sat, 11) 
thresh = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10); 


h, w = img.shape[:2] 
bgdModel = np.zeros((1,65),np.float64) 
fgdModel = np.zeros((1,65),np.float64) 
grabcut_mask = thresh/255*3 #background should be 0, probable foreground = 3 
cv2.grabCut(img, grabcut_mask,(0,0,w,h),bgdModel,fgdModel,5,cv2.GC_INIT_WITH_MASK) 
grabcut_mask = np.where((grabcut_mask ==2)|(grabcut_mask ==0),0,1).astype('uint8')
60/341:
import cv2 
import numpy as np 

img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1] 
sat = cv2.medianBlur(sat, 11) 
thresh = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10); 


h, w = img.shape[:2] 
bgdModel = np.zeros((1,65),np.float64) 
fgdModel = np.zeros((1,65),np.float64) 
grabcut_mask = thresh/255*3 #background should be 0, probable foreground = 3 
cv2.grabCut(img, grabcut_mask,(0,0,w,h),bgdModel,fgdModel,5,cv2.GC_INIT_WITH_MASK) 
grabcut_mask = np.where((grabcut_mask ==2)|(grabcut_mask ==0),0,1).astype('uint8')
50/464:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(s, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/465:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/466:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(s, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/467:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(s, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/468:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/469:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/470:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/471:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/472:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/473:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/474:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
50/475:
    BLUR = 21
    #CANNY_THRESH_1 = 100
    #CANNY_THRESH_2 = 200
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

    img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    h,s,v = cv2.split(hsv)
    
    
    # To find the optimal threshold value
    v = np.median(gray)
    sigma = 0.33
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))

    CANNY_THRESH_1 = lower
    CANNY_THRESH_2 = upper
        
    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    edges = cv2.dilate(edges, None)
    edges = cv2.erode(edges, None)
    
    #thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, 10);
    #thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    #edges = thresh
    
    # ---- Find contour in edges, sort by area---------------------------------
    
    # Empty list
    contour_info = []
    
    # 1st argument: source image, 2nd argument: contour retrieval mode, 
    #  3rd argument: contour approximation method
    # contour: list of all the contours in the image
    # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object
    # cv2.CHAIN_APPROX_NONE: all the boundary points are stored. 
    contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
   
    
   # Loop over contour

    for c in contours:
        
        # contour_info.append: Expand the empty list with c, cv2.isContourConvex, cv2.contourArea
        # c = countour, input vector of 2D points stored in Nx2 numpy array
        # cv2.isContourConvex: To test input contour is convex or not
        #                      Contour must be simple w/o self-intersections. 
        #                      If not,the function output is undefined
        # cv2.ContourArea: To compute the contour area using Green formula
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
        
        # Sort contour_info with key element (2nd element) in reverse direction
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
    
    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----
    # Mask is black (rgb = 0), polygon is white (rgb = 255)
    
    # np.zeros: Return a new array of given shape and type, filled with zeros.
    # edges.shpae = 256 x 256
    mask = np.zeros(edges.shape)
    # cv2.fillConvexPoly: Fills the convex polygon
    # cv.fillConvexPoly(img, points, color[, lineType[, shift]] )
    # points = polygon vertices (highest point)
    #cv2.fillConvexPoly(mask, max_contour[0], (255))
    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    
    
    
    #-- Smooth mask, then blur it --------------------------------------------------------
    #mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)
    #mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)
    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    # np.dstack([mask]*3): To create 3channel alpha mask 
    #                      Expand the mask from 2 dimenstion to 3 dimension 
    #                      (grayscale -> color)
    mask_stack = np.dstack([mask]*3)
    #
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
50/476:
output = [rgb, masked]

titles = ['Original', 'Background Removal']

for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/342:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
60/343:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'f']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/344:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(5):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/345:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/346:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/347:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_xor(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/348:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_or(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/349:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_xor(thresh2_S, thresh2_H]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/350:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_xor(thresh2_S, thresh2_H)]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/351:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/352:
img = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
60/353:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/354:
edges = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
plt.imshow(masked)
60/355:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

cv2.imshow('image', final )

k = cv2.waitKey(0)

if k==27:
    cv2.destroyAllWindows()
60/356:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

plt.imshow( final )
60/357:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

plt.imshow( final )
60/358:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

plt.imshow( final )
60/359:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

plt.imshow( final )
60/360:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges

plt.imshow( final )
60/361:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/362:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [255,255,255]).all(axis = 2))] = [0,0,0]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/363:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/364:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/365:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (5,5,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/366:
imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (5,5,width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/367:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (5,5,width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/368:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (5,5,width-5,height-5)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/369:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-7,height-7)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/370:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample1.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-7,height-7)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/371:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-7,height-7)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/372:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-7,height-7)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/373:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/374:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-7,height-7)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/375:
img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,1]
val = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,0]
sat = cv2.medianBlur(sat, 11)
val = cv2.medianBlur(val, 11)
hue = cv2.medianBlur(hue, 11)

thresh_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);

mean_S, stdev_S = cv2.meanStdDev(img, mask = 255 - thresh_S)
mean_S = mean_S.ravel().flatten()
stdev_S = stdev_S.ravel()
chrom_S = chromacity_distortion(img, mean_S, stdev_S)
chrom255_S = cv2.normalize(chrom_S, chrom_S, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_V, stdev_V = cv2.meanStdDev(img, mask = 255 - thresh_V)
mean_V = mean_V.ravel().flatten()
stdev_V = stdev_V.ravel()
chrom_V = chromacity_distortion(img, mean_V, stdev_V)
chrom255_V = cv2.normalize(chrom_V, chrom_V, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

mean_H, stdev_H = cv2.meanStdDev(img, mask = 255 - thresh_H)
mean_H = mean_H.ravel().flatten()
stdev_H = stdev_H.ravel()
chrom_H = chromacity_distortion(img, mean_H, stdev_H)
chrom255_H = cv2.normalize(chrom_H, chrom_H, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX).astype(np.uint8)[:,:,None]

thresh2_S = cv2.adaptiveThreshold(sat , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_V = cv2.adaptiveThreshold(val , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);
thresh2_H = cv2.adaptiveThreshold(hue , 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 401, -10);


#output = [img, thresh_S, thresh_V, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))]
#titles = ['Original Image', 'Mask S', 'Mask V', 'S + V', 'f']
60/376:

output = [img, thresh_S, thresh_V,thresh_H, cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V)),cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_H))]
titles = ['Original Image', 'Mask S', 'Mask V','Mask H', 'S + V', 'S+H']
for i in range(6):
    plt.subplot(2,3, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/377:
edges = cv2.bitwise_and(thresh2_S, cv2.bitwise_not(thresh2_V))

contour_info = []
contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )

for c in contours:
    contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)  
max_contour = contour_info[0]

mask = np.zeros(edges.shape)

for c in contour_info:
    cv2.fillConvexPoly(mask, c[0], (255))
        
BLUR = 21
mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)
mask_stack = np.dstack([mask]*3)
mask_stack  = mask_stack.astype('float32') / 255.0
print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
img = img.astype('float32') / 255.0
    # Blending
masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
masked = (masked * 255).astype('uint8')
plt.imshow(masked)
60/378:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (2,2,width-3,height-3)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/379:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-3,height-3)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/380:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (1,1,width-3,height-3)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/381:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (1,1,width,height)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/382:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample1.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (1,1,width,height)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/383:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (1,1,width,height)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/384:
imgo = cv2.imread("/Users/Hyunjee/Desktop/one.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/385:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample1.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/386:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/387:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/388:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/389:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
60/390:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
rect = (10,10,width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/391:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (100,90, 150, 70)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/392:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-30,height-30)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/393:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-50,height-50)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/394:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-20,height-20)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/395:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/396:
    im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/397:
    im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    #if external_mask is not None:
        #mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/398:
    im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 10
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    #if external_mask is not None:
        #mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/399:
    im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 50
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    #if external_mask is not None:
        #mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/400:
    im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 55
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    #if external_mask is not None:
        #mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/401:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 55
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/402: mask_leaf("/Users/Hyunjee/Desktop/sample3.jpg", external_mask=None)
60/403:
import hdbscan
from collections import Counter


def hdbscan_mask(im_name):

    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    indices = np.dstack(np.indices(im.shape[:2]))
    data = np.concatenate((indices, im), axis=-1)
    data = data[:,2:]

    data = imb.reshape(im.shape[0]*im.shape[1], 3)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=20)
    clusterer.fit(data)

    plt.figure(figsize=(10,10))
    plt.imshow(clusterer.labels_.reshape(im.shape[0:2]))
    plt.show()

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    cv2.circle(mask, (int(width/2.), int(height/2.)), 100, 1, -3) #possible fg

    #grab cluster number for circle
    vals_im = clusterer.labels_.reshape(im.shape[0:2])

    vals = vals_im[mask == 1]
    commonvals = []
    cnts = Counter(vals)
    for v, count in cnts.most_common(20):
    #print '%i: %7d' % (v, count)
    if v == -1:
        continue
    commonvals.append(v)

    tst = np.in1d(vals_im, np.array(commonvals))
    tst = tst.reshape(vals_im.shape)

    hmask = tst.astype(np.uint8)

    plt.figure(figsize=(10,10))
    plt.imshow(hmask)
    plt.show()

    return hmask

hmask = hdbscan_mask("/Users/Hyunjee/Desktop/sample3.jpg")
60/404:
import hdbscan
from collections import Counter


def hdbscan_mask(im_name):

    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    indices = np.dstack(np.indices(im.shape[:2]))
    data = np.concatenate((indices, im), axis=-1)
    data = data[:,2:]

    data = imb.reshape(im.shape[0]*im.shape[1], 3)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=20)
    clusterer.fit(data)

    plt.figure(figsize=(10,10))
    plt.imshow(clusterer.labels_.reshape(im.shape[0:2]))
    plt.show()

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    cv2.circle(mask, (int(width/2.), int(height/2.)), 100, 1, -3) #possible fg

    #grab cluster number for circle
    vals_im = clusterer.labels_.reshape(im.shape[0:2])

    vals = vals_im[mask == 1]
    commonvals = []
    cnts = Counter(vals)
    for v, count in cnts.most_common(20):
    #print '%i: %7d' % (v, count)
        if v == -1:
            continue
    commonvals.append(v)

    tst = np.in1d(vals_im, np.array(commonvals))
    tst = tst.reshape(vals_im.shape)

    hmask = tst.astype(np.uint8)

    plt.figure(figsize=(10,10))
    plt.imshow(hmask)
    plt.show()

    return hmask

hmask = hdbscan_mask("/Users/Hyunjee/Desktop/sample3.jpg")
60/405: mask_leaf("/Users/Hyunjee/Desktop/one.jpg", external_mask=None)
60/406:
import hdbscan
from collections import Counter


def hdbscan_mask(im_name):

    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    indices = np.dstack(np.indices(im.shape[:2]))
    data = np.concatenate((indices, im), axis=-1)
    data = data[:,2:]

    data = imb.reshape(im.shape[0]*im.shape[1], 3)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=20)
    clusterer.fit(data)

    plt.figure(figsize=(10,10))
    plt.imshow(clusterer.labels_.reshape(im.shape[0:2]))
    plt.show()

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    cv2.circle(mask, (int(width/2.), int(height/2.)), 100, 1, -3) #possible fg

    #grab cluster number for circle
    vals_im = clusterer.labels_.reshape(im.shape[0:2])

    vals = vals_im[mask == 1]
    commonvals = []
    cnts = Counter(vals)
    for v, count in cnts.most_common(20):
    #print '%i: %7d' % (v, count)
        if v == -1:
            continue
    commonvals.append(v)

    tst = np.in1d(vals_im, np.array(commonvals))
    tst = tst.reshape(vals_im.shape)

    hmask = tst.astype(np.uint8)

    plt.figure(figsize=(10,10))
    plt.imshow(hmask)
    plt.show()

    return hmask

hmask = hdbscan_mask("/Users/Hyunjee/Desktop/sample3.jpg")
60/407:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 30
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/408: mask_leaf("/Users/Hyunjee/Desktop/one.jpg", external_mask=None)
60/409:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 60
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/410: mask_leaf("/Users/Hyunjee/Desktop/one.jpg", external_mask=None)
60/411:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/412: mask_leaf("/Users/Hyunjee/Desktop/one.jpg", external_mask=None)
60/413:
import hdbscan
from sklearn.datasets import make_blobs
from collections import Counter


def hdbscan_mask(im_name):

    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    indices = np.dstack(np.indices(im.shape[:2]))
    data = np.concatenate((indices, im), axis=-1)
    data = data[:,2:]

    data = imb.reshape(im.shape[0]*im.shape[1], 3)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=20)
    clusterer.fit(data)

    plt.figure(figsize=(10,10))
    plt.imshow(clusterer.labels_.reshape(im.shape[0:2]))
    plt.show()

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    cv2.circle(mask, (int(width/2.), int(height/2.)), 100, 1, -3) #possible fg

    #grab cluster number for circle
    vals_im = clusterer.labels_.reshape(im.shape[0:2])

    vals = vals_im[mask == 1]
    commonvals = []
    cnts = Counter(vals)
    for v, count in cnts.most_common(20):
    #print '%i: %7d' % (v, count)
        if v == -1:
            continue
    commonvals.append(v)

    tst = np.in1d(vals_im, np.array(commonvals))
    tst = tst.reshape(vals_im.shape)

    hmask = tst.astype(np.uint8)

    plt.figure(figsize=(10,10))
    plt.imshow(hmask)
    plt.show()

    return hmask

hmask = hdbscan_mask("/Users/Hyunjee/Desktop/sample3.jpg")
60/414:
import hdbscan
from sklearn.datasets import make_blobs
from collections import Counter


def hdbscan_mask(im_name):

    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    indices = np.dstack(np.indices(im.shape[:2]))
    data = np.concatenate((indices, im), axis=-1)
    data = data[:,2:]

    data = imb.reshape(im.shape[0]*im.shape[1], 3)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, min_samples=20)
    clusterer.fit(data)

    plt.figure(figsize=(10,10))
    plt.imshow(clusterer.labels_.reshape(im.shape[0:2]))
    plt.show()

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    cv2.circle(mask, (int(width/2.), int(height/2.)), 100, 1, -3) #possible fg

    #grab cluster number for circle
    vals_im = clusterer.labels_.reshape(im.shape[0:2])

    vals = vals_im[mask == 1]
    commonvals = []
    cnts = Counter(vals)
    for v, count in cnts.most_common(20):
    #print '%i: %7d' % (v, count)
        if v == -1:
            continue
    commonvals.append(v)

    tst = np.in1d(vals_im, np.array(commonvals))
    tst = tst.reshape(vals_im.shape)

    hmask = tst.astype(np.uint8)

    plt.figure(figsize=(10,10))
    plt.imshow(hmask)
    plt.show()

    return hmask

hmask = hdbscan_mask("/Users/Hyunjee/Desktop/sample3.jpg")
60/415:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(im)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/416: mask_leaf("/Users/Hyunjee/Desktop/one.jpg", external_mask=None)
60/417: mask_leaf("/Users/Hyunjee/Desktop/leaf.jpg", external_mask=None)
60/418: mask_leaf("/Users/Hyunjee/Desktop/sample1.jpg", external_mask=None)
60/419: mask_leaf("/Users/Hyunjee/Desktop/sample2.jpg", external_mask=None)
60/420: mask_leaf("/Users/Hyunjee/Desktop/sample3.jpg", external_mask=None)
60/421:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
#cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_MASK)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/422:
def mask_leaf(im_name, external_mask=None):
    #im = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
    im = cv2.imread(im_name)
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
    if external_mask is not None:
        mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_RECT)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(im)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()
60/423: mask_leaf("/Users/Hyunjee/Desktop/sample3.jpg", external_mask=None)
60/424:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/425:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/426:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final]
titles = ['Original ', 'final']
for i in range(2):
    plt.subplot(1,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/427:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final, img1,mask]
titles = ['Original ', 'final']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/428:
imgo = cv2.imread("/Users/Hyunjee/Desktop/sample2.jpg")
height, width = imgo.shape[:2]

#Create a mask holder
mask = np.zeros(imgo.shape[:2],np.uint8)

#Grab Cut the object
bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

#Hard Coding the Rect The object must lie within this rect.
#rect = (10,10,width-30,height-30)
rect = (2,2, width-10,height-10)
cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img1 = imgo*mask[:,:,np.newaxis]

#Get the background
background = imgo - img1

#Change all pixels in the background that are not black to white
background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

#Add the background and the image
final = background + img1

#To be done - Smoothening the edges


output = [imgo, final, img1,mask]
titles = ['Original ', 'final','1','2']
for i in range(4):
    plt.subplot(2,2, i+1)
    plt.imshow(output[i], cmap = 'gray')
    plt.title(titles[i])
    plt.xticks([])
    plt.yticks([])
    
plt.show()
imgo.shape
60/429:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final, img1,mask]
    titles = ['Original ', 'final','1','2']
    for i in range(4):
        plt.subplot(2,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/leaf.jpg")
60/430:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final, img1,mask]
    titles = ['Original ', 'final','1','2']
    for i in range(4):
        plt.subplot(2,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg")
60/431:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final, img1,mask]
    titles = ['Original ', 'final','1','2']
    for i in range(4):
        plt.subplot(2,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/leaf3.jpg")
60/432:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final, img1,mask]
    titles = ['Original ', 'final','1','2']
    for i in range(4):
        plt.subplot(2,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/one.jpg")
60/433:
img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
sat = cv2.cvtColor(img, cv2.COLOR_BGR2HSI)[:,:,1]
inte = cv2.cvtColor(img, cv2.COLOR_BGR2HSI)[:,:,2]
hue = cv2.cvtColor(img, cv2.COLOR_BGR2HSI)[:,:,0]
sat = cv2.medianBlur(sat, 11)
inte = cv2.medianBlur(inte, 11)
hue = cv2.medianBlur(hue, 11)
60/434:

img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
#sharp_img = cv.bgsegm.createBackgroundSubtractorMOG().apply(img)
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
plt.imshow(sharp_img)
60/435:

img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
sharp_img = cv.bgsegm.createBackgroundSubtractorMOG().apply(img)
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
plt.imshow(sharp_img)
60/436:

img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
sharp_img = cv2.bgsegm.createBackgroundSubtractorMOG().apply(img)
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
plt.imshow(sharp_img)
60/437:

img = cv2.imread("/Users/Hyunjee/Desktop/sample3.jpg")
#sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
#sharp_img = cv2.bgsegm.createBackgroundSubtractorMOG().apply(img)
sharp_img = cv2.createBackgroundSubtractorMOG2().apply(img)
plt.imshow(sharp_img)
61/1:
import cv2
import numpy as np
import time
from matplotlib import pyplot as plt

from utils import *
from review import files


# constants for filling holes mode
FILL = {
    'NO': 0,
    'FLOOD': 1,
    'THRESHOLD': 2,
    'MORPH': 3,
}


def remove_whites(image, marker):
    """
    Remove pixels resembling white from marker as background
    Args:
        image:
        marker: to be overloaded with white pixels to be removed
    Returns:
        nothing
    """
    # setup the white remover to process logical_and in place
    white_remover = np.full((image.shape[0], image.shape[1]), True)

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 0] > 200)
    white_remover[image[:, :, 0] <= 200] = False # blue channel

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 1] > 220)
    white_remover[image[:, :, 1] <= 220] = False  # green channel

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 2] > 200)
    white_remover[image[:, :, 2] <= 200] = False  # red channel

    # remove whites from marker
    marker[white_remover] = False


def remove_blacks(image, marker):
    """
    Remove pixels resembling black from marker as background
    Args:
        image:
        marker: to be overloaded with black pixels to be removed
    Returns:
        nothing
    """
    # setup the black remover to process logical_and in place
    black_remover = np.full((image.shape[0], image.shape[1]), True)

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 0] < 30)
    black_remover[image[:, :, 0] >= 30] = False  # blue channel

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 1] < 30)
    black_remover[image[:, :, 1] >= 30] = False  # green channel

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 2] < 30)
    black_remover[image[:, :, 2] >= 30] = False  # red channel

    # remove blacks from marker
    marker[black_remover] = False


def remove_blues(image, marker):
    """
    Remove pixels resembling blues better than green from marker as background
    Args:
        image:
        marker: to be overloaded with blue pixels to be removed
    Returns:
        nothing
    """
    # choose pixels that have higher blue than green
    blue_remover = image[:, :, 0] > image[:, :, 1]

    # remove blues from marker
    marker[blue_remover] = False


def color_index_marker(color_index_diff, marker):
    """
    Differentiate marker based on the difference of the color indexes
    Threshold below some number(found empirically based on testing on 5 photos,bad)
    If threshold number is getting less, more non-green image
     will be included and vice versa
    Args:
        color_index_diff: color index difference based on green index minus red index
        marker: marker to be updated
    Returns:
        nothing
    """
    marker[color_index_diff <= -0.05] = False


def texture_filter(image, marker, threshold=220, window=3):
    """
    Update marker based on texture of an image
    Args:
        image (ndarray of grayscale image):
        marker (ndarray size of image): marker to be updated
        threshold (number): minimum size of texture measurement(entropy) allowed
        window (int): window size of a square the texture computed from
    Returns: nothing
    """

    window = window - window//2 - 1
    for x in range(0, image.shape[0]):
        for y in range(0, image.shape[1]):
            # print('x y', x, y)
            # print('window', image[x:x + window, y:y + window])
            x_start = x - window if x < window else x
            y_start = y - window if y < window else y
            x_stop = x + window if x < image.shape[0] - window else image.shape[0]
            y_stop = y + window if y < image.shape[1] - window else image.shape[1]

            local_entropy = np.sum(image[x_start:x_stop, y_start:y_stop]
                                   * np.log(image[x_start:x_stop, y_start:y_stop] + 1e-07))
            # print('entropy', local_entropy)
            if local_entropy > threshold:
                marker[x, y] = False


def otsu_color_index(excess_green, excess_red):
    return cv2.threshold(excess_green - excess_red, 0, 255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)


def generate_floodfill_mask(bin_image):
    """
    Generate a mask to remove backgrounds adjacent to image edge
    Args:
        bin_image (ndarray of grayscale image): image to remove backgrounds from
    Returns:
        a mask to backgrounds adjacent to image edge
    """
    y_mask = np.full(
        (bin_image.shape[0], bin_image.shape[1]), fill_value=255, dtype=np.uint8
    )
    x_mask = np.full(
        (bin_image.shape[0], bin_image.shape[1]), fill_value=255, dtype=np.uint8
    )

    xs, ys = bin_image.shape[0], bin_image.shape[1]

    for x in range(0, xs):
        item_indexes = np.where(bin_image[x,:] != 0)[0]
        # min_start_edge = ys
        # max_final_edge = 0
        if len(item_indexes):
            start_edge, final_edge = item_indexes[0], item_indexes[-1]
            x_mask[x, start_edge:final_edge] = 0
            # if start_edge < min_start_edge:
            #     min_start_edge = start_edge
            # if final_edge > max_final_edge:
            #     max_final_edge = final_edge

    for y in range(0, ys):
        item_indexes = np.where(bin_image[:,y] != 0)[0]
        if len(item_indexes):
            start_edge, final_edge = item_indexes[0], item_indexes[-1]

            y_mask[start_edge:final_edge, y] = 0
            # mask[:start_edge, y] = 255
            # mask[final_edge:, y] = 255

    return np.logical_or(x_mask, y_mask)


def select_largest_obj(img_bin, lab_val=255, fill_mode=FILL['FLOOD'],
                       smooth_boundary=False, kernel_size=15):
    """
    Select the largest object from a binary image and optionally
    fill holes inside it and smooth its boundary.
    Args:
        img_bin (2D array): 2D numpy array of binary image.
        lab_val ([int]): integer value used for the label of the largest
                object. Default is 255.
        fill_mode (string {no,flood,threshold,morph}): hole filling techniques which are
            - no: no filling of holes
            - flood: floodfilling technique without removing image edge sharing holes
            - threshold: removing holes based on minimum size of hole to be removed
            - morph: closing morphological operation with some kernel size to remove holes
        smooth_boundary ([boolean]): whether smooth the boundary of the
                largest object using morphological opening or not. Default
                is false.
        kernel_size ([int]): the size of the kernel used for morphological
                operation. Default is 15.
    Returns:
        a binary image as a mask for the largest object.
    """

    # set up components
    n_labels, img_labeled, lab_stats, _ = \
        cv2.connectedComponentsWithStats(img_bin, connectivity=8, ltype=cv2.CV_32S)

    # find largest component label(label number works with labeled image because of +1)
    largest_obj_lab = np.argmax(lab_stats[1:, 4]) + 1

    # create a mask that will only cover the largest component
    largest_mask = np.zeros(img_bin.shape, dtype=np.uint8)
    largest_mask[img_labeled == largest_obj_lab] = lab_val

    if fill_mode == FILL['FLOOD']:
        # fill holes using opencv floodfill function

        # set up seedpoint(starting point) for floodfill
        bkg_locs = np.where(img_labeled == 0)
        bkg_seed = (bkg_locs[0][0], bkg_locs[1][0])

        # copied image to be floodfill
        img_floodfill = largest_mask.copy()

        # create a mask to ignore what shouldn't be filled(I think no effect)
        h_, w_ = largest_mask.shape
        mask_ = np.zeros((h_ + 2, w_ + 2), dtype=np.uint8)

        cv2.floodFill(img_floodfill, mask_, seedPoint=bkg_seed,
                    newVal=lab_val)
        holes_mask = cv2.bitwise_not(img_floodfill)  # mask of the holes.

        # get a mask to avoid filling non-holes that are adjacent to image edge
        non_holes_mask = generate_floodfill_mask(largest_mask)
        holes_mask = np.bitwise_and(holes_mask, np.bitwise_not(non_holes_mask))

        largest_mask = largest_mask + holes_mask
    elif fill_mode == FILL['MORPH']:
        # fill holes using closing morphology operation
        kernel_ = np.ones((50, 50), dtype=np.uint8)
        largest_mask = cv2.morphologyEx(largest_mask, cv2.MORPH_CLOSE,
                                        kernel_)
    elif fill_mode == FILL['THRESHOLD']:
        # fill background-holes based on hole size threshold
        # default hole size threshold is some percentage
        #   of size of the largest component(i.e leaf component)

        # invert to setup holes of background, sorry for the incovenience
        inv_img_bin = np.bitwise_not(largest_mask)

        # set up components
        inv_n_labels, inv_img_labeled, inv_lab_stats, _ = \
            cv2.connectedComponentsWithStats(inv_img_bin, connectivity=8, ltype=cv2.CV_32S)

        # find largest component label(label number works with labeled image because of +1)
        inv_largest_obj_lab = np.argmax(inv_lab_stats[1:, 4]) + 1

        # setup sizes and number of components
        inv_sizes = inv_lab_stats[1:, -1]
        sizes = lab_stats[1:, -1]
        inv_nb_components = inv_n_labels - 1

        # find the greater side of the image
        inv_max_side = np.amax(inv_img_labeled.shape)

        # set the minimum size of hole that is allowed to stay
        inv_min_size = int(0.3 * sizes[largest_obj_lab - 1]) # todo: specify good min size

        # generate the mask that allows holes greater than minimum size(weird)
        inv_mask = np.zeros((inv_img_labeled.shape), dtype=np.uint8)
        for inv_i in range(0, inv_nb_components):
            if inv_sizes[inv_i] >= inv_min_size:
                inv_mask[inv_img_labeled == inv_i + 1] = 255

        largest_mask = largest_mask + np.bitwise_not(inv_mask)

    if smooth_boundary:
        # smooth edge boundary
        kernel_ = np.ones((kernel_size, kernel_size), dtype=np.uint8)
        largest_mask = cv2.morphologyEx(largest_mask, cv2.MORPH_OPEN,
                                        kernel_)

    return largest_mask


def simple_test():
    # image = read_image(files['jpg1'])
    # g_img = excess_green(image)
    # r_img = excess_red(image)
    # debug(image[0], 'image')
    # debug(g_img[0], 'excess_green')
    # debug(r_img[0], 'excess_red')
    # debug(g_img[0]-r_img[0], 'diff')

    original_image = read_image(files['jpg1'], cv2.IMREAD_GRAYSCALE)
    marker = np.full((original_image.shape[0], original_image.shape[1]), True)
    texture_filter(original_image, marker)


def test():

    image = read_image(files['jpg1'])

    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    plt.imshow(rgb_image)
    plt.show()

    # plt.imshow(cv2.cvtColor(excess_green(image), cv2.COLOR_BGR2RGB))
    # plt.show()
    #
    # plt.imshow(cv2.cvtColor(excess_red(image), cv2.COLOR_BGR2RGB))
    # plt.show()


if __name__ == '__main__':
    simple_test()
60/438:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final, img1,mask]
    titles = ['Original ', 'final','1','2']
    for i in range(4):
        plt.subplot(2,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/one.jpg")
60/439:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/one.jpg")
60/440:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/leaf.jpg")
62/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
63/1:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
63/2: import cv2
64/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
65/1:
import cv2
import numpy as np
import time
from matplotlib import pyplot as plt

from utils import *
from review import files


# constants for filling holes mode
FILL = {
    'NO': 0,
    'FLOOD': 1,
    'THRESHOLD': 2,
    'MORPH': 3,
}


def remove_whites(image, marker):
    """
    Remove pixels resembling white from marker as background
    Args:
        image:
        marker: to be overloaded with white pixels to be removed
    Returns:
        nothing
    """
    # setup the white remover to process logical_and in place
    white_remover = np.full((image.shape[0], image.shape[1]), True)

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 0] > 200)
    white_remover[image[:, :, 0] <= 200] = False # blue channel

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 1] > 220)
    white_remover[image[:, :, 1] <= 220] = False  # green channel

    # below line same as: white_remover = np.logical_and(white_remover,  image[:, :, 2] > 200)
    white_remover[image[:, :, 2] <= 200] = False  # red channel

    # remove whites from marker
    marker[white_remover] = False


def remove_blacks(image, marker):
    """
    Remove pixels resembling black from marker as background
    Args:
        image:
        marker: to be overloaded with black pixels to be removed
    Returns:
        nothing
    """
    # setup the black remover to process logical_and in place
    black_remover = np.full((image.shape[0], image.shape[1]), True)

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 0] < 30)
    black_remover[image[:, :, 0] >= 30] = False  # blue channel

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 1] < 30)
    black_remover[image[:, :, 1] >= 30] = False  # green channel

    # below line same as: black_remover = np.logical_and(black_remover,  image[:, :, 2] < 30)
    black_remover[image[:, :, 2] >= 30] = False  # red channel

    # remove blacks from marker
    marker[black_remover] = False


def remove_blues(image, marker):
    """
    Remove pixels resembling blues better than green from marker as background
    Args:
        image:
        marker: to be overloaded with blue pixels to be removed
    Returns:
        nothing
    """
    # choose pixels that have higher blue than green
    blue_remover = image[:, :, 0] > image[:, :, 1]

    # remove blues from marker
    marker[blue_remover] = False


def color_index_marker(color_index_diff, marker):
    """
    Differentiate marker based on the difference of the color indexes
    Threshold below some number(found empirically based on testing on 5 photos,bad)
    If threshold number is getting less, more non-green image
     will be included and vice versa
    Args:
        color_index_diff: color index difference based on green index minus red index
        marker: marker to be updated
    Returns:
        nothing
    """
    marker[color_index_diff <= -0.05] = False


def texture_filter(image, marker, threshold=220, window=3):
    """
    Update marker based on texture of an image
    Args:
        image (ndarray of grayscale image):
        marker (ndarray size of image): marker to be updated
        threshold (number): minimum size of texture measurement(entropy) allowed
        window (int): window size of a square the texture computed from
    Returns: nothing
    """

    window = window - window//2 - 1
    for x in range(0, image.shape[0]):
        for y in range(0, image.shape[1]):
            # print('x y', x, y)
            # print('window', image[x:x + window, y:y + window])
            x_start = x - window if x < window else x
            y_start = y - window if y < window else y
            x_stop = x + window if x < image.shape[0] - window else image.shape[0]
            y_stop = y + window if y < image.shape[1] - window else image.shape[1]

            local_entropy = np.sum(image[x_start:x_stop, y_start:y_stop]
                                   * np.log(image[x_start:x_stop, y_start:y_stop] + 1e-07))
            # print('entropy', local_entropy)
            if local_entropy > threshold:
                marker[x, y] = False


def otsu_color_index(excess_green, excess_red):
    return cv2.threshold(excess_green - excess_red, 0, 255,cv2.THRESH_BINARY + cv2.THRESH_OTSU)


def generate_floodfill_mask(bin_image):
    """
    Generate a mask to remove backgrounds adjacent to image edge
    Args:
        bin_image (ndarray of grayscale image): image to remove backgrounds from
    Returns:
        a mask to backgrounds adjacent to image edge
    """
    y_mask = np.full(
        (bin_image.shape[0], bin_image.shape[1]), fill_value=255, dtype=np.uint8
    )
    x_mask = np.full(
        (bin_image.shape[0], bin_image.shape[1]), fill_value=255, dtype=np.uint8
    )

    xs, ys = bin_image.shape[0], bin_image.shape[1]

    for x in range(0, xs):
        item_indexes = np.where(bin_image[x,:] != 0)[0]
        # min_start_edge = ys
        # max_final_edge = 0
        if len(item_indexes):
            start_edge, final_edge = item_indexes[0], item_indexes[-1]
            x_mask[x, start_edge:final_edge] = 0
            # if start_edge < min_start_edge:
            #     min_start_edge = start_edge
            # if final_edge > max_final_edge:
            #     max_final_edge = final_edge

    for y in range(0, ys):
        item_indexes = np.where(bin_image[:,y] != 0)[0]
        if len(item_indexes):
            start_edge, final_edge = item_indexes[0], item_indexes[-1]

            y_mask[start_edge:final_edge, y] = 0
            # mask[:start_edge, y] = 255
            # mask[final_edge:, y] = 255

    return np.logical_or(x_mask, y_mask)


def select_largest_obj(img_bin, lab_val=255, fill_mode=FILL['FLOOD'],
                       smooth_boundary=False, kernel_size=15):
    """
    Select the largest object from a binary image and optionally
    fill holes inside it and smooth its boundary.
    Args:
        img_bin (2D array): 2D numpy array of binary image.
        lab_val ([int]): integer value used for the label of the largest
                object. Default is 255.
        fill_mode (string {no,flood,threshold,morph}): hole filling techniques which are
            - no: no filling of holes
            - flood: floodfilling technique without removing image edge sharing holes
            - threshold: removing holes based on minimum size of hole to be removed
            - morph: closing morphological operation with some kernel size to remove holes
        smooth_boundary ([boolean]): whether smooth the boundary of the
                largest object using morphological opening or not. Default
                is false.
        kernel_size ([int]): the size of the kernel used for morphological
                operation. Default is 15.
    Returns:
        a binary image as a mask for the largest object.
    """

    # set up components
    n_labels, img_labeled, lab_stats, _ = \
        cv2.connectedComponentsWithStats(img_bin, connectivity=8, ltype=cv2.CV_32S)

    # find largest component label(label number works with labeled image because of +1)
    largest_obj_lab = np.argmax(lab_stats[1:, 4]) + 1

    # create a mask that will only cover the largest component
    largest_mask = np.zeros(img_bin.shape, dtype=np.uint8)
    largest_mask[img_labeled == largest_obj_lab] = lab_val

    if fill_mode == FILL['FLOOD']:
        # fill holes using opencv floodfill function

        # set up seedpoint(starting point) for floodfill
        bkg_locs = np.where(img_labeled == 0)
        bkg_seed = (bkg_locs[0][0], bkg_locs[1][0])

        # copied image to be floodfill
        img_floodfill = largest_mask.copy()

        # create a mask to ignore what shouldn't be filled(I think no effect)
        h_, w_ = largest_mask.shape
        mask_ = np.zeros((h_ + 2, w_ + 2), dtype=np.uint8)

        cv2.floodFill(img_floodfill, mask_, seedPoint=bkg_seed,
                    newVal=lab_val)
        holes_mask = cv2.bitwise_not(img_floodfill)  # mask of the holes.

        # get a mask to avoid filling non-holes that are adjacent to image edge
        non_holes_mask = generate_floodfill_mask(largest_mask)
        holes_mask = np.bitwise_and(holes_mask, np.bitwise_not(non_holes_mask))

        largest_mask = largest_mask + holes_mask
    elif fill_mode == FILL['MORPH']:
        # fill holes using closing morphology operation
        kernel_ = np.ones((50, 50), dtype=np.uint8)
        largest_mask = cv2.morphologyEx(largest_mask, cv2.MORPH_CLOSE,
                                        kernel_)
    elif fill_mode == FILL['THRESHOLD']:
        # fill background-holes based on hole size threshold
        # default hole size threshold is some percentage
        #   of size of the largest component(i.e leaf component)

        # invert to setup holes of background, sorry for the incovenience
        inv_img_bin = np.bitwise_not(largest_mask)

        # set up components
        inv_n_labels, inv_img_labeled, inv_lab_stats, _ = \
            cv2.connectedComponentsWithStats(inv_img_bin, connectivity=8, ltype=cv2.CV_32S)

        # find largest component label(label number works with labeled image because of +1)
        inv_largest_obj_lab = np.argmax(inv_lab_stats[1:, 4]) + 1

        # setup sizes and number of components
        inv_sizes = inv_lab_stats[1:, -1]
        sizes = lab_stats[1:, -1]
        inv_nb_components = inv_n_labels - 1

        # find the greater side of the image
        inv_max_side = np.amax(inv_img_labeled.shape)

        # set the minimum size of hole that is allowed to stay
        inv_min_size = int(0.3 * sizes[largest_obj_lab - 1]) # todo: specify good min size

        # generate the mask that allows holes greater than minimum size(weird)
        inv_mask = np.zeros((inv_img_labeled.shape), dtype=np.uint8)
        for inv_i in range(0, inv_nb_components):
            if inv_sizes[inv_i] >= inv_min_size:
                inv_mask[inv_img_labeled == inv_i + 1] = 255

        largest_mask = largest_mask + np.bitwise_not(inv_mask)

    if smooth_boundary:
        # smooth edge boundary
        kernel_ = np.ones((kernel_size, kernel_size), dtype=np.uint8)
        largest_mask = cv2.morphologyEx(largest_mask, cv2.MORPH_OPEN,
                                        kernel_)

    return largest_mask


def simple_test():
    # image = read_image(files['jpg1'])
    # g_img = excess_green(image)
    # r_img = excess_red(image)
    # debug(image[0], 'image')
    # debug(g_img[0], 'excess_green')
    # debug(r_img[0], 'excess_red')
    # debug(g_img[0]-r_img[0], 'diff')

    original_image = read_image(files['jpg1'], cv2.IMREAD_GRAYSCALE)
    marker = np.full((original_image.shape[0], original_image.shape[1]), True)
    texture_filter(original_image, marker)


def test():

    image = read_image(files['jpg1'])

    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    plt.imshow(rgb_image)
    plt.show()

    # plt.imshow(cv2.cvtColor(excess_green(image), cv2.COLOR_BGR2RGB))
    # plt.show()
    #
    # plt.imshow(cv2.cvtColor(excess_red(image), cv2.COLOR_BGR2RGB))
    # plt.show()


if __name__ == '__main__':
    simple_test()
64/2:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
66/1:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
66/2: import cv2
67/1: import cv2
68/1:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
68/2: import cv2
68/3:
import matplotlib.pyplot as plt
import numpy as np
import scipy
import skimage
import os

%matplotlib inline



import matplotlib.pyplot as plt

from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,
                                 denoise_wavelet, estimate_sigma)
from skimage import data, img_as_float
from skimage.util import random_noise
from skimage import io
from skimage.util import img_as_float
70/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
70/2:
#im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
70/3:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    
mask_leaf("/Users/Hyunjee/Desktop/leaf.jpg")
71/1:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg"
71/2:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg")
71/3:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
71/4:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
73/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
74/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
75/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
76/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
76/2:
#im = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg") 
im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
76/3:
#def mask_leaf(im_name, external_mask=None):

    #im = cv2.imread(im_name)
    im = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg") 
    im = cv2.blur(im, (5,5))

    height, width = im.shape[:2]

    mask = np.ones(im.shape[:2], dtype=np.uint8) * 2 #start all possible background
    '''
    #from docs:
    0 GC_BGD defines an obvious background pixels.
    1 GC_FGD defines an obvious foreground (object) pixel.
    2 GC_PR_BGD defines a possible background pixel.
    3 GC_PR_FGD defines a possible foreground pixel.
    '''

    #2 circles are "drawn" on mask. a smaller centered one I assume all pixels are definite foreground. 
    #a bigger circle, probably foreground.
    r = 100
    cv2.circle(mask, (int(width/2.), int(height/2.)), 2*r, 3, -3) #possible fg
    #next 2 are greens...dark and bright to increase the number of fg pixels.
    mask[(im[:,:,0] < 45) & (im[:,:,1] > 55) & (im[:,:,2] < 55)] = 1  #dark green
    mask[(im[:,:,0] < 190) & (im[:,:,1] > 190) & (im[:,:,2] < 200)] = 1  #bright green
    mask[(im[:,:,0] > 200) & (im[:,:,1] > 200) & (im[:,:,2] > 200) & (mask != 1)] = 0 #pretty white

    cv2.circle(mask, (int(width/2.), int(height/2.)), r, 1, -3) #fg

    #if you pass in an external mask derived from some other operation it is factored in here.
   # if external_mask is not None:
    #    mask[external_mask == 1] = 1

    bgdmodel = np.zeros((1,65), np.float64)
    fgdmodel = np.zeros((1,65), np.float64)
    cv2.grabCut(im, mask, None, bgdmodel, fgdmodel, 1, cv2.GC_INIT_WITH_MASK)

    #show mask
    plt.figure(figsize=(10,10))
    plt.imshow(mask)
    plt.show()

    #mask image
    mask2 = np.where((mask==1) + (mask==3), 255, 0).astype('uint8')
    output = cv2.bitwise_and(im, im, mask=mask2)
    plt.figure(figsize=(10,10))
    plt.imshow(output)
    plt.show()

#mask_leaf('leaf1.jpg', external_mask=None)
#mask_leaf('leaf2.jpg', external_mask=None)
76/4: im.shape
76/5:
image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

#cv2.drawContours(image, contours, -1, (255,0,0), thickness = 2)
76/6: plt.imshow(contours)
76/7:
    BLUR = 21
    MASK_DILATE_ITER = 10
    MASK_ERODE_ITER = 10
    MASK_COLOR = (0.0,0.0,0.0) # In BGR format

        
    image = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg",cv2.IMREAD_UNCHANGED)

    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hsv = cv2.split(hsv)
    gray = hsv[0]

    gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

    
    
    #v = np.median(gray)
    #sigma = 0.33
    #lower = int(max(0, (1.0 - sigma) * v))
    #upper = int(min(255, (1.0 + sigma) * v))

    #CANNY_THRESH_1 = lower
    #CANNY_THRESH_2 = upper
        
    #edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)
    #edges = cv2.dilate(edges, None)
    #edges = cv2.erode(edges, None)
    
    ret,edges = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)
    
    contour_info = []
    #contours, __ = cv2.findContours(edges, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE  )
    contours  = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

    for c in contours:
        
        contour_info.append((c, cv2.isContourConvex(c), cv2.contourArea(c),))
     
        contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)
    
    max_contour = contour_info[0]
    
  
    mask = np.zeros(edges.shape)

    
    for c in contour_info:
        cv2.fillConvexPoly(mask, c[0], (255))
    
    
    

    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)

    mask_stack = np.dstack([mask]*3)
    
    mask_stack  = mask_stack.astype('float32') / 255.0
    print(mask_stack.shape)
    
    #-- Blend masked img into MASK_COLOR background --------------------------------------
    # For easy blending
    img = img.astype('float32') / 255.0
    # Blending
    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)
    # Convert back to unit8
    masked = (masked * 255).astype('uint8')
76/8:
image = cv2.imread('image.jpg',cv2.IMREAD_UNCHANGED)

hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
hsv = cv2.split(hsv)
gray = hsv[0]

gray = cv2.GaussianBlur(gray, (3,3), sigmaX=-1, sigmaY=-1)

ret,binary = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)

contours = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

cv2.drawContours(image, contours, -1, (255,0,0), thickness = 2)


im = cv2.imread("/Users/Hyunjee/Desktop/one.jpg") 
gray =  cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
ret, thresh1 = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
dist_transform = cv2.distanceTransform(thresh1,cv2.DIST_L2,5)
ret, stalk = cv2.threshold(dist_transform,0.095*dist_transform.max(),255,0)

stalk = np.uint8(stalk)
final = cv2.cvtColor(stalk, cv2.COLOR_GRAY2RGB)
plt.imshow(stalk)
77/1: import cv2
78/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
78/2:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg"
78/3:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
    imgo.shape
    mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg")
78/4:

    mask_leaf("/Users/Hyunjee/Desktop/leaf2.jpg")
79/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
79/2:

def mask_leaf(im_name):
    imgo = cv2.imread(im_name)
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
79/3: mask_leaf("/Users/Hyunjee/Desktop/worm.jpg")
80/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
80/2:
    imgo = cv2.imread("/Users/Hyunjee/Desktop/worm.jpg")
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
81/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
81/2:
    imgo = cv2.imread("/Users/Hyunjee/Desktop/worm.jpg")
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
83/1:
import pandas
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
83/2:
# Loading the CSV file
houseprice=pandas.read_csv('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/AML/Lab 1/lab1/datasets/regression_data.csv')
houseprice=houseprice[['Price (Older)', 'Price (New)']] # Choose 2 columns
83/3:
# Split the data
X=houseprice[['Price (Older)']]
Y=houseprice[['Price (New)']]

# Split the data into training and testing(75% training and 25% testing data)
xTrain,xTest,yTrain,yTest=train_test_split(X,Y)
83/4:
# sklearn functions implementation
def linearRegrPredict(xTrain, yTrain,xTest ):
    # Create linear regression object
    regr=LinearRegression()
    # Train the model using the training sets
    regr.fit(xTrain,yTrain)
    # Make predictions using the testing set
    y_pred = regr.predict(xTest)
    #print("Accuracy Score:",regr.score(xTest,yTest))
    return y_pred


y_pred = linearRegrPredict(xTrain, yTrain, xTest)
#print (y_pred)

# Plot testing set predictions
plt.scatter(xTest, yTest)
plt.plot(xTest, y_pred, 'r-')
plt.show()
83/5:
# Transform dataframes to numpy arrays
xTrain1=np.array(xTrain.values).flatten()
xTest1=np.array(xTest.values).flatten()
yTrain1=np.array(yTrain.values).flatten()
yTest1=np.array(yTest.values).flatten()
83/6:
def paramEstimates(xTrain, yTrain):
    beta = np.sum(np.multiply(xTrain,(np.add(yTrain, -np.mean(yTrain))))) / np.sum(np.multiply(xTrain, (np.add(xTrain, - np.mean(xTrain)))))
    alpha = np.mean(yTrain) - beta*np.mean(xTrain)
    return alpha, beta


def linearRegrNEWPredict(xTrain, yTrain,xTest):
    alpha, beta = paramEstimates(xTrain, yTrain)
    print (alpha)
    print(beta)
    #y_pred1 = ...
    return y_pred1


 
y_pred1=linearRegrNEWPredict(xTrain1, yTrain1,xTest1)
#print (y_pred1)


#Plot testing set predictions
plt.scatter(xTest, yTest)
plt.plot(xTest1, y_pred1, 'r-')
plt.show()
83/7:
def paramEstimates(xTrain, yTrain):
    beta = np.sum(np.multiply(xTrain,(np.add(yTrain, -np.mean(yTrain))))) / np.sum(np.multiply(xTrain, (np.add(xTrain, - np.mean(xTrain)))))
    alpha = np.mean(yTrain) - beta*np.mean(xTrain)
    return alpha, beta


def linearRegrNEWPredict(xTrain, yTrain,xTest):
    alpha, beta = paramEstimates(xTrain, yTrain)
    print (alpha)
    print(beta)
    y_pred1 = alpha + beta*xTest1
    return y_pred1


 
y_pred1=linearRegrNEWPredict(xTrain1, yTrain1,xTest1)
print (y_pred1)


#Plot testing set predictions
plt.scatter(xTest, yTest)
plt.plot(xTest1, y_pred1, 'r-')
plt.show()
83/8:
def SSR(yTest, y_pred):
    # Complete the equation
    ssr = np.sum(yTest-y_pred)**2
    return ssr

y_pred_SSR = SSR(yTest,y_pred)
y_pred1_SSR = SSR(yTest1,y_pred1)

print("Scikit-learn linear regression SSR: %.4f" % y_pred_SSR)
print("Our implementation of linear regression SSR: %.4f" % y_pred1_SSR)
85/1:
# import libraries
import pandas
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
85/2:
# Loading the CSV file
dataset=pandas.read_csv('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/AML/Lab 1/lab1/datasets/multi_regr_data.csv')
print(dataset.shape) #(data_number,feature_number)
85/3:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
#print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
#print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
#print(xtrain.shape)
#print(xtest.shape)
85/4:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
85/5:
# sklearn functions implementation
def multilinearRegrPredict(xtrain, ytrain,xtest ):
    # Create linear regression object
    reg=LinearRegression()
    # Train the model using the training sets
    reg.fit(xtrain,ytrain)
    # Make predictions using the testing set
    y_pred = reg.predict(xtest)
    print(list(reg.predict(xtrain))[:5]) #.predict(x axis of test data) returns a list of the predicted value of every item in the xTest .
    print("Accuracy Score from library implementation:", reg.score(xtest, ytest)) #.score(Predicted value, Y axis of Test data) methods returns the Accuracy Score or how much percentage the predicted value and the actual value matches

    return y_pred

y_pred = multilinearRegrPredict(xtrain, ytrain, xtest )
#print (y_pred)
85/6:
# sklearn functions implementation
def multilinearRegrPredict(xtrain, ytrain,xtest ):
    # Create linear regression object
    reg=LinearRegression()
    # Train the model using the training sets
    reg.fit(xtrain,ytrain)
    # Make predictions using the testing set
    y_pred = reg.predict(xtest)
    print(list(reg.predict(xtrain))[:5]) #.predict(x axis of test data) returns a list of the predicted value of every item in the xTest .
    print("Accuracy Score from library implementation:", reg.score(xtest, ytest)) #.score(Predicted value, Y axis of Test data) methods returns the Accuracy Score or how much percentage the predicted value and the actual value matches

    return y_pred

y_pred = multilinearRegrPredict(xtrain, ytrain, xtest )
print (y_pred)
85/7:
# sklearn functions implementation
def multilinearRegrPredict(xtrain, ytrain,xtest ):
    # Create linear regression object
    reg=LinearRegression()
    # Train the model using the training sets
    reg.fit(xtrain,ytrain)
    # Make predictions using the testing set
    y_pred = reg.predict(xtest)
    print(list(reg.predict(xtrain))[:5]) #.predict(x axis of test data) returns a list of the predicted value of every item in the xTest .
    print("Accuracy Score from library implementation:", reg.score(xtest, ytest)) #.score(Predicted value, Y axis of Test data) methods returns the Accuracy Score or how much percentage the predicted value and the actual value matches

    return y_pred

y_pred = multilinearRegrPredict(xtrain, ytrain, xtest )
#print (y_pred)
85/8:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
print (X)
85/9:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
print (Y)
85/10:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[0]]
print(a)
85/11:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[1]]
print(a)
85/12:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[3]]
print(a)
85/13:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[-1]]
print(a)
85/14:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[0:1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[-1]]
print(X)
85/15:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[0:2]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
a=dataset[list(dataset.columns)[-1]]
print(X)
85/16:
# Split the data but use all the columns of the csv file
X = dataset[list(dataset.columns)[:-1]]
print(X.shape)
Y = dataset[list(dataset.columns)[-1]] 
print(Y.shape)
# Split the data into training and testing(75% training and 25% testing data)
xtrain,xtest,ytrain,ytest=train_test_split(X, Y, random_state=0)
print(xtrain.shape)
print(xtest.shape)
85/17:

def multiLinparamEstimates(xtrain, ytrain):
    intercept = np.ones((xtrain.shape[0], 1))
    print(xtrain.shape)
    xtrain = np.concatenate((intercept, xtrain), axis=1)
    print(xtrain.shape)
    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(xtrain), xtrain)), 
                               np.transpose(xtrain)), ytrain)
    return beta

def multilinearNEWRegrPredict(xtrain, ytrain,xtest):
    beta = multiLinparamEstimates(xtrain, ytrain)
    intercept = np.ones((xtest.shape[0], 1))
    xtest = np.concatenate((intercept, xtest), axis=1)
    y_pred = np.matmul(xtest,beta)
    return y_pred1


# Model Evaluation - R2 Score
def r2_score(Y, Y_pred):
    mean_y = np.mean(Y)
    ss_tot = sum((Y - mean_y) ** 2)
    ss_res = sum((Y - Y_pred) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    print("Accuracy Score from scratch implementation:", r2) 
    return r2
85/18:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
#print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/19:

def multiLinparamEstimates(xtrain, ytrain):
    intercept = np.ones((xtrain.shape[0], 1))
    print(xtrain.shape)
    xtrain = np.concatenate((intercept, xtrain), axis=1)
    print(xtrain.shape)
    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(xtrain), xtrain)), 
                               np.transpose(xtrain)), ytrain)
    return beta

def multilinearNEWRegrPredict(xtrain, ytrain,xtest):
    beta = multiLinparamEstimates(xtrain, ytrain)
    intercept = np.ones((xtest.shape[0], 1))
    xtest = np.concatenate((intercept, xtest), axis=1)
    y_pred = np.matmul(xtest,beta)
    return y_pred1


# Model Evaluation - R2 Score
def r2_score(Y, Y_pred):
    mean_y = np.mean(Y)
    ss_tot = sum((Y - mean_y) ** 2)
    ss_res = sum((Y - Y_pred) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    print("Accuracy Score from scratch implementation:", r2) 
    return r2
85/20:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
#print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/21:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),np.array(xtest.values))
#print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/22:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/23:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/24:

def multiLinparamEstimates(xtrain, ytrain):
    intercept = np.ones((xtrain.shape[0], 1))
    print(xtrain.shape)
    xtrain = np.concatenate((intercept, xtrain), axis=1)
    print(xtrain.shape)
    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(xtrain), xtrain)), 
                               np.transpose(xtrain)), ytrain)
    return beta

def multilinearNEWRegrPredict(xtrain, ytrain,xtest):
    beta = multiLinparamEstimates(xtrain, ytrain)
    intercept = np.ones((xtest.shape[0], 1))
    xtest = np.concatenate((intercept, xtest), axis=1)
    y_pred = np.matmul(xtest,beta)
    return y_pred


# Model Evaluation - R2 Score
def r2_score(Y, Y_pred):
    mean_y = np.mean(Y)
    ss_tot = sum((Y - mean_y) ** 2)
    ss_res = sum((Y - Y_pred) ** 2)
    r2 = 1 - (ss_res / ss_tot)
    print("Accuracy Score from scratch implementation:", r2) 
    return r2
85/25:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/26:
y_pred1 = multilinearNEWRegrPredict(np.array(xtrain.values), np.array(ytrain.values).flatten(),
                             np.array(xtest.values))
#print (y_pred1)
r2=r2_score(ytest, y_pred1)
85/27:
def SSR( y_pred,yTest):
    ssr = np.sum(yTest - y_pred)**2
    return ssr

y_pred_SSR = SSR(y_pred, np.array(ytest.values).flatten())
#print(y_pred.shape)
#print(np.array(ytest.values).flatten().shape)
y_pred1_SSR = SSR(y_pred1, np.array(ytest.values).flatten())

print("Scikit-learn multivariate linear regression SSR: %.4f" % y_pred_SSR)
print("From scratch implementation of multivariate linear regression SSR: %.4f" % y_pred1_SSR)
86/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
86/2:
    imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
89/1:
    imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
89/2:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
89/3:
    imgo = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
    height, width = imgo.shape[:2]
    
    #Create a mask holder
    mask = np.zeros(imgo.shape[:2],np.uint8)

    #Grab Cut the object
    bgdModel = np.zeros((1,65),np.float64)
    fgdModel = np.zeros((1,65),np.float64)

    #Hard Coding the Rect The object must lie within this rect.
    #rect = (10,10,width-30,height-30)
    rect = (2,2, width-10,height-10)
    cv2.grabCut(imgo,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)
    mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')
    img1 = imgo*mask[:,:,np.newaxis]

    #Get the background
    background = imgo - img1

    #Change all pixels in the background that are not black to white
    background[np.where((background > [0,0,0]).all(axis = 2))] = [255,255,255]

    #Add the background and the image
    final = background + img1

    #To be done - Smoothening the edges


    output = [imgo, final]
    titles = ['Original ', 'final']
    for i in range(2):
        plt.subplot(1,2, i+1)
        plt.imshow(output[i], cmap = 'gray')
        plt.title(titles[i])
        plt.xticks([])
        plt.yticks([])
    
    plt.show()
91/1:
import cv2
import numpy as np
from matplotlib import pyplot as plt
import matplotlib
#%matplotlib inline #uncomment if in notebook
91/2: img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
91/3: plt.imshow(img)
91/4:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/5:
## Median filtering
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_fil = median_filter(gray, 5)
91/6:
## Median filtering
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img_fil = cv2.medianBlur(gray, 5)
91/7:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = image.shape[1]/float(image.shape[0])
print(aspect)
91/8:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = img_fill.shape[1]/float(img_fill.shape[0])
print(aspect)
91/9:
# image.shape[0] = width
# image.shape[1] =  height


# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)
91/10: img.fil.shape
91/11: img_fil.shape
91/12:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 255

# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(image, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(image, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(image, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/13:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 255

# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(img_fill, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(img_fill, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(img_fill, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/14:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 255

# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(img_fil, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(img_fil, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(img_fil, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/15:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 255
WIDTH = 255

# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(img_fil, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(img_fil, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(img_fil, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/16:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 200


# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(img_fil, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(img_fil, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(img_fil, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/17:
# image.shape[0] = width
# image.shape[1] =  height

HEIGHT = 180


# To check whether the image has landscape or portrait orientation
aspect = img_fil.shape[1]/float(img_fil.shape[0])
print(aspect)

if(aspect > 1):
    # Landscape orientation -  wide image
    resized = int(aspect * HEIGHT)
    print(resized)
    scaled = cv2.resize(img_fil, (resized, HEIGHT))

if(aspect < 1):
    # Portrait orientation -  tall image
    resized = int(WIDTH / aspect)
    print(resized)
    scaled = cv2.resize(img_fil, (WIDTH, resized))
if(aspect == 1):
    scaled = cv2.resize(img_fil, (WIDTH, HEIGHT))

# size of image
print(scaled.shape)

# show image
plt.imshow(scaled)
91/18:
hist,bins = np.histogram(img_fil.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(image.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
91/19:
hist,bins = np.histogram(img_fil.flatten(),256,[0,256])
cdf = hist.cumsum()
cdf_normalized = cdf * hist.max()/ cdf.max()

plt.plot(cdf_normalized, color = 'b')
plt.hist(img_fil.flatten(),256,[0,256], color = 'r')
plt.xlim([0,256])
plt.legend(('cdf','histogram'), loc = 'upper left')
plt.show()
91/20:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (144,238,144)
dg = (0, 100, 0)
91/21:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (144,238,144)
dg = (0, 100, 0)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
plt.imshow(result)
91/22:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (144,238,144)
dg = (0, 100, 0)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
plt.imshow(result)
91/23:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (144,238,144)
dg = (0, 100, 0)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
plt.imshow(result)
91/24:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (0, 187, 51)
dg = (0, 103, 51)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
plt.imshow(result)
91/25:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
plt.imshow(result)
91/26:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(result)
91/27:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
result = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(result)
91/28:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
plt.imshow(result)
91/29:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
plt.imshow(result)
91/30:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

lg = (1, 190, 200)
dg = (18, 255, 255)

mask = cv2.inRange(hsv_img, lg, dg)
result = cv2.bitwise_and(hsv_img, hsv_img, mask = mask)
#result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
plt.imshow(result)
91/31:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

plt.imshow(img)
91/32:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
hsv_img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)

plt.imshow(hsv_img)
91/33:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
91/34:
img = cv2.imread("/Users/Hyunjee/Desktop/nemo.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
plt.imshow(result)
91/35:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
plt.imshow(result)
91/36:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
plt.imshow(result)
91/37:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imsave("result.jpg",result)
91/38:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([65,60,60])
ug = np.array([80,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/39:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([36,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/40:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/41:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/42:
img = cv2.imread("/Users/Hyunjee/Desktop/sample.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/43:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf2.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/44:
img = cv2.imread("/Users/Hyunjee/Desktop/leaf.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/45:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/46:
img = cv2.imread("/Users/Hyunjee/Desktop/tomato.png")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/47:
img = cv2.imread("/Users/Hyunjee/Desktop/try1.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/48:
img = cv2.imread("/Users/Hyunjee/Desktop/try2.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/49:
img = cv2.imread("/Users/Hyunjee/Desktop/try3.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
91/50:
img = cv2.imread("/Users/Hyunjee/Desktop/try1.jpg")
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
lg = np.array([45,25,25])
ug = np.array([70,255,255])
mask = cv2.inRange(hsv, lg, ug)
result = cv2.bitwise_and(img,img, mask=mask)
cv2.imwrite("result.jpg",result)
98/1: import numpy as np  # Python library for scientific numerical computation
98/2:
v1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=float)
print(v1)
98/3:
v1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=float)
print(v1)
98/4:
# np.linespace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)
# start: start of the interval range
# stop: end of the interval range
# num: number of samples
# end point: if true, the stop is last sample
# retstep: return spacing
# dtype: type of output array

v2 = np.linspace(start=0, stop=1, num=11, dtype=float)
print(v2)
98/5:
# np.random.randn: Return samples from normal distribution

v3 = np.random.randn(11)
print(v3)
98/6:
# np.random.randint: Return random integers from low (inclusive) to high (exclusive).
# low: Lowest integer to be drawn from distribution
# high: Largest integer to be drawn from distribution
# size: Output shape

X = np.random.randint(low=0, high=5, size=(3, 3))
print(X)
98/7:
Y = np.random.randint(low=0, high=5, size=(3, 4))
print(Y)
98/8:
# np.dot: Dot product of two arrays 

Z = np.dot(X, Y)
print(Z)
98/9: Z.shape
98/10: import pandas as pd  # Python library built on top of NumPy for dataframe construction and more advanced data analysis
98/11: d = {'Vector_1': v1, 'Vector_2': v2, 'Vector_3': v3}
98/12:
df = pd.DataFrame(data=d) # create indexed DataFrame with labelled columns
df
98/13: df.head()
101/1: import numpy as np  # Python library for scientific numerical computation
101/2:
v1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=float)
print(v1)
101/3:
# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)[source]¶
# start: The starting value of the sequence
# stop: The end value of the sequence, unless endpoint is set to False
# num: Number of samples to generate
# endpoint: If True, stop is the last sample
# retstep: If True, return (samples, step), where step is the spacing between samples
# dtype: The type of the output array
# axis: The axis in the result to store the samples

v2 = np.linspace(start=0, stop=1, num=11, dtype=float)
print(v2)
101/4:
v3 = np.random.randn(11) # Random numbers are generated from normal distribution
print(v3)
101/5:
# np.random.randint(low, high=None, size=None, dtype='l')
# low: Lowest signed integer to be drawn from distribution
# high: Highest signed integer to be drawn from distribution
# size: Output shape

X = np.random.randint(low=0, high=5, size=(3, 3))
print(X)
101/6:
Y = np.random.randint(low=0, high=5, size=(3, 4))
print(Y)
101/7:
Z = np.dot(X, Y) # Dot product of two arrays
print(Z)
101/8: Z.shape
101/9: import pandas as pd  # Python library built on top of NumPy for dataframe construction and more advanced data analysis
101/10: d = {'Vector_1': v1, 'Vector_2': v2, 'Vector_3': v3}
101/11:
df = pd.DataFrame(data=d) # create indexed DataFrame with labelled columns
df
101/12: df.head()
101/13: df.tail(n=3)
101/14: df.index
101/15: df.index
101/16: df.columns
101/17: df.to_numpy()
101/18: df.describe()
101/19: df.sort_values(by='Vector_3', ascending=True)
101/20: df['Vector_2']
101/21:
import matplotlib.pyplot as plt
%matplotlib inline
101/22:
plt.scatter(df.index, df['Vector_3'].sort_values(ascending=True))
plt.xlabel('Index')
plt.ylabel('Vector_3')
101/23:
mu = 50 # mean of the distribution
sigma = 10 # standard deviation of the distribution 

np.random.seed(1) # fixing the random state for reproducibility (everyone gets the same result)

x = np.random.normal(mu, sigma, 500) # producing a 500 x 1 NumPy array of random samples
101/24:
num_bins = 50
n, bins, patches = plt.hist(x, bins=num_bins, density=True)

# Best fit of the Gaussian distribution
y = ((1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * (1 / sigma * (bins - mu)) ** 2))

# Plot the best fit as a red dashed line
plt.plot(bins, y, linestyle='--', linewidth=3, color='r')

plt.xlabel('x')
plt.ylabel('Probability density')
plt.title(r'Histogram of N: $\mu=50$, $\sigma=10$')

plt.xlim([np.round(np.min(x)), np.round(np.max(x))])
plt.ylim([0, np.round(np.max(n), decimals=2)])
102/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

### to be completed

Name = ['Beuys, Joseph', 'Constable, John', 'Daniell, William', 'Forbes, Elizabeth', 'Flaxman, John', 
        'Phillips, Thomas', 'Paolozzi, Sir Eduardo', 'Paolozzi, Sir Eduardo', 'Schendel, Mira', 'Turner, William',
        'Warhol, Andy']
102/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

### to be completed

Name = ['Beuys, Joseph', 'Constable, John', 'Daniell, William', 'Forbes, Elizabeth', 'Flaxman, John', 
        'Phillips, Thomas', 'Paolozzi, Sir Eduardo', 'Paolozzi, Sir Eduardo', 'Schendel, Mira', 'Turner, William',
        'Warhol, Andy']

Gender = ['Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male']

Year_Birth = np.array([1921, 1776, 1769, 1859, 1755, 1770, 1924, 1919, 1775, 1928], dtype=np.int64)

Year_Death = np.array([1986, 1837, 1837, 1912, 1826, 1845, 2005, 1988, 1851, 1987], dtype=np.int64)

N_works = np.array([588, 249, 612, 120, 287, 274, 385, 3, 1861, 272], dtype=np.int64)

columns = ['Name', 'Gender', 'Year_Birth', 'Year_Death', 'N_works']

d = {'Name': Name, 'Gender': Gender, 'Year_Birth': Year_Birth, 'Year_Death': Year_Death, 'N_works': N_works}

TateDataset = pd.DataFrame(data=d)
TateDataset
102/3:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

### to be completed

Name = ['Beuys, Joseph', 'Constable, John', 'Daniell, William', 'Forbes, Elizabeth', 'Flaxman, John', 'Phillips, Thomas',
        'Paolozzi, Sir Eduardo', 'Schendel, Mira', 'Turner, William', 'Warhol, Andy']

Gender = ['Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male']

Year_Birth = np.array([1921, 1776, 1769, 1859, 1755, 1770, 1924, 1919, 1775, 1928], dtype=np.int64)

Year_Death = np.array([1986, 1837, 1837, 1912, 1826, 1845, 2005, 1988, 1851, 1987], dtype=np.int64)

N_works = np.array([588, 249, 612, 120, 287, 274, 385, 3, 1861, 272], dtype=np.int64)

columns = ['Name', 'Gender', 'Year_Birth', 'Year_Death', 'N_works']

d = {'Name': Name, 'Gender': Gender, 'Year_Birth': Year_Birth, 'Year_Death': Year_Death, 'N_works': N_works}

TateDataset = pd.DataFrame(data=d)
TateDataset
102/4: TateDataset.to_csv('TateData.csv', index=False, header=True)
102/5:
Tate_1 = pd.read_csv('TateData.csv')
Tate_1
102/6:
### to be completed

TateDataset.to_csv('TateData.csv', index=True, header=False)
Tate_1 = pd.read_csv('TateData.csv')
Tate_1
102/7: TateDataset.to_excel('TateData.xlsx',index=False)
102/8:
Tate_2 = pd.read_excel('TateData.xlsx') 
Tate_2
102/9:
import pickle

TateDataset.to_pickle('TateData.pkl')
102/10:
Tate_4 = pd.read_pickle('TateData.pkl')
Tate_4
102/11:
# Check data type of the columns for Tate_1
Tate_1.dtypes
103/1:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted
103/2:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted.head(1)
103/3:
# Check data type of the columns for Tate_1
Tate_1.dtypes
103/4:
### to be completed
TateDataset.to_csv('TateData.csv', index=True, header=False)
Tate_1 = pd.read_csv('TateData.csv')
Tate_1
103/5:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

### to be completed

Name = ['Beuys, Joseph', 'Constable, John', 'Daniell, William', 'Forbes, Elizabeth', 'Flaxman, John', 'Phillips, Thomas',
        'Paolozzi, Sir Eduardo', 'Schendel, Mira', 'Turner, William', 'Warhol, Andy']

Gender = ['Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male']
Year_Birth = np.array([1921, 1776, 1769, 1859, 1755, 1770, 1924, 1919, 1775, 1928], dtype=np.int64)
Year_Death = np.array([1986, 1837, 1837, 1912, 1826, 1845, 2005, 1988, 1851, 1987], dtype=np.int64)
N_works = np.array([588, 249, 612, 120, 287, 274, 385, 3, 1861, 272], dtype=np.int64)

columns = ['Name', 'Gender', 'Year_Birth', 'Year_Death', 'N_works']

d = {'Name': Name, 'Gender': Gender, 'Year_Birth': Year_Birth, 'Year_Death': Year_Death, 'N_works': N_works}

TateDataset = pd.DataFrame(data=d)
TateDataset
103/6: TateDataset.to_csv('TateData.csv', index=False, header=True)
103/7:
Tate_1 = pd.read_csv('TateData.csv')
Tate_1
103/8:
### to be completed
TateDataset.to_csv('TateData.csv', index=True, header=False)
Tate_1 = pd.read_csv('TateData.csv')
Tate_1
103/9:
# Check data type of the columns for Tate_1
Tate_1.dtypes
103/10:
# Check data type of the columns for TateDataset
TateDataset.dtypes
### to be completed
103/11: TateDataset['Gender']
103/12:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted.head(1)
103/13:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted#.head(1)
103/14:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted.head(1)
103/15:
# Method 2:
TateDataset.loc[TateDataset['Year_Birth'].idxmax(), :]
### to be completed
102/12:
# Check data type of the columns for TateDataset
# TateDataset: Dataframe
TateDataset.dtypes
### to be completed
102/13: TateDataset['Gender']
102/14:
# Method 1:

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=False)
Sorted.head(1)
103/16:
# Method 2:

# dataframe.idxmax(): Returns index of first occurrence of maximum over requested axis

TateDataset.loc[TateDataset['Year_Birth'].idxmax(), :]
102/15:
# Method 2:

# dataframe.idxmax(): Returns index of first occurrence of maximum over requested axis

TateDataset.loc[TateDataset['Year_Birth'].idxmax(), :]
102/16:
## Look for the oldest artist using the two methods.

Sorted = TateDataset.sort_values(by='Year_Birth', ascending=True)
Sorted.head(1)
TateDataset.loc[TateDataset['Year_Birth'].idxmin(), :]
102/17:
# Create graph with labeled axis
TateDataset['N_works'].plot()
plt.xlabel('Artist Index')
plt.ylabel('# works')

# Maximum value in the data set
MaxValue = TateDataset['N_works'].max()

# Name associated with the maximum value of works
MaxName = TateDataset['Name'][TateDataset['N_works'] == TateDataset['N_works'].max()].values

print("The most popular artist is " + MaxName + " with " + str(MaxValue) + " works at the Tate.")
102/18:
plt.plot(TateDataset['Name'], TateDataset['N_works'])
plt.xticks(rotation=90)
plt.show()
102/19:
## Histogram of Artists' date of birth

### to be completed
102/20:
## Histogram of Artists' date of birth
plt.hist(TateDataset['Year_Birth'], range=(1750, 1950), bins=8, density=True)
plt.show()
102/21:
## Bar plot of Artists' gender

print(TateDataset.groupby(by='Gender').size())
genders = TateDataset.groupby(by='Gender').size().keys()
count = TateDataset.groupby(by='Gender').size().values
plt.bar(x=genders, height=count)
plt.show()
104/1:
import pandas as pd
import numpy as np

###########################
# Task: 
#   Try to import the dataset using Pandas and print the head.

# example of answer:
df = pd.read_csv('./laqndata_new.csv')
df.head()
###########################





### to be completed
104/2:
NO_Data = df[df['Species']=='NO']

NO_Data.describe()
104/3:
###########################
# Task: 
#   Extract other pollutant value saving in the similar variable as shown before.


### to be completed
NO2_Data = df[df['Species']=='NO2']
NO2_data.describe()


###########################
104/4:
import pandas as pd
import numpy as np

###########################
# Task: 
#   Try to import the dataset using Pandas and print the head.

# example of answer:
df = pd.read_csv('./laqndata_new.csv')
df
###########################





### to be completed
104/5:
import pandas as pd
import numpy as np

###########################
# Task: 
#   Try to import the dataset using Pandas and print the head.

# example of answer:
df = pd.read_csv('./laqndata_new.csv')
df.head()
###########################





### to be completed
104/6:
###########################
# Task: 
#   Extract other pollutant value saving in the similar variable as shown before.


### to be completed
NO2_Data = df[df['Species']=='NO2']
NO2_Data.describe()


###########################
104/7:
NOX_Data = df[df['Species']=='NOX']
NOX_Data.describe()
104/8:
PM10_Data = df[df['Species']=='PM10']
PM10_Data.describe()
104/9:
import matplotlib.pyplot as plt

###########################
# Task: 
#   Plot the value of NO concentration in January in the area of CD9.
###########################

# We select the data of Camden ('CD9')
CD_NO_Data = NO_Data[NO_Data['Site']=='CD9']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing

start_index = CD_NO_Data[CD_NO_Data['ReadingDateTime']=='01/01/2018 00:00'].index[0]
end_index = CD_NO_Data[CD_NO_Data['ReadingDateTime']=='31/01/2018 00:00'].index[0]


# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(CD_NO_Data.loc[start_index:end_index]['ReadingDateTime'],CD_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in CD area - Jan.')
plt.show()
#####################
104/10:
import matplotlib.pyplot as plt

###########################
# Task: 
#   Plot the value of NO concentration in January in the area of CD9.
###########################

# We select the data of Camden ('CD9')
CD_NO_Data = NO_Data[NO_Data['Site']=='CD9']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing

start_index = CD_NO_Data[CD_NO_Data['ReadingDateTime']=='01/01/2018 00:00'].index[0]
end_index = CD_NO_Data[CD_NO_Data['ReadingDateTime']=='31/01/2018 00:00'].index[0]


# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(CD_NO_Data.loc[start_index:end_index]['ReadingDateTime'],CD_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in CD area - Jan.')
plt.show()
#####################
104/11:
###########################
# Task: 
#   Plot the value of NO concentration in January in the area of GN3.
###########################

GN_NO_Data = NO_Data[NO_Data['Site']=='GN3']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = GN_NO_Data[GN_NO_Data['ReadingDateTime']=='01/01/2018 00:00'].index[0]
end_index = GN_NO_Data[GN_NO_Data['ReadingDateTime']=='31/01/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(GN_NO_Data.loc[start_index:end_index]['ReadingDateTime'],GN_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in GN area - Jan.')
plt.show()

###########################
104/12:
###########################
# Task: 
#   Plot the value of NO concentration in April in the area of KT6.
###########################


KT_NO_Data = NO_Data[NO_Data['Site']=='KT6']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = KT_NO_Data[GN_NO_Data['ReadingDateTime']=='01/04/2018 00:00'].index[0]
end_index = KT_NO_Data[GN_NO_Data['ReadingDateTime']=='30/04/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(KT_NO_Data.loc[start_index:end_index]['ReadingDateTime'],KT_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in KT area - Apr.')
plt.show()


###########################
104/13:
###########################
# Task: 
#   Plot the value of NO concentration in April in the area of KT6.
###########################


KT_NO_Data = NO_Data[NO_Data['Site']=='KT6']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = KT_NO_Data[KT_NO_Data['ReadingDateTime']=='01/04/2018 00:00'].index[0]
end_index = KT_NO_Data[KT_NO_Data['ReadingDateTime']=='30/04/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(KT_NO_Data.loc[start_index:end_index]['ReadingDateTime'],KT_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in KT area - Apr.')
plt.show()


###########################
104/14:
###########################
# Task: 
#   Plot the value of NO concentration in April in the area of BT4.
###########################


BT_NO_Data = NO_Data[NO_Data['Site']=='BT6']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='01/04/2018 00:00'].index[0]
end_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='30/04/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(BT_NO_Data.loc[start_index:end_index]['ReadingDateTime'],BT_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in BT area - Apr.')
plt.show()


###########################
104/15:
###########################
# Task: 
#   Plot the value of NO concentration in April in the area of BT4.
###########################


BT_NO_Data = NO_Data[NO_Data['Site']=='BT6']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='01/04/2018 00:00'].index[0]
end_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='30/04/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(BT_NO_Data.loc[start_index:end_index]['ReadingDateTime'],BT_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in BT area - Apr.')
plt.show()


###########################
104/16:
###########################
# Task: 
#   Plot the value of NO concentration in April in the area of BT4.
###########################


BT_NO_Data = NO_Data[NO_Data['Site']=='BT4']

# Now to select only the desidereted values, we need to find the start and end indices of the month. 
# As before, we can use Boolean Indexing
start_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='01/04/2018 00:00'].index[0]
end_index = BT_NO_Data[BT_NO_Data['ReadingDateTime']=='30/04/2018 00:00'].index[0]

# We are now ready to plot the data
plt.subplot()
# pandas.dataframa.loc: Access a group of rows and columns by label(s) or a boolean array.
plt.plot(BT_NO_Data.loc[start_index:end_index]['ReadingDateTime'],BT_NO_Data.loc[start_index:end_index]['Value'])
plt.xlabel('Date')
plt.ylabel(r'NO:Units ($\mu g / m^3$)')
plt.xticks(rotation=90)
plt.title('NO concentration in BT area - Apr.')
plt.show()


###########################
104/17:
GN_NO_data=NO_Data[NO_Data['Site']=='GN3']
GN_NO_data.loc[GN_NO_data['Value'].idxmax()]
104/18: GN_NO_data.loc[GN_NO_data['Value'].idxmin()]
104/19:
BT_NO_data.loc[GN_NO_data['Value'].idxmax()]
BT_NO_data.loc[GN_NO_data['Value'].idxmin()]
104/20:
BT_NO_Data.loc[GN_NO_data['Value'].idxmax()]
BT_NO_Data.loc[GN_NO_data['Value'].idxmin()]
104/21: BT_NO_Data.loc[GN_NO_data['Value'].idxmax()]
104/22:
BT_NO_data=NO_Data[NO_Data['Site']=='BT4']
BT_NO_data.loc[GN_NO_data['Value'].idxmax()]
104/23: BT_NO_Data.loc[GN_NO_data['Value'].idxmax()]
104/24:
CD_NO_Data = NO_Data[NO_Data['Site']=='CD9']
CD_NO_data.loc[GN_NO_data['Value'].idxmax()]
104/25:
CD_NO_data = NO_Data[NO_Data['Site']=='CD9']
CD_NO_data.loc[GN_NO_data['Value'].idxmax()]
104/26:
CD_NO_data = NO_Data[NO_Data['Site']=='CD9']
CD_NO_data.loc[CD_NO_data['Value'].idxmax()]
104/27: BT_NO_data.loc[BT_NO_data['Value'].idxmax()]
104/28: BT_NO_Data.loc[BT_NO_data['Value'].idxmax()]
104/29: KT_NO_data.loc[KT_NO_data['Value'].idxmax()]
104/30: KT_NO_Data.loc[KT_NO_Data['Value'].idxmax()]
104/31: CD_NO_data.loc[CD_NO_data['Value'].idxmin()]
104/32: KT_NO_Data.loc[KT_NO_Data['Value'].idxmin()]
104/33: BT_NO_data.loc[BT_NO_data['Value'].idxmin()]
104/34:
months=['01','02','03','04','05','06','07','08','09','10','11','12']
GN_NO_data_MaxValues=[]
for idx,month in enumerate(months):
    month_start_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx]+'/2018 00:00'].index[0]
    if idx==11:
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='30/12/2018 00:00'].index[0]
    else:
            
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx+1]+'/2018 00:00'].index[0]-1

    maxloc=GN_NO_data.loc[month_start_index:month_end_index]['Value'].idxmax()
    GN_NO_data_MaxValues.append(GN_NO_data.loc[maxloc]['Value'])
plt.subplot()
plt.plot(GN_NO_data_MaxValues)
plt.title('Monthly Max NO concentration in 2018 in area of GN')
plt.xticks(range(12),months)
plt.xlabel('month')
plt.ylabel(r'$\mu$ $g/m^{-3}$')
plt.show()
104/35:
###########################
# Task: 
#   Plot the monthly min value of NO concentration in 2018 in area of GN3.

###########################


months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
GN_NO_data_MaxValues=[]

# enumerate: Allows to loop over something and have an automatic counter

for idx in enumerate(months):
    month_start_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx]+'/2018 00:00'].index[0]
    if idx==11:
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='30/12/2018 00:00'].index[0]
    else:
        # -1: To get the last day of the month, if not it will give the first day of the next month   
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx+1]+'/2018 00:00'].index[0]-1

    maxloc=GN_NO_data.loc[month_start_index:month_end_index]['Value'].idxmax()
    GN_NO_data_MaxValues.append(GN_NO_data.loc[maxloc]['Value'])
plt.subplot()
plt.plot(GN_NO_data_MaxValues)
plt.title('Monthly Max NO concentration in 2018 in area of GN')
plt.xticks(range(12),months)
plt.xlabel('month')
plt.ylabel(r'$\mu$ $g/m^{-3}$')
plt.show()


###########################
104/36:
###########################
# Task: 
#   Plot the monthly min value of NO concentration in 2018 in area of GN3.

###########################


months=['01','02','03','04','05','06','07','08','09','10','11','12']
GN_NO_data_MinValues=[]

# enumerate: Allows to loop over something and have an automatic counter

for idx,month in enumerate(months):
    month_start_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx]+'/2018 00:00'].index[0]
    if idx==11:
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='30/12/2018 00:00'].index[0]
    else:
        # -1: To get the last day of the month, if not it will give the first day of the next month   
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx+1]+'/2018 00:00'].index[0]-1

    minloc=GN_NO_data.loc[month_start_index:month_end_index]['Value'].idxmin()
    GN_NO_data_MinValues.append(GN_NO_data.loc[minloc]['Value'])
plt.subplot()
plt.plot(GN_NO_data_MinValues)
plt.title('Monthly Min NO concentration in 2018 in area of GN')
plt.xticks(range(12),months)
plt.xlabel('month')
plt.ylabel(r'$\mu$ $g/m^{-3}$')
plt.show()


###########################
104/37: NO_Data['Value'].mean()
104/38:
###########################
# Task: 
#   Plot the monthly mean value of NO concentration in 2018 in area of GN3.

###########################

months=['01','02','03','04','05','06','07','08','09','10','11','12']
GN_NO_data_Mean=[]

# enumerate: Allows to loop over something and have an automatic counter

for idx,month in enumerate(months):
    month_start_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx]+'/2018 00:00'].index[0]
    if idx==11:
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='30/12/2018 00:00'].index[0]
    else:
        # -1: To get the last day of the month, if not it will give the first day of the next month   
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx+1]+'/2018 00:00'].index[0]-1

    meanloc=GN_NO_data.loc[month_start_index:month_end_index]['Value'].mean()
    GN_NO_data_Mean.append(meanloc)
plt.subplot()
plt.plot(GN_NO_data_Mean)
plt.title('Monthly Mean NO concentration in 2018 in area of GN')
plt.xticks(range(12),months)
plt.xlabel('month')
plt.ylabel(r'$\mu$ $g/m^{-3}$')
plt.show()




###########################
104/39: NO_Data['Value'].var()
104/40:
###########################
# Task: 
#   Plot the monthly variance value of NO concentration in 2018 for the area of GN3.

###########################


months=['01','02','03','04','05','06','07','08','09','10','11','12']
GN_NO_data_Variance=[]

# enumerate: Allows to loop over something and have an automatic counter

for idx,month in enumerate(months):
    month_start_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx]+'/2018 00:00'].index[0]
    if idx==11:
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='30/12/2018 00:00'].index[0]
    else:
        # -1: To get the last day of the month, if not it will give the first day of the next month   
        month_end_index=GN_NO_data[GN_NO_data['ReadingDateTime']=='01/'+months[idx+1]+'/2018 00:00'].index[0]-1

    varloc=GN_NO_data.loc[month_start_index:month_end_index]['Value'].var()
    GN_NO_data_Variance.append(varloc)
plt.subplot()
plt.plot(GN_NO_data_Variance)
plt.title('Monthly Variance NO concentration in 2018 in area of GN')
plt.xticks(range(12),months)
plt.xlabel('month')
plt.ylabel(r'$\mu$ $g/m^{-3}$')
plt.show()




###########################
104/41:
df_concentration = pd.DataFrame(data=np.vstack((NO_Data['Value'][0:1000], NO2_Data['Value'][0:1000], NOX_Data['Value'][0:1000], PM10_Data['Value'][0:1000])).transpose(),
                                columns=['NO', 'NO2', 'NOX', 'PM10'])


df_concentration.corr()
104/42:
###########################
# Task: 
#   Produce two scatter plots, one for two strongly 
#   correlated pollutants and one for two un-correlated ones. Visualize the differences.

###########################


plt.subplot()
plt.scatter((NO_Data['Value'][0:1000]), (NOX_Data['Value'][0:1000]))
plt.title('Strongly Correlated')
plt.xlabel('NO')
plt.ylabel('NOX')
plt.show()


###########################
104/43:
###########################
# Task: 
#   (1)When are the highest and lowest values of Oxides of Nitrogen (NOX) concentration there during 2018 in CD9? 
#     How much are these values? In which areas do you have these values?
#
###########################

# 1.1 When are the highest and the lowest values of Oxides of Nitrogen (NOX) concentration there during 2018 in CD9? 
#     How much are these values?

CD_NOX_data = NOX_Data[NOX_Data['Site']=='CD9']

CD_NOX_max_time = CD_NOX_data['ReadingDateTime'][CD_NOX_data['Value'] == CD_NOX_data['Value'].max()].values
CD_NOX_min_time = CD_NOX_data['ReadingDateTime'][CD_NOX_data['Value'] == CD_NOX_data['Value'].min()].values

CD_NOX_max_value = CD_NOX_data['Value'].max()
CD_NOX_min_value = CD_NOX_data['Value'].min()

# CD_NOX_data['Value'].head()
print(CD_NOX_max_time, CD_NOX_max_value)
print(CD_NOX_min_time, CD_NOX_min_value)

# 1.2 In which areas do you have these values?
locate_area_max_value = NOX_Data['Site'][NOX_Data['Value'] == CD_NOX_max_value]
locate_area_min_value = NOX_Data['Site'][NOX_Data['Value'] == CD_NOX_min_value]
print('max area: ', locate_area_max_value)
print('min area: ', locate_area_min_value)

#########
104/44:
###########################
# Task: 
#   (2) Find in which days the value of PM10 concentration is below 16  𝑢𝑔/𝑚3. 
#       Plot PM10 concentration only for these days.
###########################

PM10_Data = df[df['Species'] == 'PM10']
# set_index: To set a List, Series or Data frame as index of a Data Frame
PM10_Data = PM10_Data.set_index('ReadingDateTime')
PM10_conc_below_16 = PM10_Data.loc[PM10_Data['Value'] < 16]
PM10_conc_below_16.head()

plt.subplot()
PM10_conc_below_16['Value'].plt(rot=45)
plt.title(r'PM10 concentration against days when PM10 concentration is below 16 $\mu$ $g/m^(-3)$')
plt.xlabel(r'Days when concentration is below 16 $\mu$ $g/m^(-3)$')
plt.ylabel('PM10 concentration' + '(' + r'$\mu$ $g/m^(-3)$' + ')')
plt.show()



#########
104/45:
###########################
# Task: 
#   (2) Find in which days the value of PM10 concentration is below 16  𝑢𝑔/𝑚3. 
#       Plot PM10 concentration only for these days.
###########################

PM10_Data = df[df['Species'] == 'PM10']
# set_index: To set a List, Series or Data frame as index of a Data Frame
PM10_Data = PM10_Data.set_index('ReadingDateTime')
PM10_conc_below_16 = PM10_Data.loc[PM10_Data['Value'] < 16]
PM10_conc_below_16.head()

plt.subplot()
PM10_conc_below_16['Value'].plot(rot=45)
plt.title(r'PM10 concentration against days when PM10 concentration is below 16 $\mu$ $g/m^(-3)$')
plt.xlabel(r'Days when concentration is below 16 $\mu$ $g/m^(-3)$')
plt.ylabel('PM10 concentration' + '(' + r'$\mu$ $g/m^(-3)$' + ')')
plt.show()



#########
104/46:
###########################
# Task: 
#   (2) Find in which days the value of PM10 concentration is below 16  𝑢𝑔/𝑚3. 
#       Plot PM10 concentration only for these days.
###########################

PM10_Data = df[df['Species'] == 'PM10']
# set_index: To set a List, Series or Data frame as index of a Data Frame
PM10_Data = PM10_Data.set_index('ReadingDateTime')
PM10_conc_below_16 = PM10_Data.loc[PM10_Data['Value'] < 16]
PM10_conc_below_16.head()

plt.subplot()
PM10_conc_below_16['Value'].plot(rot=45)
plt.title(r'PM10 concentration against days when PM10 concentration is below 16 $\mu$ $g/m^(-3)$')
plt.xlabel(r'Days when concentration is below 16 $\mu$ $g/m^{-3}$')
plt.ylabel('PM10 concentration' + '(' + r'$\mu$ $g/m^{-3}$' + ')')
plt.show()



#########
104/47:
###########################
# Task: 
#   (2) Find in which days the value of PM10 concentration is below 16  𝑢𝑔/𝑚3. 
#       Plot PM10 concentration only for these days.
###########################

PM10_Data = df[df['Species'] == 'PM10']
# set_index: To set a List, Series or Data frame as index of a Data Frame
PM10_Data = PM10_Data.set_index('ReadingDateTime')
PM10_conc_below_16 = PM10_Data.loc[PM10_Data['Value'] < 16]
PM10_conc_below_16.head()

plt.subplot()
PM10_conc_below_16['Value'].plot(rot=45)
plt.title(r'PM10 concentration against days when PM10 concentration is below 16 $\mu$ $g/m^{-3}$')
plt.xlabel(r'Days when concentration is below 16 $\mu$ $g/m^{-3}$')
plt.ylabel('PM10 concentration' + '(' + r'$\mu$ $g/m^{-3}$' + ')')
plt.show()



#########
104/48:
#  [TO DO (3)] - CD9

PM10_Data = df[df['Species'] == 'PM10']
CD_PM10_data = PM10_Data[PM10_Data['Site'] == 'CD9']
CD_PM10_datetime_best = CD_PM10_data['ReadingDateTime'][CD_PM10_data['Value'] == Cd_PM10_data['Value'].min()].values
104/49:
#  [TO DO (3)] - CD9

PM10_Data = df[df['Species'] == 'PM10']
CD_PM10_data = PM10_Data[PM10_Data['Site'] == 'CD9']
CD_PM10_datetime_best = CD_PM10_data['ReadingDateTime'][CD_PM10_data['Value'] == CD_PM10_data['Value'].min()].values
104/50:
#  [TO DO (3)] - GN3

GN_PM10_data = PM10_Data[PM10_Data['Site'] == 'GN3']
GN_PM10_datetime_best = GN_PM10_data['ReadingDateTime'][GN_PM10_data['Value'] == GN_PM10_data['Value'].min()].values
104/51:
#  [TO DO (3)] - BT4

BT_PM10_data = PM10_Data[PM10_Data['Site'] == 'BT4']
BT_PM10_datetime_best = BT_PM10_data['ReadingDateTime'][BT_PM10_data['Value'] == BT_PM10_data['Value'].min()].values
104/52:
print('CD9: ', CD_PM10_datetime_best )
print('GN3: ', GN_PM10_datetime_best )
print('BT4: ', BT_PM10_datetime_best )
108/1:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
108/2:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
109/1:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
110/1:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
111/1:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
111/2:
from _future_ import print_function

import tensorflow as tf
import numpy
import matplotlib.pyplot as plt
rng = numpy.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50

# Training Data
train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0]

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")

# Construct a linear model
pred = tf.add(tf.multiply(X, W), b)

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)
# Gradient descent
#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

    # Run the initializer
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if (epoch+1) % display_step == 0:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c), \
                "W=", sess.run(W), "b=", sess.run(b))

    print("Optimization Finished!")
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    print("Training cost=", training_cost, "W=", sess.run(W), "b=", sess.run(b), '\n')

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()

    # Testing example, as requested (Issue #2)
    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])

    print("Testing... (Mean square loss Comparison)")
    testing_cost = sess.run(
        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})  # same function as cost above
    print("Testing cost=", testing_cost)
    print("Absolute mean square loss difference:", abs(
        training_cost - testing_cost))

    plt.plot(test_X, test_Y, 'bo', label='Testing data')
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
    plt.legend()
    plt.show()
119/1: import tensorflow as tf
119/2: import tensorflow as tf
119/3: import tensorflow as tf
119/4: import tensorflow as tf
123/1: import tensorflow as tf
125/1: import tensorflow as tf
125/2: # 1. Labelling
128/1:
import numpy as np
import pandas as pd
np.random.seed(1)
128/2: full_labels = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Annotations/Disease.csv')
128/3: full_labels.head()
128/4: full_labels = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Annotations/Disease.csv')
128/5: full_labels = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Disease.csv')
128/6: full_labels.head()
128/7: grouped = full_labels.groupby('filename')
128/8: grouped.apply(lambda x: len(x)).value_counts()
128/9: gb = full_labels.groupby('filename')
128/10: grouped_list = [gb.get_group(x) for x in gb.groups]
128/11: len(grouped_list)
128/12:
train_index = np.random.choice(len(grouped_list), size=160, replace=False)
test_index = np.setdiff1d(list(range(200)), train_index)
129/1:
import numpy as np
import pandas as pd
np.random.seed(1)
129/2: full_labels = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Annotations/Disease.csv')
129/3: full_labels = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Disease.csv')
129/4: full_labels.head()
129/5: grouped = full_labels.groupby('filename')
129/6: grouped.apply(lambda x: len(x)).value_counts()
129/7: gb = full_labels.groupby('filename')
129/8: grouped_list = [gb.get_group(x) for x in gb.groups]
129/9: len(grouped_list)
129/10:
train_index = np.random.choice(len(grouped_list), size=4, replace=False)
test_index = np.setdiff1d(list(range(0)), train_index)
129/11: len(train_index), len(test_index)
129/12:
# take first 200 files
train = pd.concat([grouped_list[i] for i in train_index])
test = pd.concat([grouped_list[i] for i in test_index])
129/13:
# take first 200 files
train = pd.concat([grouped_list[i] for i in train_index])
#test = pd.concat([grouped_list[i] for i in test_index])
129/14: len(train), len(test)
129/15: len(train), #len(test)
129/16:
train.to_csv('train_labels.csv', index=None)
test.to_csv('test_labels.csv', index=None)
129/17:
train.to_csv('train_labels.csv', index=None)
#test.to_csv('test_labels.csv', index=None)
126/1:
import pandas as pd
import matplotlib.pyplot as pt
%matplotlib inline
from matplotlib import patches
126/2:
#read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Disease.csv')
train.head
126/3:
#read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Disease.csv')
train.head()
126/4:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/5:
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import patches
126/6:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/7:
# Number of unique training images
train['image_name'].nunique()
126/8:
# Number of unique training images
train['image_names'].nunique()
126/9:
# Number of unique training images
train['filename'].nunique()
126/10:
# Nnumber of classes
train['class'].value_counts()
126/11:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row.class == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/12:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
        # assign different color to different classes of objects
        if row.class == 'Disease':
            edgecolor = 'r'
            ax.annotate('Disease', xy=(xmax-40,ymin+20))
        #elif row.cell_type == 'WBC':
            #edgecolor = 'b'
            #ax.annotate('WBC', xy=(xmax-40,ymin+20))
       # elif row.cell_type == 'Platelets':
           # edgecolor = 'g'
            #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/13:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
        # assign different color to different classes of objects
    if row.class == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/14:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
        # assign different color to different classes of objects
    if row.class = 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/15:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[].class == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/16:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == 'Disease']:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/17:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.['filenames'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == 'Disease']:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/18:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filenames'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == 'Disease']:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/19:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == 'Disease']:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/20:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == "Disease"]:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/21:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == "Disease"].any():
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/22:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == "Disease"].empty:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/23:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == "Disease"]:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/24:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if train[train['class'] == train["Disease"]]:
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/25:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    print(row)
    
    # assign different color to different classes of objects
#     if train["class"].apply(lambda x: x):
#         edgecolor = 'r'
#         ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/26:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    print(row)
    
    # assign different color to different classes of objects
#     if train["class"].apply(lambda x: x):
#         edgecolor = 'r'
#         ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
#     rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
#     ax.add_patch(rect)s
126/27:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    print(row['class'])
    
    # assign different color to different classes of objects
#     if train["class"].apply(lambda x: x):
#         edgecolor = 'r'
#         ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
#     rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
#     ax.add_patch(rect)s
126/28:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row['class'] == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)s
126/29:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row['class'] == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/30:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/31:
cd keras-frcnn
python train_frcnn.py -o simple -p annotate.txt
126/32:
import os
os.getcwd()
126/33: os.chdir('/Users/Hyunjee/Desktop/keras-frcnn')
126/34: os.getcwd()
126/35: python train_frcnn.py -o simple -p annotate.txt
126/36:
cd keras-frcnn
python train_frcnn.py -o simple -p annotate.txt
126/37:

python train_frcnn.py -o simple -p annotate.txt
126/38:
# read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/Disease.csv')
train.head()
126/39:
# read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/keras-frcnn/train.csv')
train.head()
126/40:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/41:
# Number of unique training images
train['filename'].nunique()
126/42:
# Number of unique training images
train['image_names'].nunique()
126/43:
# Nnumber of classes
train['cell_type'].value_counts()
126/44:
# fig = plt.figure()

# #add axes to the image
# ax = fig.add_axes([0,0,1,1])

# # read and plot the image
# image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
# plt.imshow(image)

# # iterating over the image for different objects
# for _,row in train[train['filename'] == "s1.jpg"].iterrows():
#     xmin = row.xmin
#     xmax = row.xmax
#     ymin = row.ymin
#     ymax = row.ymax
    
#     width = xmax - xmin
#     height = ymax - ymin
    
#     # assign different color to different classes of objects
#     if row['class'] == 'Disease':
#         edgecolor = 'r'
#         ax.annotate('Disease', xy=(xmax-40,ymin+20))
#     #elif row.cell_type == 'WBC':
#         #edgecolor = 'b'
#         #ax.annotate('WBC', xy=(xmax-40,ymin+20))
#    # elif row.cell_type == 'Platelets':
#        # edgecolor = 'g'
#         #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
#     # add bounding boxes to the image
#     rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
#     ax.add_patch(rect)


fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row.cell_type == 'Disease':
        edgecolor = 'r'
        ax.annotate('RBC', xy=(xmax-40,ymin+20))
#     elif row.cell_type == 'WBC':
#         edgecolor = 'b'
#         ax.annotate('WBC', xy=(xmax-40,ymin+20))
#     elif row.cell_type == 'Platelets':
#         edgecolor = 'g'
#         ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/45:
# fig = plt.figure()

# #add axes to the image
# ax = fig.add_axes([0,0,1,1])

# # read and plot the image
# image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
# plt.imshow(image)

# # iterating over the image for different objects
# for _,row in train[train['filename'] == "s1.jpg"].iterrows():
#     xmin = row.xmin
#     xmax = row.xmax
#     ymin = row.ymin
#     ymax = row.ymax
    
#     width = xmax - xmin
#     height = ymax - ymin
    
#     # assign different color to different classes of objects
#     if row['class'] == 'Disease':
#         edgecolor = 'r'
#         ax.annotate('Disease', xy=(xmax-40,ymin+20))
#     #elif row.cell_type == 'WBC':
#         #edgecolor = 'b'
#         #ax.annotate('WBC', xy=(xmax-40,ymin+20))
#    # elif row.cell_type == 'Platelets':
#        # edgecolor = 'g'
#         #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
#     # add bounding boxes to the image
#     rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
#     ax.add_patch(rect)


fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train.image_names == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row.cell_type == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
#     elif row.cell_type == 'WBC':
#         edgecolor = 'b'
#         ax.annotate('WBC', xy=(xmax-40,ymin+20))
#     elif row.cell_type == 'Platelets':
#         edgecolor = 'g'
#         ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/46:
import os
os.getcwd()
126/47: os.chdir('/Users/Hyunjee/Desktop/keras-frcnn')
126/48: os.getcwd()
126/49:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/50:
data = pd.DataFrame()
data['format'] = train['image_names']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/51:
data = pd.DataFrame()
data['format'] = train['image_names']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['cell_type'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/52:
# read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/keras-frcnn/train.csv')
train.head()
126/53:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/54:
# Number of unique training images
train['filenames'].nunique()
126/55:
# Number of unique training images
train['filename'].nunique()
126/56:
# Nnumber of classes
train['class'].value_counts()
126/57:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row['class'] == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/58:
import os
os.getcwd()
126/59: os.chdir('/Users/Hyunjee/Desktop/keras-frcnn')
126/60: os.getcwd()
126/61:
data = pd.DataFrame()
data['format'] = train['image_names']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['cell_type'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/62:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['cell_type'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/63:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/64:
# read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/keras-frcnn/train.csv')
train.head()
126/65:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/66:
# Number of unique training images
train['filename'].nunique()
126/67:
# Nnumber of classes
train['class'].value_counts()
126/68:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row['class'] == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/69:
import os
os.getcwd()
126/70: os.chdir('/Users/Hyunjee/Desktop/keras-frcnn')
126/71: os.getcwd()
126/72:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/73:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/74:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
126/75:
# read the csv file using read_csv function of pandas
train = pd.read_csv('/Users/Hyunjee/Desktop/keras-frcnn/train.csv')
train.head()
126/76:
# reading single image using imread functionof matplotlib
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)
126/77:
# Number of unique training images
train['filename'].nunique()
126/78:
# Nnumber of classes
train['class'].value_counts()
126/79:
fig = plt.figure()

#add axes to the image
ax = fig.add_axes([0,0,1,1])

# read and plot the image
image = plt.imread('/Users/Hyunjee/Desktop/TensorFlow_API_Custom_Object_Detection/Dataset/JPEG_Images/s1.jpg')
plt.imshow(image)

# iterating over the image for different objects
for _,row in train[train['filename'] == "s1.jpg"].iterrows():
    xmin = row.xmin
    xmax = row.xmax
    ymin = row.ymin
    ymax = row.ymax
    
    width = xmax - xmin
    height = ymax - ymin
    
    # assign different color to different classes of objects
    if row['class'] == 'Disease':
        edgecolor = 'r'
        ax.annotate('Disease', xy=(xmax-40,ymin+20))
    #elif row.cell_type == 'WBC':
        #edgecolor = 'b'
        #ax.annotate('WBC', xy=(xmax-40,ymin+20))
   # elif row.cell_type == 'Platelets':
       # edgecolor = 'g'
        #ax.annotate('Platelets', xy=(xmax-40,ymin+20))
        
    # add bounding boxes to the image
    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor = edgecolor, facecolor = 'none')
    
    ax.add_patch(rect)
126/80:
import os
os.getcwd()
126/81: os.chdir('/Users/Hyunjee/Desktop/keras-frcnn')
126/82: os.getcwd()
126/83:
data = pd.DataFrame()
data['format'] = train['filename']

# as the images are in train_images folder, add train_images before the image name
for i in range(data.shape[0]):
    data['format'][i] = 'train_images/' + data['format'][i]

# add xmin, ymin, xmax, ymax and class as per the format required
for i in range(data.shape[0]):
    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['class'][i]

data.to_csv('annotate.txt', header=None, index=None, sep=' ')
133/1: Generate xml file of each jpg file image
133/2: ## Training
133/3: import opencv
132/1: import cv2
133/4: import cv2
133/5:
3.4

Run setup.py
133/6:
6.2

Change directory to 

/Users/Hyunjee/Desktop/tf1.12/model/research/object_detection
133/7:
## 6.3

To monitor training processes in a browser.

Change directory to 

/Users/Hyunjee/Desktop/tf1.12/model/research/object_detection
133/8:
## 6.4

Export inference graph

Change XXXX in a code to number in .ckpt file

/Users/Hyunjee/Desktop/tf1.12/model/research/object_detection/training/model.ckpt-XXXX
141/1:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris
141/2:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/3:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/4:
data = load_iris()
df=pd.DataFrame(data)
df
141/5:
data = load_iris()
df=pd.DataFrame(data.data)
df
141/6:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(y.head())
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/7:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(Y.head())
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/8:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/9:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(X)
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/10:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums
print(Y)
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/11:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/12:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import lab2_landmarks as l2
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/13:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203/lab2_landmarks.py')
import lab2_landmarks as l2

import lab2_landmarks as l2
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/14:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##

from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/15:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/16:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks# as l2
141/17:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks# as l2
141/18:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/19:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/20: gender = l2.extract_features_labels()
141/21:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/22:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/23:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/24:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/25:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##

from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/26:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/27:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/28:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/29:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##

from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/30:
import sys
sys.path.append('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/AMLS_assignment_kit/project_organization_example/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/31:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/32: gender = l2.extract_features_labels()
141/33:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/34:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/35:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/36:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/37: gender = l2.extract_features_labels()
141/38:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/39:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/40:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/41:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/42: gender = l2.extract_features_labels()
141/43:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/44:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/45:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/46:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/47: gender = l2.extract_features_labels()
141/48:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/49:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2] # First two Colums

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
141/50:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/51: gender = l2.extract_features_labels()
141/52:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/53:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##

from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/54:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/55:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/56: gender = l2.extract_features_labels()
141/57:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
print(images_dir)
141/58:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/59:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
print(images_dir)
141/60:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print( image_paths)
141/61:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
#image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
one = [os.path.join(images_dir, l)
print(one)
141/62:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
#image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
one = [os.path.join(images_dir, l)
one
141/63:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
#image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
one = os.path.join(images_dir, l)
print (one)
141/64:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/65:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/66:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/67:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/68: gender = l2.extract_features_labels()
141/69:
import sys
sys.path.append('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203')
import lab2_landmarks as l2
141/70:
def get_data():

    X, y = l2.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100]
    tr_Y = Y[:100]
    te_X = X[100:]
    te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y
141/71: gender = l2.extract_features_labels()
141/72:
# sklearn functions implementation

def img_SVM(training_images, training_labels, test_images, test_labels):
    classifier = svm.SVC(kernel='linear')

    classifier.fit(training_images, training_labels)

    pred = classifier.predict(test_images)

    print(pred)

    print("Accuracy:", accuracy_score(test_labels, pred))

tr_X, tr_Y, te_X, te_Y= get_data()
pred=img_SVM(tr_X.reshape((100, 68*2)), list(zip(*tr_Y))[0], te_X.reshape((35, 68*2)), list(zip(*te_Y))[0])
141/73:
basedir = ('/Users/Hyunjee/Desktop/OneDrive/UCL/YEAR 4/ELEC0134 Applied Machine Learning Systems/lab2/part2_1/dataset')
images_dir = os.path.join(basedir,'celeba')
#image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
one = os.path.join(images_dir, l)
print (one)
141/74:
import os
import itertools
141/75:
root_path = '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba'
    
folders = ['train','test','validation']
genders = ['male','female']
    
for folder,gender in itertools.product(folders, genders):
    os.makedirs(os.path.join(root_path,folder,gender))
141/76:
data = pd.read.csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head
141/77:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
141/78:
data = pd.read.csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head
141/79:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head
141/80:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head

new = data.str.split("\", n=1, expand=True)
141/81:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data.head

data.drop(data.index[1])
data.head
141/82:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head

data.drop(data.index[1])
data.head
141/83: data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
141/84:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head
141/85:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", index_col='No.', names=['No.','img_name','gender','smiling'])
data.head
141/86:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.head
141/87:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
new=data.str.split('\')
data.head
141/88:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
new=data.str.split('\')
data.head
pd.concat([df[[0]]], df[1].str.split('\', expand = True)], axis=1)
141/89:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
new=data.str.split('\')
data.head
pd.concat([df[[0]], df[1].str.split('\ ', expand = True)], axis=1)
141/90:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
141/91:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]=data.\timg_name\tgender\tsmiling.str.split("\", expand =True, )
141/92:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data.\timg_name\tgender\tsmiling.str.split("\", expand= True, )
141/93:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data[0].str.split()
141/94:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data['target'].str.split(pat='\'. expand = True)
141/95:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
#data[['img_name','gender','smiling']]= data['target'].str.split(pat='\'. expand = True)
141/96:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data['target'].str.split(pat=''. expand = True)
141/97:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data['target'].str.split(pat='/'. expand = True)
141/98:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data['target'].str.split(pat='/', expand = True)
141/99:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data[['img_name','gender','smiling']]= data[0].str.split(pat='/', expand = True)
141/100:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
#data[['img_name','gender','smiling']]= data[0].str.split(pat='/', expand = True)
new = data[['img_name','gender','smiling']].apply(lambda x: pd.Series(str(x).split('/')))
141/101:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
141/102:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data.drop(df.index[0])
141/103:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data
data.drop(df.index[0])
data
141/104:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
data.drop(df.index[0])
data
141/105:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
del data.index.name
data
141/106:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
data.index.name = None
data
141/107:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
print data.index.name
141/108:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
print (data.index.name)
141/109:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
print (data[0])
141/110:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data[0]
141/111:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data[0].head
141/112:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head
141/113:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = '/')
#data
#data.index.name = None
data.head
141/114:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(0)
141/115:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(1)
141/116: data[0] = data[0].str.split('/').str(0)
141/117:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(2)
141/118:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(3)
141/119:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(3)
data.coumns
141/120:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
#data
#data.index.name = None
data.head(3)
data.coulmns
141/121:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
data.columns = data.columns.str.strip()
#data
#data.index.name = None
data.head(3)
data.coulmns
141/122:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")

data.coulmns = ['a','b','c']
data.head
141/123:
data = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")

data.coulmns = ['a']
data.head
141/124:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(s).reset_index()
df
141/125:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df
141/126:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['a']
df
141/127:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['a','v']
df
141/128:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
df
141/129:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
141/130:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index', 'img_name', 'gender', 'smiling']] = df.Total.str.split("t", expand = True)
141/131:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index', 'img_name', 'gender', 'smiling']] = df.Total.str.split("/", expand = True)
141/132:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index', 'img_name']] = df.Total.str.split("/", expand = True)
141/133:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("/", expand = True)
141/134:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("/", expand = True)
df
141/135:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("t", expand = True)
df
141/136:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("t", expand = True)
df
141/137:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split(" ", expand = True)
df
141/138:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("\", expand = True)
df
141/139:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("/", expand = True)
df
141/140:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("t", expand = True)
df
141/141:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("0", expand = True)
df
141/142:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split("t", expand = True)
df
141/143:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df
df[['Index']] = df.Total.str.split(" ", expand = True)
df
141/144:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)
df
141/145:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/146:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/147:
data = np.loadtxt("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv, delimiter='\')
# Transpose data if needs be
data = np.transpose(data)
141/148:
data = np.loadtxt("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv, delimiter='/')
# Transpose data if needs be
data = np.transpose(data)
141/149:
data = np.loadtxt("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv, delimiter=',')
# Transpose data if needs be
data = np.transpose(data)
141/150:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "/")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/151:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t"")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/152:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/153:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df
141/154:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

#df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))
new=df.split('.')[1].split('/')[-1]

new
141/155:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split(" ").str[0]
df["country"] = df["Toal"].str.split(" ").str[-1]
df
141/156:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split(" ").str[0]
df["country"] = df["Total"].str.split(" ").str[-1]
df
141/157:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split("\").str[0]
df["country"] = df["Total"].str.split("\").str[-1]
df
141/158:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv", sep = "t")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split("/").str[0]
df["country"] = df["Total"].str.split("/").str[-1]
df
141/159:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split("/").str[0]
df["country"] = df["Total"].str.split("/").str[-1]
df
141/160:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("t")))


df["img_name"] = df["Total"].str.split("\\").str[0]
df["country"] = df["Total"].str.split("\\").str[-1]
df
141/161:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\\")))


df["img_name"] = df["Total"].str.split("\\").str[0]
df["country"] = df["Total"].str.split("\\").str[-1]
df
141/162:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split('\\')))


df["img_name"] = df["Total"].str.split("\\").str[0]
df["country"] = df["Total"].str.split("\\").str[-1]
df
141/163:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\\x")))


df["img_name"] = df["Total"].str.split("\\").str[0]
df["country"] = df["Total"].str.split("\\").str[-1]
df
141/164:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\\x")))


#df["img_name"] = df["Total"].str.split("\\").str[0]
#df["country"] = df["Total"].str.split("\\").str[-1]
df
141/165:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\\x")))


#df["img_name"] = df["Total"].str.split("\\").str[0]
#df["country"] = df["Total"].str.split("\\").str[-1]
df
141/166:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\t")))


#df["img_name"] = df["Total"].str.split("\\").str[0]
#df["country"] = df["Total"].str.split("\\").str[-1]
df
141/167:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']

#df[['Index']] = df.Total.str.split(" ", expand = True)

#df[['Index']] = df.Total.apply(lambda x: pd.Series(str(x).split("\t")))


df["img_name"] = df["Total"].str.split("\t").str[0]
#df["country"] = df["Total"].str.split("\\").str[-1]
df
141/168:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']


df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\\").str[-1]

df
141/169:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']


df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[-1]

df
141/170:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']


df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]

df
141/171:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']


df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
df
141/172:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
df
141/173:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total','Index']
df
141/174:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Total']
df
141/175:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
141/176: #Load csv file
141/177:
celeb_label = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
celeb_label = pd.DataFrame(df).reset_index()
celeb_label.columns = ['Index','Total']
del celeb_label['Index']
celeb_label["Index"] = celeb_label["Total"].str.split("\t").str[0]
celeb_label["img_name"] = celeb_label["Total"].str.split("\t").str[1]
celeb_label["gender"] = celeb_label["Total"].str.split("\t").str[2]
celeb_label["smiling"] = celeb_label["Total"].str.split("\t").str[3]
del celeb_label['Total']
del celeb_label['Index']
celeb_label

  #delim_whitespace=True
141/178:
celeb_label = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",delim_whitespace=True)
celeb_label = pd.DataFrame(df).reset_index()
celeb_label.columns = ['Index','Total']
del celeb_label['Index']
celeb_label["Index"] = celeb_label["Total"].str.split("\t").str[0]
celeb_label["img_name"] = celeb_label["Total"].str.split("\t").str[1]
celeb_label["gender"] = celeb_label["Total"].str.split("\t").str[2]
celeb_label["smiling"] = celeb_label["Total"].str.split("\t").str[3]
del celeb_label['Total']
del celeb_label['Index']
celeb_label

  #delim_whitespace=True
141/179:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv",  delim_whitespace=True)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
141/180:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
141/181:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df
141/182:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
141/183:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == -1]
celeb_female
141/184:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
celeb_female
141/185:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
celeb_female
celeb_male = df.loc[df['gender'] == '1']
celeb_male
141/186:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img
celeb_male = df.loc[df['gender'] == '1']
celeb_male
141/187:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img
#celeb_male = df.loc[df['gender'] == '1']
#celeb_male
141/188:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img

celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img
141/189:
for filename in os.listdir('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img'):
    for name in female_imgs:
        if name in filename:
            os.rename(os.path.join('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img', filename), '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train')
            break
141/190:
for filename in os.listdir('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img'):
    for name in female_img:
        if name in filename:
            os.rename(os.path.join('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img', filename), '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train')
            break
141/191:
import os
import glob
import shutil
141/192:

srcpath = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
srcfiles = os.listdir(srcpath)

print(srcfiles)
new_path = os.path.join("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
141/193:

srcpath = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
srcfiles = os.listdir(srcpath)

destpath = os.path.join("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

destfiles = list(female_img)
destfiles
141/194:
import os
import glob
import shutil
import arcpy
import os.path
141/195: f = female_img.img.name.tolist()
141/196: f = female_img.img_name.tolist()
141/197:
f = female_img.img_name.tolist()
f
141/198:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for f in unique_f:
    for filename in glob.glob('*.jpg' % f):
        shuntil.move(filename, dst)
141/199:
f = female_img.img_name.tolist()
f
141/200:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for f in unique_f:
    for filename in glob.glob('*.jpg'):
        shuntil.move(filename, dst)
141/201: for filename in glob.glob('*.jpg')
141/202:
for filename in glob.glob('*.jpg'):
    print filename
141/203:
for filename in glob.glob('*.jpg'):
    print (filename)
141/204:
for filename in glob.glob('*.jpg'):
    print (filename)
141/205:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for root, dirs, files in os.walk(src):
    for file in files:
        if file == f:
            shutill.move(os.path.join(root,file), dst)
141/206:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for root, dirs, files in os.walk(src):
    for file in files:
        if file == female_img:
            shutill.move(os.path.join(root,file), dst)
141/207:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for root, dirs, files in os.walk(src):
    for file in files:
        if file == female_img[0]:
            shutill.move(os.path.join(root,file), dst)
141/208:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for root, dirs, files in os.walk(src):
    for file in files:
        if file == female_img[,0]:
            shutill.move(os.path.join(root,file), dst)
141/209:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

f = female_img.img_name.tolist()
unique_f = set(f)

for root, dirs, files in os.walk(src):
    for file in files:
        if file == unique_f:
            shutill.move(os.path.join(root,file), dst)
141/210:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))
print jpg_files
141/211:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))
print (jpg_files)
141/212:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))
print (jpg_files)
141/213:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))
print (jpg_files)

for fname in jpg_files:
    if fname == female_img:
        shutil.move(fname, dst)
141/214:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))


for fname in jpg_files:
    if fname == female_img:
        shutil.move(fname, dst)
141/215:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))


for fname in jpg_files:
    if fname == female_img:
        shutil.move(fname, dst)
141/216:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))

f=female_img.tolist()

for fname in jpg_files:
    if fname == f:
        shutil.move(fname, dst)
141/217:
from glob import glob
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

jpg_files = glob(os.path.join(src,"*.jpg"))

f=female_img.img_name.tolist()

for fname in jpg_files:
    if fname == f:
        shutil.move(fname, dst)
141/218:
import os
import glob
import shutil

import os.path
import fnmatch
141/219:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

for root, dirs, files, in os.walk(source):
    for fname in fnmatch.filter(files, "*.jpg"):
        src_path = os.path.join(root, fname)
        des_path = os.path.join(destination, fname)
        if os.path.exists(des_path):
            print "there was a name collision!"
            # handle it
        print "moving '{f}' to '{d}'".format(
            f=src_path, d=destination)
        shutil.move(fname, destination)
141/220:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

for root, dirs, files, in os.walk(source):
    for fname in fnmatch.filter(files, "*.jpg"):
        src_path = os.path.join(root, fname)
        des_path = os.path.join(destination, fname)
        if os.path.exists(des_path):
            print ("there was a name collision!")
            # handle it
        print ("moving '{f}' to '{d}'").format(
            f=src_path, d=destination)
        shutil.move(fname, destination)
141/221:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img
female_img.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv')


celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img
141/222:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
             os.rename(os.path.join("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img", filename), "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
             break
141/223:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
             print(filename)
            break
141/225:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
141/226:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
    
names.head
141/227:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
    
names
141/228:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 
f = female_img.img_name.tolist()
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
    
f
141/229:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 
f = female_img.img_name.tolist()
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in f:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
    
filename
141/230:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 
f = female_img.img_name.tolist()
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in f:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
    
filename.shape
141/231:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 
f = female_img.img_name.tolist()
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in f:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
    
filename.shape
141/232:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
    names= [line.strip() for line in names_file] 
f = female_img.img_name.tolist()
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in f:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/233:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
             os.rename(os.path.join("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img", filename), "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
             break
141/234:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
             shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/1")
            break
141/236:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/1")
            break
141/237:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
            break
141/238:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/1")
            break
141/239:
f = female_img.img_name.tolist()
f
141/240:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/241:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img", "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/242:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            count(filename)
            break
141/243:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
141/244:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/245:

src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

#jpg_files = glob(os.path.join(src,"*.jpg"))

#f=female_img.img_name.tolist()



source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"



# open the file, make a list of all filenames, close the file
#with open('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/female_img.csv') as names_file:
    # use .strip() to remove trailing whitespace and line breaks
names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/246:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
#df["Index"] = df["Total"].str.split("\t").str[0]
#df["img_name"] = df["Total"].str.split("\t").str[1]
#df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
#del df['Total']
#del df['Index']
df
141/247:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
#del df['Total']
#del df['Index']
df
141/248:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
141/249:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"


source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(os.path.abspath(filename), "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/250:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"


source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/251:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"


source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            filename.count()
            break
141/252:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"


source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/253:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/254:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            #print(filename)
            break
141/255:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img.shape


celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img.shape
141/256:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img.shape
141/257:
celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img.shape
141/258:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/259:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            list.count(filename)
            break
141/260:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(list.count(filename))
            break
141/261:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            a=list.count(filename)
            print(a)
            break
141/262:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            a=list.count(filename)
            print(a)
            break
141/263:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            a=len(filename)
            print(a)
            break
141/264:
f = female_img.img_name.tolist()
f
141/265:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            a=len(filename)
           
            break
print(a)
141/266:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            len(filename)
           
            break
141/267:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
           
            break
141/268:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/269:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
        print(filename)
            break
141/270:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/271:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            print(filename)
            break
141/272:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img.shape
141/273:
celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img.shape
141/274:
import os
import glob
import shutil

import os.path
import fnmatch
141/275:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
destination = ("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            break
141/276:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/277:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/278:
f = female_img.img_name.tolist()
f
141/279:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()
print(names)
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/280:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(filename)
            break
141/281:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img")
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(filename)
            break
141/282:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"


source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
            print(filename)
            break
141/283:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, destination)
            print(filename)
            break
141/284:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(filename)
            break
141/285:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()
print(names)
for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(filename)
            break
141/286:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()

for filename in os.listdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(name)
            break
141/287:

#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()
141/288:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(names)
            break
141/289:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            #shutil.move(filename, destination)
            print(filename)
            break
141/290:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, destination)
            break
141/291:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
             # move the file
            shutil.move(filename, destination)
            break
141/292:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()

names
141/293:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
source = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
destination = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names= female_img.img_name.tolist()
141/294:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/295:
for filename in os.listdir(source):
    print(filename)
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/296:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        print(name in filename)
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/297:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/298:
fna = []
nna = []

for filename in os.listdir(source):
    fna.append(filename)
    for name in names:
        nna.append(name)
        # no need for re.search, just use the "in" operator
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/299: fna
141/300: nna
141/301: os.listdir(source)
141/302: names
141/303: '22i2.jpg' in names
141/304: '22i2.jpg' in names
141/305: '2212.jpg' in names
141/306: names
141/307: '0.jpg' in names
141/308: '67.jpg' in names
141/309: names
141/310:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/311:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
#             print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/312: # os.listdir(source)
141/313: len(names)
141/314:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            counter += 1
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/315:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name in filename:
            print(filename, name)
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/316:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name == filename:
            print(filename, name)
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/317:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name == filename:
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/318:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name == filename:
            print(filename, name)
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/319:
counter = 0

for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name == filename:
            counter += 1
#             print("Hi why am I doing this on a Macbook")
             # move the file
#             shutil.move(filename, destination)
            break
    
counter
141/320:
for filename in os.listdir(source):
    for name in names:
        # no need for re.search, just use the "in" operator
        if name == filename:
#             print("Hi why am I doing this on a Macbook")
             # move the file
            shutil.move(filename, destination)
            break
141/321:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names_female= female_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_female:
        if name == filename:
             # move the file
            shutil.move(filename, dst_female)
            break
141/322:


dst_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

names_male= male_img.img_name.tolist()

for filename in os.listdir(source):
    for name in names_male:
        if name == filename:
             # move the file
            shutil.move(filename, dst_male)
            break
141/323:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
dst_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names_female= female_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_female:
        if name == filename:
             # move the file
            shutil.move(filename, dst_female)
            break
141/324:
dst_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

names_male= male_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_male:
        if name == filename:
             # move the file
            shutil.move(filename, dst_male)
            break
141/325:
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
141/326:
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)
print X_train.shape, y_train.shape
print X_test.shape, y_test.shape
141/327:
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)
print (X_train.shape, y_train.shape)
print (X_test.shape, y_test.shape)
143/1:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
143/2:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris
143/3:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())

# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(Y)
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
143/4:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())
print(df)
# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(Y)
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
143/5:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())
print(df)
# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]
print(X)
# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
143/6:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())
print(df)
# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
print(x_train)
143/7:
# Loading the data file
data = load_iris()
df=pd.DataFrame(data.data,columns=data.feature_names)
#print(df.head())
print(df)
# Drop rest of the features and extract the target values
Y = data.target
X = data.data[:, :2]

# Shuffle and split the data into training and test set
X, Y = shuffle(X,Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)
print(y_train)
144/1:
def get_data():
    X, y = import_data.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100] ; tr_Y = Y[:100]
    te_X = X[100:] ; te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y

print(tr_X)
144/2:
import tensorflow as tf
import lab2_data as import_data
import numpy as np
144/3:
def get_data():
    X, y = import_data.extract_features_labels()
    Y = np.array([y, -(y - 1)]).T
    tr_X = X[:100] ; tr_Y = Y[:100]
    te_X = X[100:] ; te_Y = Y[100:]

    return tr_X, tr_Y, te_X, te_Y

print(tr_X)
141/328: os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
141/329:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation")
141/330:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation")
141/331:
src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train_FileNames_female))
print('Validation: ', len(val_FileNames_female))
print('Testing: ', len(test_FileNames_female))

# Copy-pasting images
for name in train:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in valid:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/332:
src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in valid:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/333:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in valid:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test:
    shutil.copy(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/334:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/male")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/male")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/male")
141/335:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in valid:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/336:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in valid_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/337:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in val_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/valid/female")

for name in test_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/338:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in val_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/female")

for name in test_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/339:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

src_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"
src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_female = os.listdir(src_female)
np.random.shuffle(allFileNames_female)
train, valid, test = np.split(np.array(allFileNames_female),[int(len(allFileNames_female)*0.7), int(len(allFileNames_female)*0.85)])



train_female = [src_female+'/'+ name for name in train.tolist()]
val_female = [src_female+'/' + name for name in valid.tolist()]
test_female = [src_female+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_female))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")

for name in val_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/female")

for name in test_female:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
141/340:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_male = os.listdir(src_male)
np.random.shuffle(allFileNames_male)
train, valid, test = np.split(np.array(allFileNames_male),[int(len(allFileNames_male)*0.7), int(len(allFileNames_male)*0.85)])

train_male = [src_male+'/'+ name for name in train.tolist()]
val_male = [src_male+'/' + name for name in valid.tolist()]
test_male = [src_male+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_male))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/male")

for name in val_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/male")

for name in test_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/male")
141/341:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

src_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

allFileNames_male = os.listdir(src_male)
np.random.shuffle(allFileNames_male)
train, valid, test = np.split(np.array(allFileNames_male),[int(len(allFileNames_male)*0.7), int(len(allFileNames_male)*0.85)])

train_male = [src_male+'/'+ name for name in train.tolist()]
val_male = [src_male+'/' + name for name in valid.tolist()]
test_male = [src_male+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_male))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/male")

for name in val_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/male")

for name in test_male:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/male")
141/342:
from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D

from keras import models
141/343:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(178,218,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
141/344:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', 
                    mode='min', verbose=1, save_best_only=True)
cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', 
                 metrics=['accuracy'])
141/345:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
141/346:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
141/347:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


history = my_model.fit_generator(
        train_generator,
        epochs=30,
        steps_per_epoch=2667,
        validation_data=validation_generator,
        validation_steps=667, callbacks=cb_list)
141/348:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


history = model_gender.fit_generator(
        train_generator,
        epochs=30,
        steps_per_epoch=2667,
        validation_data=validation_generator,
        validation_steps=667, callbacks=cb_list)
141/349:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


history = model_gender.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // batch_size,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // batch_size,
        callbacks=cb_list)
141/350:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


history = model_gender.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/351:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


# history = model_gender.fit_generator(
#         train_generator,
#         epochs=25,
#         steps_per_epoch=train_generator.samples // 12,
#         validation_data=validation_generator,
#         validation_steps=validation_generator.samples // 12,
#         callbacks=cb_list)
141/352:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


# history = model_gender.fit_generator(
#         train_generator,
#         epochs=25,
#         steps_per_epoch=train_generator.samples // 12,
#         validation_data=validation_generator,
#         validation_steps=validation_generator.samples // 12,
#         callbacks=cb_list)

history = model_gender.fit_generator(
        train_generator,
        epochs=1,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/353:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')


# history = model_gender.fit_generator(
#         train_generator,
#         epochs=25,
#         steps_per_epoch=train_generator.samples // 12,
#         validation_data=validation_generator,
#         validation_steps=validation_generator.samples // 12,
#         callbacks=cb_list)
141/354:
bs = 1

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=1,
        steps_per_epoch=train_generator.samples // bs,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // bs,
        callbacks=cb_list)
141/355:
bs = 10

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=1,
        steps_per_epoch=train_generator.samples // bs,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // bs,
        callbacks=cb_list)
141/356:
bs = 300

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=bs,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=1,
        steps_per_epoch=train_generator.samples // bs,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // bs,
        callbacks=cb_list)
141/357:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/358:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/359:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/360:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=11,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
146/1:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/2:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
146/3:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/4:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/5:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
#df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/6:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
#df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/7:
# Smiling (1), Not Smiling (0)

celeba_smiling = df.loc[df['smiling'] == '1']
smiling_img = celeb_smiling[['img_name']]
smiling_img.shape
146/8:
# Smiling (1), Not Smiling (0)

celeb_smiling = df.loc[df['smiling'] == '1']
smiling_img = celeb_smiling[['img_name']]
smiling_img.shape
146/9:
celeb_not_smiling = df.loc[df['smiling'] == '-1']
not_smiling_img = not_celeb_smiling[['img_name']]
not_smiling_img.shape
146/10:
celeb_not_smiling = df.loc[df['smiling'] == '-1']
not_smiling_img = celeb_not_smiling[['img_name']]
not_smiling_img.shape
146/11:
celeb_not_smiling = df.loc[df['smiling'] == '-1']
not_smiling_img = celeb_not_smiling[['img_name']]
not_smiling_img.head
146/12:
# Smiling (1), Not Smiling (-1)

celeb_smiling = df.loc[df['smiling'] == '1']
smiling_img = celeb_smiling[['img_name']]
smiling_img.head
146/13:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "//Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling"

names_smiling= smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/14:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "//Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling"

names_smiling= smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/15:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "//Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling"

names_smiling= smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/16:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
146/17:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "//Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling"

names_smiling= smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
141/361:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/18:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smiling= smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
141/362:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/363:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178, 218),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 218),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=11,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/364:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
141/365:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
141/366:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
141/367:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=11,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/368:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=11,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
146/19:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B2/celeba/img/smiling")
dst_smiling = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smiling = smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/20:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
#df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
146/21:
# Smiling (1), Not Smiling (-1)

celeb_smiling = df.loc[df['smiling'] == '1']
smiling_img = celeb_smiling[['img_name']]
smiling_img.head
146/22:
celeb_not_smiling = df.loc[df['smiling'] == '-1']
not_smiling_img = celeb_not_smiling[['img_name']]
not_smiling_img.head
146/23:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smiling = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smiling = smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/24:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smiling = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smiling = smiling_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smiling:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smiling)
            break
146/25:
# Smiling (1), Not Smiling (-1)

celeb_smile = df.loc[df['smiling'] == '1']
smile_img = celeb_smiling[['img_name']]
smile_img.head
146/26:
celeb_no_smile= df.loc[df['smiling'] == '-1']
no_smile_img = celeb_not_smiling[['img_name']]
no_smile_img.head
146/27:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/28:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()
print(names_smile)
for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/29:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/30:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    print(filename)
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/31:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            print(filename)
            #shutil.move(filename, dst_smile)
            break
146/32:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(name, dst_smile)
            break
146/33:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/34:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
141/369:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female")
dst_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names_female= female_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_female:
        if name == filename:
             # move the file
            shutil.move(filename, dst_female)
            break
141/370:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/371:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=11,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
146/35:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fmatch
141/372:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/373:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=9,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
146/36:
celeb_no_smile= df.loc[df['smiling'] == '-1']
no_smile_img = celeb_not_smiling[['img_name']]
no_smile_img.shape
146/37:
# Smiling (1), Not Smiling (-1)

celeb_smile = df.loc[df['smiling'] == '1']
smile_img = celeb_smiling[['img_name']]
smile_img.shape
146/38:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        print(name)
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/39:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        name.shape
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/40:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        len(name)
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/41:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        print len(name)
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/42:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
141/374:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/375:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
141/376:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female")
dst_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names_female = female_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_female:
        if name == filename:
             # move the file
            shutil.move(filename, dst_female)
            break
141/377:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male")
dst_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

names_male= male_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_male:
        if name == filename:
             # move the file
            shutil.move(filename, dst_male)
            break
141/378:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male")
dst_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

names_male= male_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_male:
        if name == filename:
             # move the file
            shutil.move(filename, dst_male)
            break
141/379:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
146/43:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
146/44:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/45:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move(filename, dst_smile)
            break
146/46:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"+filename, dst_smile)
            break
146/47:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_smile)
            break
146/48:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.move("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
146/49:
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
146/50:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/smiling")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/NotSmiling")

os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/smiling")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/NotSmiling")
146/51:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
146/52:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")
146/53:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
146/54:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
141/380:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/381:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=15,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
146/55:
from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D

from keras import models
146/56:
# starting point 
model_emotion= models.Sequential()

# Add first convolutional block
model_emotion.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3)))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_emotion.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_emotion.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_emotion.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_emotion.add(GlobalAveragePooling2D())
# fully connected layer
model_emotion.add(Dense(64, activation='relu'))
model_emotion.add(BatchNormalization())
# make predictions
model_emotion.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_emotion.summary()
146/57:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
146/58:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_emotion.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
146/59:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
146/60:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_emotion.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
141/382:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
141/383:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=12,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
146/61:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
146/62:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train',
        target_size=(128, 128),
        batch_size=64,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation',
        target_size=(128, 128),
        batch_size=64,
        class_mode='categorical')

history = model_emotion.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 64,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 64,
        callbacks=cb_list)
146/63:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/1:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=15,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/2:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
152/3:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
152/4:
# Gender: -1(Female), 1(Male)

celeb_female = df.loc[df['gender'] == '-1']
female_img = celeb_female[['img_name']]
female_img.shape
152/5:
celeb_male = df.loc[df['gender'] == '1']
male_img = celeb_male[['img_name']]
male_img.shape
152/6:
import os
import glob
import shutil

import os.path
import fnmatch
152/7:
#src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#dst = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train"

os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female")
dst_female = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/female"

names_female = female_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_female:
        if name == filename:
             # move the file
            shutil.move(filename, dst_female)
            break
152/8:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male")
dst_male = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/img/male"

names_male= male_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_male:
        if name == filename:
             # move the file
            shutil.move(filename, dst_male)
            break
152/9:
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
152/10:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train/male")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/test/male")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/female")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation/male")
152/11:
from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D

from keras import models
152/12:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/13:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/14:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/15:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=15,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
151/1:
from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D

from keras import models
151/2:
# starting point 
model_emotion= models.Sequential()

# Add first convolutional block
model_emotion.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3))) 
## 6464
model_emotion.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_emotion.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_emotion.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_emotion.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_emotion.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_emotion.add(GlobalAveragePooling2D())
# fully connected layer
model_emotion.add(Dense(64, activation='relu'))
model_emotion.add(BatchNormalization())
# make predictions
model_emotion.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_emotion.summary()
151/3:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_emotion.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
151/4:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
151/5:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_emotion.fit_generator(
        train_generator,
        epochs=25,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/16:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/17:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/18:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(64,64,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/19:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/20:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/21:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(64, 64),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(64, 64),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
151/6:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
151/7:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_emotion.png", dpi=300)
151/8:
import lab2_landmarks as l2
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
152/22:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/23:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/24:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/25:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
152/26:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/27:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/28:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(178,178,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/29:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/30:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/31:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(178,178),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(178, 178),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
151/9:
import sys
sys.path.append(os.path.abspath("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203")
import lab2_landmarks as l2
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
151/10:
import sys
sys.path.append(os.path.abspath("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203"))
import lab2_landmarks as l2
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
151/11:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
151/12:
import sys
sys.path.append(os.path.abspath("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203"))
import lab2_landmarks as l2
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
151/13:
import sys
sys.path.append(os.path.abspath("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203"))
import lab2_landmarks as l2
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
152/32:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/33:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(96,96,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/34:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/35:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/36:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(96,96),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(96, 96),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/37:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/38:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(96,96),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(96, 96),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
152/39:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,178,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/40:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/41:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/42:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,178),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 178),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/43:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/44:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,178),
        batch_size=12,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 178),
        batch_size=12,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
152/45:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/46:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,178),
        batch_size=64,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 178),
        batch_size=64,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 12,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 12,
        callbacks=cb_list)
152/47:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,178),
        batch_size=64,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 178),
        batch_size=64,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 64,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 64,
        callbacks=cb_list)
152/48:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/49:
# starting point 
model_gender= models.Sequential()

# Add first convolutional block
model_gender.add(Conv2D(16, (3, 3), activation='relu', padding='same',input_shape=(128,128,3)))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# second block
model_gender.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# third block
model_gender.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))
# fourth block
model_gender.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model_gender.add(MaxPooling2D((2, 2), padding='same'))

# global average pooling
model_gender.add(GlobalAveragePooling2D())
# fully connected layer
model_gender.add(Dense(64, activation='relu'))
model_gender.add(BatchNormalization())
# make predictions
model_gender.add(Dense(2, activation='sigmoid'))


# Show a summary of the model. Check the number of trainable parameters
model_gender.summary()
152/50:
# use early stopping to optimally terminate training through callbacks
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# save best model automatically
mc= ModelCheckpoint('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A1/model_gender.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)

cb_list=[es,mc]


# compile model 
model_gender.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
152/51:

from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
152/52:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=64,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=64,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 64,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 64,
        callbacks=cb_list)
152/53:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/54:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=64,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/55:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=20,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
152/56:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/57:
# set up data generator
data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)

# get batches of training images from the directory
train_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/train',
        target_size=(128,128),
        batch_size=32,
        class_mode='categorical')

# get batches of validation images from the directory
validation_generator = data_generator.flow_from_directory(
        '/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/Datasets/celeba/validation',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')

history = model_gender.fit_generator(
        train_generator,
        epochs=13,
        steps_per_epoch=train_generator.samples // 32,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // 32,
        callbacks=cb_list)
151/14: os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B1")
151/15:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B1")
print(os.path.abspath(os.curdir))
151/16:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B1/cartoon_set")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = (basedir,'labels.csv')

print(os.path.abspath(images_dir))
151/17:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B1/cartoon_set")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = (basedir,'labels.csv')

print(os.path.abspath(labels_filename))
151/18:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/B1/cartoon_set")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = os.path.join(basedir,'labels.csv')

print(os.path.abspath(labels_filename))
151/19:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/20:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/21:
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))
151/22:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
151/23:
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))
151/24:
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))
151/25:
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))
151/26:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    lines = labels_file.readlines()
    gender_labels = {line.split(',')[0] : int(line.split(',')[6]) for line in lines[2:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    gender_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, gender_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/27: feature, label = extract_features_labels(images_dir,labels_filename )
151/28:
def extract_features_labels(images_dir, labels_dir):
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    lines = labels_file.readlines()
    gender_labels = {line.split(',')[0] : int(line.split(',')[6]) for line in lines[2:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    gender_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, gender_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/29:
feature, label = 
extract_features_labels
extract_features_labels(images_dir,labels_filename )
151/30: feature, label = extract_features_labels(images_dir,labels_filename )
151/31:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    lines = labels_file.readlines()
    gender_labels = {line.split(',')[0] : int(line.split(',')[6]) for line in lines[2:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    gender_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, gender_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/32: feature, label = extract_features_labels()
151/33:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
#df["Index"] = df["Total"].str.split("\t").str[0]
#df["img_name"] = df["Total"].str.split("\t").str[1]
#df["gender"] = df["Total"].str.split("\t").str[2]
#df["smiling"] = df["Total"].str.split("\t").str[3]
#del df['Total']
#del df['Index']
df
151/34:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]del 
df['Total']
del df['Index']
df
151/35:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
151/36:
df = pd.read_csv("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/labels.csv")
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
151/37:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
152/58:
# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig("model_gender.png", dpi=300)
152/59:

epoch_nums = range(1,num_epochs+1)
training_loss = history.history["loss"]
validation_loss = history.history["val_loss"]
plt.plot(epoch_nums, training_loss)
plt.plot(epoch_nums, validation_loss)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()
152/60:

#epoch_nums = range(1,num_epochs+1)
training_loss = history.history["loss"]
validation_loss = history.history["val_loss"]
plt.plot(epoch_nums, training_loss)
plt.plot(epoch_nums, validation_loss)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()
152/61:

epoch_nums = range(1,epochs+1)
training_loss = history.history["loss"]
validation_loss = history.history["val_loss"]
plt.plot(epoch_nums, training_loss)
plt.plot(epoch_nums, validation_loss)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()
152/62:
epochs =13
epoch_nums = range(1,epochs+1)
training_loss = history.history["loss"]
validation_loss = history.history["val_loss"]
plt.plot(epoch_nums, training_loss)
plt.plot(epoch_nums, validation_loss)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()
152/63:
# epochs =13
# epoch_nums = range(1,epochs+1)
# training_loss = history.history["loss"]
# validation_loss = history.history["val_loss"]
# plt.plot(epoch_nums, training_loss)
# plt.plot(epoch_nums, validation_loss)
# plt.xlabel('epoch')
# plt.ylabel('loss')
# plt.legend(['training', 'validation'], loc='upper right')
# plt.show()



# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
#plt.savefig("model_gender.png", dpi=300)
152/64:
# epochs =13
# epoch_nums = range(1,epochs+1)
# training_loss = history.history["loss"]
# validation_loss = history.history["val_loss"]
# plt.plot(epoch_nums, training_loss)
# plt.plot(epoch_nums, validation_loss)
# plt.xlabel('epoch')
# plt.ylabel('loss')
# plt.legend(['training', 'validation'], loc='upper right')
# plt.show()



# plot training and validation accuracy
import matplotlib.pyplot as plt
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
#plt.ylim([.5,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
#plt.savefig("model_gender.png", dpi=300)
151/38:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
df.to_csv(index=False)
151/39:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv')
labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/40:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/41:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
# del df['Index']
# df["Index"] = df["Total"].str.split("\t").str[0]
# df["img_name"] = df["Total"].str.split("\t").str[1]
# df["gender"] = df["Total"].str.split("\t").str[2]
# df["smiling"] = df["Total"].str.split("\t").str[3]
# del df['Total']
# del df['Index']
 df
# df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
# labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/42:
df = pd.read_csv(labels_filename)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
# del df['Index']
# df["Index"] = df["Total"].str.split("\t").str[0]
# df["img_name"] = df["Total"].str.split("\t").str[1]
# df["gender"] = df["Total"].str.split("\t").str[2]
# df["smiling"] = df["Total"].str.split("\t").str[3]
# del df['Total']
# del df['Index']
df
# df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
# labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/43:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels_filename = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/44:
df = pd.read_csv(labels)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
# del df['Index']
# df["Index"] = df["Total"].str.split("\t").str[0]
# df["img_name"] = df["Total"].str.split("\t").str[1]
# df["gender"] = df["Total"].str.split("\t").str[2]
# df["smiling"] = df["Total"].str.split("\t").str[3]
# del df['Total']
# del df['Index']
df
# df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
# labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/45:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)
images_dir = os.path.join(basedir,'img')
labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/46:
df = pd.read_csv(labels)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
# del df['Index']
# df["Index"] = df["Total"].str.split("\t").str[0]
# df["img_name"] = df["Total"].str.split("\t").str[1]
# df["gender"] = df["Total"].str.split("\t").str[2]
# df["smiling"] = df["Total"].str.split("\t").str[3]
# del df['Total']
# del df['Index']
df
# df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
# labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/47:
df = pd.read_csv(labels)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
labels_filename = os.path.join(basedir,'celeb_labels.csv')
151/48:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/49: feature, label = extract_features_labels()
151/50:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    #labels_file = open(os.path.join(basedir, labels_filename), 'r')
    labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    #emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/51: feature, label = extract_features_labels()
151/52:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    #labels_file = open(os.path.join(basedir, labels_filename), 'r')
    labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    #emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = img.img_to_array(
                img.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/53: feature, label = extract_features_labels()
151/54:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/55: feature, label = extract_features_labels()
151/56:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing import image
151/57:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/58: feature, label = extract_features_labels()
151/59:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
151/60:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/61: feature, label = extract_features_labels()
151/62:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
    
print(len(image_paths)
print(image_paths)
151/63:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print(len(image_paths)
print(image_paths)
151/64:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print(len(image_paths)

print(os.path.abspath(image_paths))
151/65:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print(len(image_paths))

print(os.path.abspath(image_paths))
151/66:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print(len(image_paths))
print(image_paths)
print(os.path.abspath(image_paths))
151/67:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir)]
print(len(image_paths))
print(image_paths)
151/68:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
for item in os.listdir(images_dir):
    if not item.startswith('.') and os.path.isfile(os.path.join(root, item)):
        print item



labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/69:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
for item in os.listdir(images_dir):
    if not item.startswith('.') and os.path.isfile(os.path.join(root, item)):
        print (item)



labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/70:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
for item in os.listdir(images_dir):
    if not item.startswith('.') and os.path.isfile(os.path.join(images_dir, item)):




labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/71:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
for item in os.listdir(images_dir):
    if not item.startswith('.') and os.path.isfile(os.path.join(images_dir, item)):
        print(len(image_paths))
        print(image_paths)



labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/72:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
# for item in os.listdir(images_dir):
#     if not item.startswith('.') and os.path.isfile(os.path.join(images_dir, item)):
#         print(len(image_paths))
#         print(image_paths)



labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
151/73:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]print(len(image_paths))
print(image_paths)
151/74:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
print(len(image_paths))
print(image_paths)
151/75:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/76:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
print(len(image_paths))
print(image_paths)
151/77: feature, label = extract_features_labels()
151/78:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
import opencv as cv2
151/79:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
import opencv-python as cv2
151/80:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
import cv2
151/81:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(gender_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/82: feature, label = extract_features_labels()
151/83:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/84: feature, label = extract_features_labels()
151/85:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            #file_name= img_path.split('.')[1].split('/')[-1]
            file_name= img_path.split('.')[0].split('\\')[-1] ##getting name of file; remove png/jpg + dir


            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/86: feature, label = extract_features_labels()
151/87:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_Store')]
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/88: feature, label = extract_features_labels()
151/89:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]

for img in image_paths:
    file_name = img_path.split('.')[1].split('/')[-1]
    print(file_name)
151/90:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]

for img in image_paths:
    file_name = img.split('.')[1].split('/')[-1]
    print(file_name)
151/91:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]

for img in image_paths:
    file_name = img
    print(file_name)
151/92:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]

for img in image_paths:
    file_name = img.split('/')[-1]
#     file_name = img.split('.')[1].split('/')[-1]
    print(file_name)
151/93:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]

for img in image_paths:
    file_name = img.split('/')[-1].split('.')[0]
#     file_name = img.split('.')[1].split('/')[-1]
    print(file_name)
151/94:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('DS_store')]
print(image_paths)

# for img in image_paths:
#     file_name = img.split('/')[-1].split('.')[0]
#     print(file_name)
151/95:
image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
print(image_paths)

# for img in image_paths:
#     file_name = img.split('/')[-1].split('.')[0]
#     print(file_name)
151/96:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    for img in image_paths:
    file_name = img.split('/')[-1].split('.')[0]
    #print(file_name)
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/97:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    for img in image_paths:
        file_name = img.split('/')[-1].split('.')[0]
    #print(file_name)
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('.')[1].split('/')[-1]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/98: feature, label = extract_features_labels()
151/99:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
151/100: feature, label = extract_features_labels()
   1:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import array_to_img
import cv2
   2:
import numpy as np
from sklearn.metrics import classification_report,accuracy_score
from sklearn import svm
   3:
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

basedir = os.path.abspath(os.curdir)

images_dir = os.path.join(basedir,'img')
# for item in os.listdir(images_dir):
#     if not item.startswith('.') and os.path.isfile(os.path.join(images_dir, item)):
#         print(len(image_paths))
#         print(image_paths)



labels = os.path.join(basedir,'labels.csv')

#print(os.path.abspath(labels_filename))
   4:
df = pd.read_csv(labels)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']
df
df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
labels_filename = os.path.join(basedir,'celeb_labels.csv')
   5:
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))
   6:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
   7: feature, label = extract_features_labels()
   8:
# import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import classification_report,accuracy_score
import pandas as pd
from sklearn.datasets import load_iris ##
import os
from sklearn.metrics import classification_report,accuracy_score
import glob
import shutil
import os.path
import fnmatch
import dlib
from keras.preprocessing import image
import cv2
   9:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[2:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
  10: feature, label = extract_features_labels()
  11:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
  12: feature, label = extract_features_labels()
  13:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  14:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[1] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  15:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  16:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0].split['.'][1] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  17:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0].split['.'][0] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  18:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0].split['.'][-1] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  19:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split(',')[0] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  20:
labels_file = open(os.path.join(basedir, labels_filename), 'r')
#labels_file = open(os.path.join(basedir, labels), 'r')
    
lines = labels_file.readlines()
emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
print(emotion_labels)
  21:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
  22: feature, label = extract_features_labels()
  23:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  24:
def extract_features_labels(images_dir):
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
  25:
train_images_dir = (beasedir, 'train')


# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


train_feature, train_label = extract_features_labels(train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  26:
train_images_dir = (basedir, 'train')


# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


train_feature, train_label = extract_features_labels(train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  27:
s_train_images_dir = (basedir, 'train','smiling')
ns_train_images_dir = (basedir, 'train','NotSmiling')



# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


train_feature, train_label = extract_features_labels(s_train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  28:
def extract_features_labels(images_dir):
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        emotion_labels:      an array containing the gender label (smiling=0 and notsmiling=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so smiling=0 and not smiling=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
  29:
s_train_images_dir = (basedir, 'train','smiling')
print(s_train_images_dir)
ns_train_images_dir = (basedir, 'train','NotSmiling')



# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


#train_feature, train_label = extract_features_labels(s_train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  30:
s_train_images_dir = (basedir, 'train','smiling')
print(os.path.abspath(s_train_images_dir))
ns_train_images_dir = (basedir, 'train','NotSmiling')



# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


#train_feature, train_label = extract_features_labels(s_train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  31:
s_train_images_dir = os.path.join(basedir, 'train','smiling')
print(os.path.abspath(s_train_images_dir))
ns_train_images_dir = (basedir, 'train','NotSmiling')



# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


#train_feature, train_label = extract_features_labels(s_train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  32:
s_train_images_dir = os.path.join(basedir, 'train','smiling')
print(os.path.abspath(s_train_images_dir))
ns_train_images_dir = (basedir, 'train','NotSmiling')



# os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba")

# basedir = os.path.abspath(os.curdir)


# /Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling


train_feature, train_label = extract_features_labels(s_train_images_dir)
#valid_feature, valid_label = extract_features_labels()
#test_feature, test_label = extract_features_labels()
  33:
s_train_images_dir = os.path.join(basedir, 'train','smiling')
ns_train_images_dir = (basedir, 'train','NotSmiling')

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = (basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = (basedir, 'validation','NotSmiling')

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  34:
s_train_images_dir = os.path.join(basedir, 'train','smiling')
ns_train_images_dir = os.path.join(basedir, 'train','NotSmiling')

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  35:
s_train_images_dir = os.path.join(basedir, 'train','smiling')
ns_train_images_dir = os.path.join(basedir, 'train','NotSmiling')

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  36:
def images_dir (folder_name)

s_folder_name_images_dir = os.path.join(basedir, 'folder_name','smiling')
ns_folder_name_images_dir = os.path.join(basedir, 'folder_name','NotSmiling')

def featureLabel (folder_name)

s_folder_name_feature, s_folder_name_label = extract_features_labels(s_folder_name_images_dir)
ns_folder_name_feature, ns_folder_name_label = extract_features_labels(ns_folder_name_images_dir)




# s_test_images_dir = os.path.join(basedir, 'test','smiling')
# ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

# s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
# ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

# s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
# ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

# s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
# ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

# s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
# ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  37:
def images_dir (folder_name):

s_folder_name_images_dir = os.path.join(basedir, 'folder_name','smiling')
ns_folder_name_images_dir = os.path.join(basedir, 'folder_name','NotSmiling')

def featureLabel (folder_name):

s_folder_name_feature, s_folder_name_label = extract_features_labels(s_folder_name_images_dir)
ns_folder_name_feature, ns_folder_name_label = extract_features_labels(ns_folder_name_images_dir)




# s_test_images_dir = os.path.join(basedir, 'test','smiling')
# ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

# s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
# ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

# s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
# ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

# s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
# ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

# s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
# ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  38:
def images_dir (folder_name):

s_folder_name_images_dir  = os.path.join(basedir, 'folder_name','smiling')
ns_folder_name_images_dir  = os.path.join(basedir, 'folder_name','NotSmiling')

def featureLabel (folder_name):

s_folder_name_feature, s_folder_name_label = extract_features_labels(s_folder_name_images_dir)
ns_folder_name_feature, ns_folder_name_label = extract_features_labels(ns_folder_name_images_dir)




# s_test_images_dir = os.path.join(basedir, 'test','smiling')
# ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

# s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
# ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

# s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
# ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

# s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
# ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

# s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
# ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  39:
def images_dir (folder_name):

    s_folder_name_images_dir  = os.path.join(basedir, 'folder_name','smiling')
    ns_folder_name_images_dir  = os.path.join(basedir, 'folder_name','NotSmiling')

def featureLabel (folder_name):

    s_folder_name_feature, s_folder_name_label = extract_features_labels(s_folder_name_images_dir)
    ns_folder_name_feature, ns_folder_name_label = extract_features_labels(ns_folder_name_images_dir)




# s_test_images_dir = os.path.join(basedir, 'test','smiling')
# ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

# s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
# ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')

# s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
# ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

# s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
# ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

# s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
# ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  40:
images_dir(train)
images_dir(test)
images_dir(validation)
  41:


s_train_images_dir  = os.path.join(basedir, 'train','smiling')
ns_train_images_dir  = os.path.join(basedir, 'train','NotSmiling')
  

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')  

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  42: print(s_valid_feature)
  43: print(s_valid_label)
  44: print(ns_valid_label)
  45: print(ns_test_label)
  46: printn(s_test_feature)
  47: print(ns_test_feature)
  48: print(s_test_feature)
  49: print(s_test_images_dir)
  50:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
  51:
# Smiling (1), Not Smiling (-1)

celeb_smile = df.loc[df['smiling'] == '1']
smile_img = celeb_smiling[['img_name']]
smile_img.shape
  52:
df = pd.read_csv(labels)
df = pd.DataFrame(df).reset_index()
df.columns = ['Index','Total']
del df['Index']
df["Index"] = df["Total"].str.split("\t").str[0]
df["img_name"] = df["Total"].str.split("\t").str[1]
df["gender"] = df["Total"].str.split("\t").str[2]
df["smiling"] = df["Total"].str.split("\t").str[3]
del df['Total']
del df['Index']

df.to_csv('/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/celeb_labels.csv', index=False)
labels_filename = os.path.join(basedir,'celeb_labels.csv')
df
  53:
# Smiling (1), Not Smiling (-1)

celeb_smile = df.loc[df['smiling'] == '1']
smile_img = celeb_smile[['img_name']]
smile_img.shape
  54:
celeb_no_smile= df.loc[df['smiling'] == '-1']
no_smile_img = celeb_not_smiling[['img_name']]
no_smile_img.shape
  55:
celeb_no_smile= df.loc[df['smiling'] == '-1']
no_smile_img = celeb_no_smile[['img_name']]
no_smile_img.shape
  56:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.move("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_smile)
            break
  57:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
src = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img"
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling")
dst_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

names_smile = smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_smile)
            break
  58:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
  59:
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
  60:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
  61:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  62:


s_train_images_dir  = os.path.join(basedir, 'train','smiling')
ns_train_images_dir  = os.path.join(basedir, 'train','NotSmiling')
  

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')  

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  63:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/smiling")
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/NotSmiling")

#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")
  64:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
# for name in train_smile:
#     shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

# for name in test_smile:
#     shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
  65:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
# for name in train_no_smile:
#     shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

# for name in test_no_smile:
#     shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  66:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
  67:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
# for name in train_no_smile:
#     shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

# for name in test_no_smile:
#     shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  68:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  69:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  70:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
  71:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
  72:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSsmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  73:


s_train_images_dir  = os.path.join(basedir, 'train','smiling')
ns_train_images_dir  = os.path.join(basedir, 'train','NotSmiling')
  

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')  

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  74:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  75:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")
  76:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
  77:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling")
dst_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

names_no_smile = no_smile_img.img_name.tolist()

for filename in os.listdir(src):
    for name in names_no_smile:
        if name == filename:
             # move the file
            shutil.copy2("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/"+filename, dst_no_smile)
            break
  78:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")

for name in test_no_smile:
    shutil.move(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  79:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")

for name in test_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")
  80:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/NotSmiling"

allFileNames_no_smile = os.listdir(src_no_smile)
np.random.shuffle(allFileNames_no_smile)
train, valid, test = np.split(np.array(allFileNames_no_smile),[int(len(allFileNames_no_smile)*0.7), int(len(allFileNames_no_smile)*0.85)])

train_no_smile = [src_no_smile+'/'+ name for name in train.tolist()]
val_no_smile = [src_no_smile+'/' + name for name in valid.tolist()]
test_no_smile = [src_no_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_no_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/NotSmiling")

for name in val_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/NotSmiling")

for name in test_no_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/NotSmiling")
  81:


s_train_images_dir  = os.path.join(basedir, 'train','smiling')
ns_train_images_dir  = os.path.join(basedir, 'train','NotSmiling')
  

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')  

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
  82:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(
            k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  83:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  84:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        
        print ("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  85:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  86:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
            print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  88:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
            print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
            # Get the landmarks/parts for the face in box d.
            shape = feature
            i += 1
            # The next lines of code just get the coordinates for the mouth
            # and crop the mouth from the image.This part can probably be optimised
            # by taking only the outer most points.
            xmouthpoints = [shape.part(x).x for x in range(48,67)]
            ymouthpoints = [shape.part(x).y for x in range(48,67)]
            maxx = max(xmouthpoints)
            minx = min(xmouthpoints)
            maxy = max(ymouthpoints)
            miny = min(ymouthpoints) 

            # to show the mouth properly pad both sides
            pad = 10
            # basename gets the name of the file with it's extension
            # splitext splits the extension and the filename
            # This does not consider the condition when there are multiple faces in each image.
            # if there are then it just overwrites each image and show only the last image.
            filename = os.path.splitext(os.path.basename(f))[0]


            os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
            os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")

            crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
            #cv2.imshow('mouth',crop_image)
            # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
            # Change the folder if you want to. They are stored in the current directory
            cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  89:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
#             # Get the landmarks/parts for the face in box d.
#             shape = feature
#             i += 1
#             # The next lines of code just get the coordinates for the mouth
#             # and crop the mouth from the image.This part can probably be optimised
#             # by taking only the outer most points.
#             xmouthpoints = [shape.part(x).x for x in range(48,67)]
#             ymouthpoints = [shape.part(x).y for x in range(48,67)]
#             maxx = max(xmouthpoints)
#             minx = min(xmouthpoints)
#             maxy = max(ymouthpoints)
#             miny = min(ymouthpoints) 

#             # to show the mouth properly pad both sides
#             pad = 10
#             # basename gets the name of the file with it's extension
#             # splitext splits the extension and the filename
#             # This does not consider the condition when there are multiple faces in each image.
#             # if there are then it just overwrites each image and show only the last image.
#             filename = os.path.splitext(os.path.basename(f))[0]


#             os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
#             os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")

#             crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
#             #cv2.imshow('mouth',crop_image)
#             # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
#             # Change the folder if you want to. They are stored in the current directory
#             cv2.imwrite(filename+'.jpg',crop_image)  
    return cropped
  90:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        return cropped
  91:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
return cropped
  92:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
    print("Processing file: {}".format(f))
    img = cv2.imread(f)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        #print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
return cropped
  93:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
return cropped
  94:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
  95: cropeye(s_train_images_dir, s_train_feature)
  96:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")

def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
  97:
os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
  98:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
os.chdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
  99:
#os.mkdir("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
os.chdir=("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped")
 100:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 101: cropeye(s_train_images_dir, s_train_feature)
 102:
# for f in glob.glob(os.path.join(images_dir, "*.jpg")):
#         print("Processing file: {}".format(f))
        img = cv2.imread("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/0.jpg")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,68)]
        ymouthpoints = [shape.part(x).y for x in range(48,68)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        #cv2.imwrite(filename+'.jpg',crop_image)  
        plt.imshow(crop_image)
 104:
# for f in glob.glob(os.path.join(images_dir, "*.jpg")):
#         print("Processing file: {}".format(f))
img = cv2.imread("/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/0.jpg")
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,68)]
        ymouthpoints = [shape.part(x).y for x in range(48,68)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        #cv2.imwrite(filename+'.jpg',crop_image)  
        plt.imshow(crop_image)
 105:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 106: print(s_train_feature)
 107: cropeye(s_train_images_dir, s_train_feature)
 108:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        #i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = [shape.part(x).x for x in range(48,67)]
        ymouthpoints = [shape.part(x).y for x in range(48,67)]
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 109: cropeye(s_train_images_dir, s_train_feature)
 110:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        #i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = shape.part(x).x for x in range(48,68)
        ymouthpoints = shape.part(x).y for x in range(48,68)
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 111:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        #i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = (shape.part(x).x for x in range(48,68))
        ymouthpoints = (shape.part(x).y for x in range(48,68))
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 112: cropeye(s_test_images_dir, s_test_feature)
 113:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
    dets = detector(img, 1)
    for k, d in enumerate(dets):
        print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
        # Get the landmarks/parts for the face in box d.
        shape = feature
        i += 1
        # The next lines of code just get the coordinates for the mouth
        # and crop the mouth from the image.This part can probably be optimised
        # by taking only the outer most points.
        xmouthpoints = (shape.part(x).x for x in range(48,68))
        ymouthpoints = (shape.part(x).y for x in range(48,68))
        maxx = max(xmouthpoints)
        minx = min(xmouthpoints)
        maxy = max(ymouthpoints)
        miny = min(ymouthpoints) 

        # to show the mouth properly pad both sides
        pad = 10
        # basename gets the name of the file with it's extension
        # splitext splits the extension and the filename
        # This does not consider the condition when there are multiple faces in each image.
        # if there are then it just overwrites each image and show only the last image.
        filename = os.path.splitext(os.path.basename(f))[0]
        
        
        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
        #cv2.imshow('mouth',crop_image)
        # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
        # Change the folder if you want to. They are stored in the current directory
        cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 114: cropeye(s_test_images_dir, s_test_feature)
 115:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
        dets = detector(img, 1)
            for k, d in enumerate(dets):
            print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
            # Get the landmarks/parts for the face in box d.
            shape = feature
            i += 1
            # The next lines of code just get the coordinates for the mouth
            # and crop the mouth from the image.This part can probably be optimised
            # by taking only the outer most points.
            xmouthpoints = (shape.part(x).x for x in range(48,68))
            ymouthpoints = (shape.part(x).y for x in range(48,68))
            maxx = max(xmouthpoints)
            minx = min(xmouthpoints)
            maxy = max(ymouthpoints)
            miny = min(ymouthpoints) 

            # to show the mouth properly pad both sides
            pad = 10
            # basename gets the name of the file with it's extension
            # splitext splits the extension and the filename
            # This does not consider the condition when there are multiple faces in each image.
            # if there are then it just overwrites each image and show only the last image.
            filename = os.path.splitext(os.path.basename(f))[0]


            crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
            #cv2.imshow('mouth',crop_image)
            # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
            # Change the folder if you want to. They are stored in the current directory
            cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 116:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
        dets = detector(img, 1)
        for k, d in enumerate(dets):
            print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
            # Get the landmarks/parts for the face in box d.
            shape = feature
            i += 1
            # The next lines of code just get the coordinates for the mouth
            # and crop the mouth from the image.This part can probably be optimised
            # by taking only the outer most points.
            xmouthpoints = (shape.part(x).x for x in range(48,68))
            ymouthpoints = (shape.part(x).y for x in range(48,68))
            maxx = max(xmouthpoints)
            minx = min(xmouthpoints)
            maxy = max(ymouthpoints)
            miny = min(ymouthpoints) 

            # to show the mouth properly pad both sides
            pad = 10
            # basename gets the name of the file with it's extension
            # splitext splits the extension and the filename
            # This does not consider the condition when there are multiple faces in each image.
            # if there are then it just overwrites each image and show only the last image.
            filename = os.path.splitext(os.path.basename(f))[0]


            crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
            #cv2.imshow('mouth',crop_image)
            # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
            # Change the folder if you want to. They are stored in the current directory
            cv2.imwrite(filename+'.jpg',crop_image)  
        
    return cropped
 117: cropeye(s_test_images_dir, s_test_feature)
 118:
def cropeye(images_dir, feature):
#     img_path = os.path.join(images_dir, image)
#     img =  cv2.imread(img_path)
#     cropped = img[230:230+58, 175:175+58]
    
#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)
    
    for f in glob.glob(os.path.join(images_dir, "*.jpg")):
        print("Processing file: {}".format(f))
        img = cv2.imread(f)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 
    
    
      # Ask the detector to find the bounding boxes of each face. The 1 in the
    # second argument indicates that we should upsample the image 1 time. This
    # will make everything bigger and allow us to detect more faces.
        dets = detector(img, 1)
        for k, d in enumerate(dets):
            print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))
            # Get the landmarks/parts for the face in box d.
            shape = feature
            i += 1
            # The next lines of code just get the coordinates for the mouth
            # and crop the mouth from the image.This part can probably be optimised
            # by taking only the outer most points.
            xmouthpoints = (shape.part(x).x for x in range(48,68))
            ymouthpoints = (shape.part(x).y for x in range(48,68))
            maxx = max(xmouthpoints)
            minx = min(xmouthpoints)
            maxy = max(ymouthpoints)
            miny = min(ymouthpoints) 

            # to show the mouth properly pad both sides
            pad = 10
            # basename gets the name of the file with it's extension
            # splitext splits the extension and the filename
            # This does not consider the condition when there are multiple faces in each image.
            # if there are then it just overwrites each image and show only the last image.
            filename = os.path.splitext(os.path.basename(f))[0]


            crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]
            #cv2.imshow('mouth',crop_image)
            # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..
            # Change the folder if you want to. They are stored in the current directory
            cv2.imwrite(filename+'.jpg',crop_image)  
        
    #return cropped
 119: cropeye(s_test_images_dir, s_test_feature)
 120: print(s_test_feature)
 121:
os.chdir = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"

src_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/smiling"
src_no_smile = "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/img/Notsmiling"

allFileNames_smile = os.listdir(src_smile)
np.random.shuffle(allFileNames_smile)
train, valid, test = np.split(np.array(allFileNames_smile),[int(len(allFileNames_smile)*0.7), int(len(allFileNames_smile)*0.85)])

train_smile = [src_smile+'/'+ name for name in train.tolist()]
val_smile = [src_smile+'/' + name for name in valid.tolist()]
test_smile = [src_smile+'/' + name for name in test.tolist()]

print('Total images: ', len(allFileNames_smile))
print('Training: ', len(train))
print('Validation: ', len(valid))
print('Testing: ', len(test))

# Copy-pasting images
for name in train_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/train/smiling")

for name in val_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/validation/smiling")

for name in test_smile:
    shutil.copy2(name, "/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/test/smiling")
 122:


s_train_images_dir  = os.path.join(basedir, 'train','smiling')
ns_train_images_dir  = os.path.join(basedir, 'train','NotSmiling')
  

s_test_images_dir = os.path.join(basedir, 'test','smiling')
ns_test_images_dir = os.path.join(basedir, 'test','NotSmiling')

s_valid_images_dir = os.path.join(basedir, 'validation','smiling')
ns_valid_images_dir = os.path.join(basedir, 'validation','NotSmiling')  

s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)
ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)

s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)
ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)

s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)
ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)
 123:
def extract_features_labels(images_dir):
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        emotion_labels:      an array containing the gender label (smiling=0 and notsmiling=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so smiling=0 and not smiling=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
 124: %history
 125: %history -g
 126:
def extract_features_labels(images_dir):
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        emotion_labels:      an array containing the gender label (smiling=0 and notsmiling=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so smiling=0 and not smiling=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
 127:
def extract_features_labels():
    """
    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.
    It also extracts the gender label for each image.
    :return:
        landmark_features:  an array containing 68 landmark points for each image in which a face was detected
        emotion_labels:      an array containing the gender label (smiling=0 and notsmiling=1) for each image in
                            which a face was detected
    """
    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]
    
    target_size = None
    labels_file = open(os.path.join(basedir, labels_filename), 'r')
    #labels_file = open(os.path.join(basedir, labels), 'r')
    
    lines = labels_file.readlines()
    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}
    #emotion_labels ={line.split('\t')[0] : int(line.split('\t')[3]) for line in lines[1:]}
    if os.path.isdir(images_dir):
        all_features = []
        all_labels = []
        for img_path in image_paths:
            file_name= img_path.split('/')[-1].split('.')[0]
            

            # load image
            img = image.img_to_array(
                image.load_img(img_path,
                               target_size=target_size,
                               interpolation='bicubic'))
            features, _ = run_dlib_shape(img)
            if features is not None:
                all_features.append(features)
                all_labels.append(emotion_labels[file_name])

    landmark_features = np.array(all_features)
    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so smiling=0 and not smiling=1
    
    return landmark_features, emotion_labels

def shape_to_np(shape, dtype="int"):
    # initialize the list of (x, y)-coordinates
    coords = np.zeros((shape.num_parts, 2), dtype=dtype)

    # loop over all facial landmarks and convert them
    # to a 2-tuple of (x, y)-coordinates
    for i in range(0, shape.num_parts):
        coords[i] = (shape.part(i).x, shape.part(i).y)

    # return the list of (x, y)-coordinates
    return coords

def rect_to_bb(rect):
    # take a bounding predicted by dlib and convert it
    # to the format (x, y, w, h) as we would normally do
    # with OpenCV
    x = rect.left()
    y = rect.top()
    w = rect.right() - x
    h = rect.bottom() - y

    # return a tuple of (x, y, w, h)
    return (x, y, w, h)


def run_dlib_shape(image):
    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks
    # load the input image, resize it, and convert it to grayscale
    resized_image = image.astype('uint8')

    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)
    gray = gray.astype('uint8')

    # detect faces in the grayscale image
    rects = detector(gray, 1)
    num_faces = len(rects)

    if num_faces == 0:
        return None, resized_image

    face_areas = np.zeros((1, num_faces))
    face_shapes = np.zeros((136, num_faces), dtype=np.int64)

    # loop over the face detections
    for (i, rect) in enumerate(rects):
        # determine the facial landmarks for the face region, then
        # convert the facial landmark (x, y)-coordinates to a NumPy
        # array
        temp_shape = predictor(gray, rect)
        temp_shape = shape_to_np(temp_shape)

        # convert dlib's rectangle to a OpenCV-style bounding box
        # [i.e., (x, y, w, h)],
        #   (x, y, w, h) = face_utils.rect_to_bb(rect)
        (x, y, w, h) = rect_to_bb(rect)
        face_shapes[:, i] = np.reshape(temp_shape, [136])
        face_areas[0, i] = w * h
    # find largest face and keep
    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])

    return dlibout, resized_image
 128: feature, label = extract_features_labels()
 129: %hist -o -g -f ipython_hi
 130: %hist -o -g -f ipython_history.md
