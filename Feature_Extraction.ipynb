{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(os.path.join(basedir,'shape_predictor_68_face_landmarks.dat'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir=(\"/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba\")\n",
    "\n",
    "basedir = os.path.abspath(os.curdir)\n",
    "\n",
    "images_dir = os.path.join(basedir,'img')\n",
    "# for item in os.listdir(images_dir):\n",
    "#     if not item.startswith('.') and os.path.isfile(os.path.join(images_dir, item)):\n",
    "#         print(len(image_paths))\n",
    "#         print(image_paths)\n",
    "\n",
    "\n",
    "\n",
    "labels = os.path.join(basedir,'labels.csv')\n",
    "\n",
    "#print(os.path.abspath(labels_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features_labels(images_dir):\n",
    "    \"\"\"\n",
    "    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.\n",
    "    It also extracts the gender label for each image.\n",
    "    :return:\n",
    "        landmark_features:  an array containing 68 landmark points for each image in which a face was detected\n",
    "        emotion_labels:      an array containing the gender label (smiling=0 and notsmiling=1) for each image in\n",
    "                            which a face was detected\n",
    "    \"\"\"\n",
    "    image_paths = [os.path.join(images_dir, l) for l in os.listdir(images_dir) if not l.endswith('.DS_Store')]\n",
    "    \n",
    "    target_size = None\n",
    "    labels_file = open(os.path.join(basedir, labels_filename), 'r')\n",
    "    #labels_file = open(os.path.join(basedir, labels), 'r')\n",
    "    \n",
    "    lines = labels_file.readlines()\n",
    "    emotion_labels = {line.split('.')[0] : int(line.split(',')[2]) for line in lines[1:]}\n",
    "    #emotion_labels ={line.split('\\t')[0] : int(line.split('\\t')[3]) for line in lines[1:]}\n",
    "    if os.path.isdir(images_dir):\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        for img_path in image_paths:\n",
    "            file_name= img_path.split('/')[-1].split('.')[0]\n",
    "            \n",
    "\n",
    "            # load image\n",
    "            img = image.img_to_array(\n",
    "                image.load_img(img_path,\n",
    "                               target_size=target_size,\n",
    "                               interpolation='bicubic'))\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(emotion_labels[file_name])\n",
    "\n",
    "    landmark_features = np.array(all_features)\n",
    "    emotion_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so smiling=0 and not smiling=1\n",
    "    \n",
    "    return landmark_features, emotion_labels\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "    # loop over all facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, shape.num_parts):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "def run_dlib_shape(image):\n",
    "    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    resized_image = image.astype('uint8')\n",
    "\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.astype('uint8')\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    num_faces = len(rects)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return None, resized_image\n",
    "\n",
    "    face_areas = np.zeros((1, num_faces))\n",
    "    face_shapes = np.zeros((136, num_faces), dtype=np.int64)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        temp_shape = predictor(gray, rect)\n",
    "        temp_shape = shape_to_np(temp_shape)\n",
    "\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)],\n",
    "        #   (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        face_shapes[:, i] = np.reshape(temp_shape, [136])\n",
    "        face_areas[0, i] = w * h\n",
    "    # find largest face and keep\n",
    "    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])\n",
    "\n",
    "    return dlibout, resized_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train_images_dir = os.path.join(basedir,'train','smiling')\n",
    "ns_train_images_dir = os.path.join(basedir,'train','NotSmiling')\n",
    "\n",
    "s_test_images_dir = os.path.join(basedir,'test','smiling')\n",
    "ns_test_images_dir = os.path.join(basedir,'test','NotSmiling')\n",
    "\n",
    "s_valid_images_dir = os.path.join(basedir,'validation','smiling')\n",
    "ns_valid_images_dir = os.path.join(basedir,'validation','NotSmiling')\n",
    "\n",
    "s_train_feature, s_train_label = extract_features_labels(s_train_images_dir)\n",
    "ns_train_feature, ns_train_label = extract_features_labels(ns_train_images_dir)\n",
    "\n",
    "s_test_feature, s_test_label = extract_features_labels(s_test_images_dir)\n",
    "ns_test_feature, ns_test_label = extract_features_labels(ns_test_images_dir)\n",
    "\n",
    "s_valid_feature, s_valid_label = extract_features_labels(s_valid_images_dir)\n",
    "ns_valid_feature, ns_valid_label = extract_features_labels(ns_valid_images_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir(\"/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped\")\n",
    "os.chdir=(\"/Users/Hyunjee/Desktop/AMLS_19-20_SN16075203/A2/celeba/cropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(images_dir, shape):\n",
    "\n",
    "    #img =  cv2.imread(images_dir)\n",
    "#     cropped = img[230:230+58, 175:175+58]    \n",
    "#     cropped = cv2.rectangle(image,(shape[36][0], shape[36][1]), (shape[45][0],shape[33][1]), (255,0,0), 1)\n",
    "    i=0\n",
    "    for f in glob.glob(os.path.join(images_dir, \"*.jpg\")):\n",
    "        print(\"Processing file: {}\".format(f))\n",
    "        img = cv2.imread(f)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Get the landmarks/parts for the face in box d.\n",
    "        i += 1\n",
    "            # The next lines of code just get the coordinates for the mouth\n",
    "            # and crop the mouth from the image.This part can probably be optimised\n",
    "            # by taking only the outer most points.\n",
    "        #for shapes in shape:\n",
    "#         for x in range(48,68):\n",
    "#             xmouthpoints = shape.part(x).x \n",
    "#             ymouthpoints = shape.part(x).y\n",
    "\n",
    "        import itertools\n",
    "        x,y=itertools.izip(*shades)\n",
    "          \n",
    "            \n",
    "        maxx = max(xmouthpoints)\n",
    "        minx = min(xmouthpoints)\n",
    "        maxy = max(ymouthpoints)\n",
    "        miny = min(ymouthpoints) \n",
    "\n",
    "            # to show the mouth properly pad both sides\n",
    "        pad = 10\n",
    "            # basename gets the name of the file with it's extension\n",
    "            # splitext splits the extension and the filename\n",
    "            # This does not consider the condition when there are multiple faces in each image.\n",
    "            # if there are then it just overwrites each image and show only the last image.\n",
    "        filename = os.path.splitext(os.path.basename(f))[0]\n",
    "\n",
    "\n",
    "        crop_image = img[miny-pad:maxy+pad,minx-pad:maxx+pad]\n",
    "            #cv2.imshow('mouth',crop_image)\n",
    "            # The mouth images are saved in the format 'mouth1.jpg, mouth2.jpg,..\n",
    "            # Change the folder if you want to. They are stored in the current directory\n",
    "        cv2.imwrite(filename+'.jpg',crop_image)  \n",
    "        \n",
    "    return cropped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
